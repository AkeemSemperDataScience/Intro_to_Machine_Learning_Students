{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More NLP\n",
    "\n",
    "<b>Note:</b> \"Run All\" for this as you open it, it will likely download some packages that take a while to download.\n",
    "\n",
    "## Truncated Singular Value Decomposition and Dimensionality Reduction\n",
    "\n",
    "When processing text we end up with feature sets that are large! There is up to one feature per different word in our text sample, as well as more for multi-word combinations if there are larger ngrams allowed, far larger than a typical feature set that we're used to. One thing we can do when vectorizing is just to cap the number of features we end up with, but that doesn't seem to be the most sophisticated or smartest approach. \n",
    "\n",
    "TSVD is one thing that we can do to chop down the feature set - or reduce the dimensions - with a little more thought. \n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a common technique in machine learning, it does its name - reduces the dimensions in our feature data. We often want to do this for several reasons: \n",
    "<ul>\n",
    "<li> To reduce the amount of time it takes to train a model.\n",
    "<li> To reduce the amount of memory required to store the data.\n",
    "<li> To reduce the amount of noise in the data.\n",
    "<li> To make the data more interpretable.\n",
    "<li> To make the data more amenable to visualization.\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset from Last Time\n",
    "\n",
    "We'll load the spam dataset and vectorize it with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 89635)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 easter prize</th>\n",
       "      <th>00 easter prize draw</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 sub 16 remove</th>\n",
       "      <th>00 sub 16 unsub</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cine actually</th>\n",
       "      <th>zoom cine actually tonight</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zouk nichols paris</th>\n",
       "      <th>zouk nichols paris free</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>zyada kisi ko kuch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 easter  00 easter prize  00 easter prize draw  00 sub  \\\n",
       "181   0.0        0.0              0.0                   0.0     0.0   \n",
       "322   0.0        0.0              0.0                   0.0     0.0   \n",
       "3399  0.0        0.0              0.0                   0.0     0.0   \n",
       "2994  0.0        0.0              0.0                   0.0     0.0   \n",
       "1173  0.0        0.0              0.0                   0.0     0.0   \n",
       "\n",
       "      00 sub 16  00 sub 16 remove  00 sub 16 unsub  00 subs  00 subs 16  ...  \\\n",
       "181         0.0               0.0              0.0      0.0         0.0  ...   \n",
       "322         0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3399        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "2994        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "1173        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "\n",
       "      zoom cine actually  zoom cine actually tonight  zouk  zouk nichols  \\\n",
       "181                  0.0                         0.0   0.0           0.0   \n",
       "322                  0.0                         0.0   0.0           0.0   \n",
       "3399                 0.0                         0.0   0.0           0.0   \n",
       "2994                 0.0                         0.0   0.0           0.0   \n",
       "1173                 0.0                         0.0   0.0           0.0   \n",
       "\n",
       "      zouk nichols paris  zouk nichols paris free  zyada  zyada kisi  \\\n",
       "181                  0.0                      0.0    0.0         0.0   \n",
       "322                  0.0                      0.0    0.0         0.0   \n",
       "3399                 0.0                      0.0    0.0         0.0   \n",
       "2994                 0.0                      0.0    0.0         0.0   \n",
       "1173                 0.0                      0.0    0.0         0.0   \n",
       "\n",
       "      zyada kisi ko  zyada kisi ko kuch  \n",
       "181             0.0                 0.0  \n",
       "322             0.0                 0.0  \n",
       "3399            0.0                 0.0  \n",
       "2994            0.0                 0.0  \n",
       "1173            0.0                 0.0  \n",
       "\n",
       "[5 rows x 89635 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names_out()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA - Latent Semantic Analysis\n",
    "\n",
    "The TSVD performs something called latent semantic analysis. The process of LSA and the math behind it are not something we need to explore in detail. (LSA is often called LSI - Latent Semantic Indexing). The idea of LSA is that it can generate \"concepts\" in the text. These concepts are found by looking at which terms occur in which documents - documents that have the same terms repeated are likely related to the same concept; other documents that share other words with those documents are likely on the same concept as well.  \n",
    "\n",
    "An important part is the word \"Latent\" - i.e. the patterns detected are hidden, not explicit in the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SVD to Trim Dataset\n",
    "\n",
    "We are starting with LOTS of feature inputs. Below we can loop through several models of different number of remaining components to see the accuracy depending on the number of features we keep in the feature set. The truncated part of truncated SVD trims the featureset down to the most significant features. \n",
    "\n",
    "We started with a lot of features - we can make predictions that are close to as accurate with far fewer, hopefully!\n",
    "\n",
    "<b>Note:</b> this might take a long time to run, depending on your computer. Change the \"for i in range()\" part to cut down on the number of iterations to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/ipykernel_13331/2949601944.py:26: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(results, labels=names, showmeans=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,15):\n",
    "\t\tn = i*5\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LinearSVC(max_iter=100, tol=.01))]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df[0:1000], y[0:1000])\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2 = tf_idf.get_feature_names_out()\n",
    "tmp_df = pd.DataFrame(tmp_vec.toarray(), columns=tok_cols2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3522                                      Try neva mate!!\n",
      "258     We tried to contact you re your reply to our o...\n",
      "4231                             I'm at home. Please call\n",
      "3050    Awesome question with a cute answer: Someone a...\n",
      "3604                      Jordan got voted out last nite!\n",
      "1918                   Is fujitsu s series lifebook good?\n",
      "2849    She's fine. Good to hear from you. How are you...\n",
      "3168                  Haven't seen my facebook, huh? Lol!\n",
      "4327    Congrats! 2 mobile 3G Videophones R yours. cal...\n",
      "1910    Becoz its  &lt;#&gt;  jan whn al the post ofic...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tmp_df, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy\n",
    "\n",
    "Since we are planning on dropping a bunch of data, we can try a model first to see what the baseline accuracy is. I'm also going to limit the number of features here, since using the entire dataset will take ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9727207465900933"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_base = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\", max_features=2000)\n",
    "tmp_vec_base = tf_idf_base.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2_base = tf_idf_base.get_feature_names_out()\n",
    "tmp_df_base = pd.DataFrame(tmp_vec_base.toarray(), columns=tok_cols2_base)\n",
    "X_tr_base, X_te_base, y_tr_base, y_te_base = train_test_split(tmp_df_base, y)\n",
    "\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr_base, y_tr_base)\n",
    "pipe_test.score(X_te_base, y_te_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement LSA and Model\n",
    "\n",
    "We can use the truncated SVD to reduce the number of features in our dataset, in much the way we'd use any other data preparation step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8700646087580761"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tmp = TruncatedSVD(n_components=10)\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"svd\", svd_tmp), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr, y_tr)\n",
    "pipe_test.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but TruncatedSVD was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>2.319122e-04</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>-0.002338</td>\n",
       "      <td>0.010072</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-3.962659e-04</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000428</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-5.526202e-04</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>-0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-1.279164e-04</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-3.165588e-04</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>-0.000433</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>-0.001186</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-2.519258e-03</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>-7.553831e-04</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>8.979838e-07</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000284</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>8.512266e-04</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.005097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000335</td>\n",
       "      <td>-0.000309</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-1.791311e-04</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.000287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.000247 -0.000237 -0.000336  0.000283 -0.000100 -0.000460  0.000070   \n",
       "1     0.001232  0.000295 -0.000280  0.001851 -0.002338  0.010072  0.000065   \n",
       "2    -0.000214 -0.000220  0.000875 -0.000215 -0.000265 -0.000428 -0.000184   \n",
       "3    -0.000171 -0.000174 -0.000207  0.001010 -0.000148 -0.000242 -0.000165   \n",
       "4    -0.000093 -0.000088 -0.000130 -0.000119 -0.000036 -0.000057 -0.000136   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174 -0.000433 -0.000407 -0.000876 -0.000900  0.000248 -0.001186 -0.000218   \n",
       "4175 -0.000211 -0.000193 -0.000327 -0.000278 -0.000063 -0.000144 -0.000285   \n",
       "4176 -0.000277 -0.000230 -0.000558 -0.000312 -0.000451 -0.000400 -0.000041   \n",
       "4177 -0.000258 -0.000284 -0.000353 -0.000170  0.000061 -0.000276 -0.000455   \n",
       "4178 -0.000226 -0.000196 -0.000335 -0.000309 -0.000077 -0.000104 -0.000387   \n",
       "\n",
       "                 7         8         9  \n",
       "0     2.319122e-04 -0.000479  0.000501  \n",
       "1    -3.962659e-04 -0.000237 -0.000057  \n",
       "2    -5.526202e-04  0.002570 -0.000351  \n",
       "3    -1.279164e-04 -0.000223 -0.000578  \n",
       "4    -3.165588e-04 -0.000177 -0.000161  \n",
       "...            ...       ...       ...  \n",
       "4174 -2.519258e-03 -0.001370  0.000467  \n",
       "4175 -7.553831e-04 -0.000248 -0.000095  \n",
       "4176  8.979838e-07  0.000151 -0.001828  \n",
       "4177  8.512266e-04  0.001369  0.005097  \n",
       "4178 -1.791311e-04 -0.000203 -0.000287  \n",
       "\n",
       "[4179 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svd_tmp.transform(X_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:1  ['boyfriend', 'aaniye', 'aaniye pudunga', 'aaniye pudunga venaam', 'athletic', 'athletic lt']\n",
      "Topic:2  ['shouted', '2gthr', '2gthr drinking', '2gthr drinking boost', 'aproach', 'aproach gal']\n",
      "Topic:3  ['curry', 'dream love', 'named', 'love start', 'attraction', 'attraction feel']\n",
      "Topic:4  ['afternoons', 'afternoons evenings', 'afternoons evenings nights', 'amp happy birthday', 'approaching', 'approaching wish']\n",
      "Topic:5  ['don make', 'bad problem', 'bad problem time', 'bed pillows', 'bed pillows floor', 'bed throw']\n",
      "Topic:6  ['bajarangabali', 'bajarangabali maruti', 'bajarangabali maruti pavanaputra', 'dodda', 'dodda problum', 'dodda problum nalli']\n",
      "Topic:7  ['4wrd', '4wrd dear', '4wrd dear loving', 'abt events', 'abt events espe', 'abt functions']\n",
      "Topic:8  ['afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police', 'called police police']\n",
      "Topic:9  ['academic', 'secretary', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Results\n",
    "\n",
    "In the second model, our feature set is far smaller, but we're still getting a very high accuracy. If the original dataset that we started with was very large, this impact would be magnified greatly. In general, NLP models use a lot of data, so this dimensionality reduction can help reduce training datasets that are massive and may even be impractical to process. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics\n",
    "\n",
    "One of the things that LSA can do is to find \"topics\" in the text. We can use the components of the SVD to find the most important words in each topic. A \"topic\" is something that is not explicitly stated in the text, but is implied by the words that are used - if we have several documents that tend to use the same words, they are likely to be about the same topic. The LSA process is able to look for these co-occuring words and the documents that contain them, and group them as being about the same topic. The mechanics of this are some matrix math that is beyond what we need to know, but we can picture it like this.\n",
    "\n",
    "![LSA Math](images/lsa_math.webp \"LSA Math\")\n",
    "\n",
    "The topic extraction is also an example of unsupervised learning - something we'll look at more soon with clustering. We don't provide the topics to the mode in advance like we would with a normal classification - we just give the LSA process the data, and it figures it out on its own.\n",
    "\n",
    "The model doesn't \"understand\" what each topic is, but it is able to pick up on trends of tokens that tend to occur together in documents. Text that contains \"ball\", \"game\", \"football\", \"play\", \"quarterback\" is likely to be about football - the model won't know it is football, but it will know that those words tend to occur together, and documents that contain those words are likely to be about the same topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:1  ['boyfriend', 'aaniye', 'aaniye pudunga', 'aaniye pudunga venaam', 'athletic', 'athletic lt']\n",
      "Topic:2  ['shouted', '2gthr', '2gthr drinking', '2gthr drinking boost', 'aproach', 'aproach gal']\n",
      "Topic:3  ['curry', 'dream love', 'named', 'love start', 'attraction', 'attraction feel']\n",
      "Topic:4  ['afternoons', 'afternoons evenings', 'afternoons evenings nights', 'amp happy birthday', 'approaching', 'approaching wish']\n",
      "Topic:5  ['don make', 'bad problem', 'bad problem time', 'bed pillows', 'bed pillows floor', 'bed throw']\n",
      "Topic:6  ['bajarangabali', 'bajarangabali maruti', 'bajarangabali maruti pavanaputra', 'dodda', 'dodda problum', 'dodda problum nalli']\n",
      "Topic:7  ['4wrd', '4wrd dear', '4wrd dear loving', 'abt events', 'abt events espe', 'abt functions']\n",
      "Topic:8  ['afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police', 'called police police']\n",
      "Topic:9  ['academic', 'secretary', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Truncated SVD\n",
    "\n",
    "Try to use the same text for predictions from the newsgroups last time. Try to use the TSVD with a limited number of components and see if the accuracy can stay similar to what we got last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y): (857, 153056)   Test (x,y): (570, 153056)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and prep datasets\n",
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD()\n",
    "news_steps = [(\"scale\", StandardScaler(with_mean=False)), ('svd', tsvd), ('m', RandomForestClassifier())]\n",
    "news_model = Pipeline(steps=news_steps)\n",
    "#news_model.fit(X_train, y_train)\n",
    "#news_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svd__n_components': 500}\n",
      "0.5894736842105263\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(news_model, \n",
    "                        param_grid={\"svd__n_components\": [10, 50, 80, 100, 200, 500, 1000]},\n",
    "                        cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "gs_model = grid_search.best_estimator_\n",
    "print(gs_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134297</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.054220</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>-0.022465</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>-0.017303</td>\n",
       "      <td>0.060730</td>\n",
       "      <td>-0.095956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263398</td>\n",
       "      <td>0.081490</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.103692</td>\n",
       "      <td>0.057370</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.089302</td>\n",
       "      <td>0.063867</td>\n",
       "      <td>-0.031270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.019212</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>-0.010799</td>\n",
       "      <td>-0.022368</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>-0.003599</td>\n",
       "      <td>-0.001883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.026441</td>\n",
       "      <td>-0.041882</td>\n",
       "      <td>-0.039955</td>\n",
       "      <td>-0.020231</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.143424</td>\n",
       "      <td>0.137325</td>\n",
       "      <td>-0.048582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>0.100209</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>-0.006733</td>\n",
       "      <td>-0.058743</td>\n",
       "      <td>0.077434</td>\n",
       "      <td>0.098721</td>\n",
       "      <td>-0.074898</td>\n",
       "      <td>-0.039261</td>\n",
       "      <td>-0.032322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>-0.006649</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>-0.023206</td>\n",
       "      <td>0.006966</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.066612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>-0.003343</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>-0.003152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.000388  0.000826  0.000519  0.000965  0.000554  0.000268  0.000458   \n",
       "1    0.000549  0.000779  0.001020  0.000972  0.000489  0.000438  0.000610   \n",
       "2    0.000671  0.000397  0.000511  0.000361  0.000516  0.000287  0.000092   \n",
       "3    0.000389  0.000460  0.000312  0.000368  0.000328  0.000146  0.000339   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "852  0.000274  0.000801  0.001144  0.000394  0.000539  0.000215  0.000535   \n",
       "853  0.000938  0.000459  0.000590  0.000275  0.000495  0.001343  0.000422   \n",
       "854  0.000427  0.000753  0.000371  0.000837  0.000989  0.000206  0.000399   \n",
       "855  0.000628  0.000693  0.000216  0.000594  0.000351  0.000637  0.000156   \n",
       "856  0.000610  0.001143  0.000602  0.000477  0.000627  0.000396  0.000355   \n",
       "\n",
       "          7         8         9    ...       490       491       492  \\\n",
       "0    0.000370  0.000469  0.000524  ... -0.134297 -0.009764  0.000608   \n",
       "1    0.000969  0.001167  0.003370  ...  0.000735  0.000547 -0.000194   \n",
       "2    0.000342  0.000582  0.000761  ...  0.263398  0.081490  0.017832   \n",
       "3    0.000153  0.000711  0.000773  ...  0.007093 -0.018374 -0.019212   \n",
       "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "852  0.001061  0.000610  0.000347  ...  0.056696  0.026441 -0.041882   \n",
       "853  0.000504  0.001024  0.000525  ... -0.022957  0.100209  0.110100   \n",
       "854  0.001139  0.000554  0.000872  ...  0.000860 -0.000264 -0.000441   \n",
       "855  0.000904  0.000675  0.000883  ...  0.016111 -0.006649  0.003189   \n",
       "856  0.000485  0.000475  0.001263  ... -0.001929 -0.003731 -0.001539   \n",
       "\n",
       "          493       494       495       496       497       498       499  \n",
       "0    0.054220  0.001199 -0.022465 -0.043596 -0.017303  0.060730 -0.095956  \n",
       "1    0.000610  0.000332 -0.000143 -0.000250  0.001372 -0.001148  0.000116  \n",
       "2   -0.103692  0.057370  0.024621 -0.000139  0.089302  0.063867 -0.031270  \n",
       "3   -0.002444  0.018488 -0.010799 -0.022368  0.019279 -0.003599 -0.001883  \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "852 -0.039955 -0.020231  0.012869 -0.003396  0.143424  0.137325 -0.048582  \n",
       "853 -0.006733 -0.058743  0.077434  0.098721 -0.074898 -0.039261 -0.032322  \n",
       "854  0.001213  0.000682  0.001465 -0.002624  0.000220 -0.000192  0.000601  \n",
       "855 -0.023206  0.006966 -0.006954  0.010503  0.007148  0.004232  0.066612  \n",
       "856  0.003522 -0.003343 -0.001117 -0.000325  0.000389  0.003312 -0.003152  \n",
       "\n",
       "[857 rows x 500 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(tsvd.fit_transform(X_train))\n",
    "used_tsvd = grid_search.best_estimator_.named_steps['svd']\n",
    "pd.DataFrame(used_tsvd.transform(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at the Topics\n",
    "\n",
    "We can also take a look at what the topics identified in the data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['000 years christians', '000 years prophecies', '000 years waited', '10 throwing', '10 throwing pieces', '10 yhvh', '10 yhvh victory', '11 12', '11 12 13', '11 israel']\n",
      "Topic 1:  ['anti religious', 'charities', 'memes', '12 best', '12 best religion', '12 conversion', '12 conversion similarly', '17th 1992', '17th 1992 jeffrey', '1876 defined']\n",
      "Topic 2:  ['10 1797', '10 1797 recently', '1070', '1070 1080', '1070 1080 english', '1080', '1080 english', '1080 english text', '11 government', '11 government united']\n",
      "Topic 3:  ['1993 version constructing', 'absence evidence', 'absence evidence validly', 'abstract concept', 'abstract concept treated', 'abusive charge', 'abusive charge inconsistency', 'abusive variety', 'abusive variety argumentum', 'abusive ve']\n",
      "Topic 4:  ['02 32 11', '11 1993 basically', '11 1993 owe', '12 said', '12 said cared', '13 29 thought', '14 hung', '14 hung lived', '15 days', '15 days peter']\n",
      "Topic 5:  ['12 old', '12 old persian', '12 saoshyant', '12 saoshyant sasanid', '1938 boyce', '1938 boyce 1982', '1938 chaldeans', '1938 chaldeans experts', '1938 later', '1938 later eschatology']\n",
      "Topic 6:  ['0511', '0511 211216', '071 430 1271', '0908 london', '071', '071 831', '0908 london wc1r', '11 december', '11 december 1992', '1244']\n",
      "Topic 7:  ['1981 hell', '1981 hell broke', '1985 david', '1985 david caligiuri', '1985 months', '1985 months later', '1985 relieved', '1985 relieved recalls', '1988 caligiuri', '1988 caligiuri having']\n",
      "Topic 8:  ['150 000 zoroastrians', '1979 revolution', '1979 revolution fundamentalist', '1993 san', '1993 san jose', '200 bay', '200 bay area', '24 hours day', '500', '500 people']\n",
      "Topic 9:  ['introns', '1992 petri', '_believe_ healthy', '_believe_ healthy sign', '_evidence_', '_evidence_ suggests', '_evidence_ suggests come', '_precise_', '_precise_ targeting', '_precise_ targeting random']\n",
      "Topic 10:  ['1066 z1dan exnet', '1066', '1066 z1dan', '15 years prove', 'abounded', 'abounded intensified', 'abounded intensified joseph', 'accept brigham', 'accept brigham young', 'accept right']\n",
      "Topic 11:  ['10 people behave', '13 john', 'astral', 'eckankar', 'psychic planes', 'soul plane', 'sound current', 'sri', '11 13 john', '13 10 people']\n",
      "Topic 12:  ['jesus ra', '38', 'child god', 'differing', 'granted mormon', 'granted mormon belief', 'jesus ra bible', 'lucifer created', 'ra john', 'ra thee']\n",
      "Topic 13:  ['atf fbi', 'boggs', 'byuvm', 'did atf', 'did pose', 'did pose threat', 'executioners', 'extermination', 'extermination order', 'gov boggs']\n",
      "Topic 14:  ['10_ edited hymenaeus', '10_ real point', '10_ real', '1850', '1850 1905', '1850 1905 wealthy', '1855', '1855 augsburg', '1855 augsburg entered', '1876 singer']\n",
      "Topic 15:  ['230 000', '230 000 servicemen', 'absolutist position', 'absolutist position judge', 'accept far', 'accept far evidence', 'account word', 'account word action', 'accurate hit', 'accurate hit industrial']\n",
      "Topic 16:  ['25 children 64', '25 children dead', '25 dead', '25 dead children', '64', '64 adults', '64 adults plus', '89 people', '89 people chance', '8mm']\n",
      "Topic 17:  ['ac uk', 'mail mail', 'mail mail server', 'mail server', 'new readers', 'rtfm', 'rtfm mit', 'rtfm mit edu', 'send mail mail', 'txt send']\n",
      "Topic 18:  ['abolitionists', 'abolitionists oppression', 'abolitionists oppression african', 'actively working', 'actively working christian', 'actually sho', 'actually sho love', 'admitted charges', 'admitted charges pederasty', 'advocation']\n",
      "Topic 19:  ['opinion thing', 'result physical', 'subjective point', 'subjective point view', 'value existence', '_any_', '_any_ _including_', '_any_ _including_ gods', '_empire_', '_empire_ longer']\n",
      "Topic 20:  ['baggage', 'cs utexas', 'cs utexas edu', 'cultural baggage', 'inquisitor', 'looking ethical', 'mr turpin', 'persuasion', 'utexas', 'utexas edu']\n",
      "Topic 21:  ['crap crap', 'crap crap crap', 'evolutionary function', 'homophobe', 'homosexuality evolutionary', 'irrational fear', 'abberation', 'abberation contrary', 'abberation contrary nature', 'abberation homosexual']\n",
      "Topic 22:  ['13 29 did', '13 29 matthew', '19 230', '19 230 denote', '2000 year', '2000 year old', '230 denote', '230 denote context', '29 did', '29 did say']\n",
      "Topic 23:  ['38 kingdom', '38 kingdom remained', 'accepted doctrine', 'accepted doctrine clearly', 'according likes', 'according likes dislikes', 'according robert', 'according robert weiss', 'address convictions', 'address convictions regardless']\n",
      "Topic 24:  ['0418 interested starting', '0418 interested', '17 files', '17 files updated', '17 named', '17 named enviroleague', '1993 earth', '1993 earth forum', '309', '309 675']\n",
      "Topic 25:  ['anti education', 'capitalism', 'capitalist does', 'does mean islam', 'edward', 'edward said', 'following qur', 'hadiths', 'half truths', 'history islamic']\n",
      "Topic 26:  ['90 right', '90 right war', 'abc', 'abc general', 'abc general tone', 'absolute percentage', 'absolute percentage involved', 'acceptance revered', 'acceptance revered author', 'actually monty']\n",
      "Topic 27:  ['11 second', 'allah knowledge', 'allah knowledge things', 'allah swa praise', 'certanity', 'knowledge things', 'messengers', 'miracles number', 'noble messenger', 'occurrence miracles']\n",
      "Topic 28:  ['18 pleased', '18 pleased lds', '1930s', '1930s major', '1930s major indication', '1993 edition', '1993 edition conclude', 'ability love', 'ability love unconditionally', 'absurdly']\n",
      "Topic 29:  ['10 15 respectively', '10 15', '10 15 quoted', '10 14', '10 15 tell', '10 15 matt', '122356', '121624 3400 cbnewsj', '14 10', '14 10 15']\n",
      "Topic 30:  ['000 source comparatively', '125', '000 source', 'sidon', '125 bc', '14 17 thousand', '14 dioceses', '17 thousand', '17 thousand range', '1967']\n",
      "Topic 31:  ['10 commandments want', '10 commandments', '1990 philosophy', '1990 philosophy people', 'ability make', 'ability make mature', 'abolish bad', 'abolish bad governments', 'accounts facts', 'accounts facts historical']\n",
      "Topic 32:  ['dot', 'iota', 'iota dot', 'talks law', 'superceded', '15 involved', '15 involved order', '17 20', '17 20 needs', '17 think come']\n",
      "Topic 33:  ['griffen', 'abandons', 'abandons child', 'abandons child diced', 'abandons woman care', 'abandons woman suffers', 'admit love', 'admit love know', 'agree griffen', 'agree griffen scum']\n",
      "Topic 34:  ['abide', 'abide consequences', 'abide consequences decisions', 'accept praise', 'accept praise times', 'according decisions', 'according decisions make', 'accuse accuse', 'accuse accuse having', 'accuse having']\n",
      "Topic 35:  ['00', '1995', '1995 accurate', '1995 accurate figures', '22 years', '22 years concept', '25 children speak', '50 worth', '50 worth medicine', '51 days died']\n",
      "Topic 36:  ['19 children burned', '_in_', '_in_ context', '_in_ context didn', 'accusing accusing', 'accusing accusing accusing', 'accusing accusing world', 'accusing evil', 'accusing evil things', 'accusing world']\n",
      "Topic 37:  ['217', '217 years', '217 years ago', '357', '357 americans', '357 americans died', 'accused criminals', 'accused criminals cops', 'ago know course', 'allies specifically']\n",
      "Topic 38:  ['official history', 'histories ve', 'histories ve read', 'bash homosexuals', 'common knowledge', 'roehm homosexuality', 'footnotes', 'homosexuality sa', '1920 30', '1920 30 just']\n",
      "Topic 39:  ['context question later', 'failed question', 'question later', '18 93', '18 93 31', '31 93', '31 93 31', '93 31', '93 31 93', 'acceptance axioms']\n",
      "Topic 40:  ['holly', 'christianity mr', 'christianity mr holly', 'finest', 'masonic lodges', 'mr holly', 'southern baptist', 'abuse report', 'abuse report acknowledges', 'achieved good']\n",
      "Topic 41:  ['24 hrs', '24 hrs betrayal', 'absolutely knowledge', 'absolutely knowledge fullfilled', 'actions fullfill', 'actions fullfill prophecy', 'actually said logic', 'agree buys', 'agree buys load', 'agrees given']\n",
      "Topic 42:  ['archival', 'archival material', 'heines', 'heines homosexuals', 'heterosexuals', 'history books', 'hitler youth', 'homosexual nazis', 'kaiser', 'nazis homosexuals']\n",
      "Topic 43:  ['13 years stomping', '22 000', '22 000 people', '26 ezekiel', '26 ezekiel predicts', '26 ezekiel says', '26 pulling', '26 pulling clear', '29 ezekiel', '29 ezekiel quotes']\n",
      "Topic 44:  ['000 years fossil', '200 000', '200 000 years', 'abiliy', 'abiliy fight', 'abiliy fight disease', 'able glean', 'able glean genetic', 'acne', 'acne years']\n",
      "Topic 45:  ['admitted control', 'admitted control aspect', 'admitted master', 'admitted master thought', 'alive jesus', 'alive jesus dead', 'anybody good dies', 'aspect times', 'aspect times know', 'believe topic']\n",
      "Topic 46:  ['apply christian', 'base claims', 'believer brings', 'cultural notions', 'drink value', 'ethical systems meaning', 'extra christian', 'extra christian principles', 'hodge', 'hodge podge']\n",
      "Topic 47:  ['17 dead', '17 dead children', '444', '444 days', '444 days hostages', '50 caliber', '50 caliber machine', '51 days stated', 'absolutely thing', 'absolutely thing government']\n",
      "Topic 48:  ['11 26 sunday', '11 26', '14 19', '15 serpent', '15 serpent bruise', '17 think jesus', '18 says', '18 says truly', '19 annuls', '19 annuls commandments']\n",
      "Topic 49:  ['accept behaved', 'accept behaved inconsistently', 'accept idea', 'accept idea day', 'accept premise ends', 'accept premise willing', 'acceptable social', 'acceptable social behavior', 'accepted premise', 'accepted premise order']\n",
      "Topic 50:  ['_can_', '_can_ objective', '_can_ objective second', 'accept assumptions', 'accept assumptions make', 'accept church', 'accept church addition', 'accept evidence ask', 'accept morality', 'accept morality begin']\n",
      "Topic 51:  ['1950 help', '1950 help sense', '_congressional', '_congressional records_', '_congressional records_ 1950', 'able convince', 'able convince individually', 'able tolerate', 'able tolerate takes', 'abortion create']\n",
      "Topic 52:  ['1985 worked', '1985 worked presidential', '1988 ms', '1988 ms elwell', '1989 manage', '1989 manage legatus', '1992 issue', '1992 issue _campaign', '92 convention', '92 convention conservative']\n",
      "Topic 53:  ['islamic scholars', 'men women different', 'women different', '_naive_', '_naive_ argument', '_naive_ argument sexism', '_wants_', '_wants_ pregnant', '_wants_ pregnant maternity', 'access leave']\n",
      "Topic 54:  ['100 dependent', '100 dependent god', '12 25', '12 25 29', '25 29', '25 29 reckon', '29 reckon', '29 reckon refuse', '2nd thessalonians', '2nd thessalonians 10']\n",
      "Topic 55:  ['accepted accurate', 'accepted accurate reports', 'acclaimed', 'acclaimed prophethood', 'acclaimed prophethood language', 'acclaims', 'acclaims bids', 'acclaims bids welcome', 'accurate reports', 'accurate reports evidences']\n",
      "Topic 56:  ['abhor', 'competent', 'field acts', 'word field acts', '18 chorion', '18 chorion ve', '27 field bought', 'abhor present', 'abhor present authority', 'abhor publishing']\n",
      "Topic 57:  ['112 writing', 'emery', 'accept bible', 'accept bible true', 'account independent', 'account independent proof', 'accounts leaving', 'accounts leaving proofs', 'actually risen', 'actually risen dead']\n",
      "Topic 58:  ['accepted bible', 'accepted bible proof', 'accepted uncritically', 'accepted uncritically does', 'accepted uncritically especially', 'accuse referencing', 'accuse referencing mythology', 'ah knows', 'ah knows god', 'alleged existance']\n",
      "Topic 59:  ['6000', '6000 years', '6000 years old', 'accounted', 'accounted course', 'accounted course existed', 'act bring', 'act bring victory', 'add weight', 'add weight claim']\n",
      "Topic 60:  ['13 years involved', 'parents friends', 'reparative', 'reparative therapy', '1976 1979', '1976 1979 program', '1979 program', '1979 program described', '1991 book', '1991 book _homosexuality_']\n",
      "Topic 61:  ['14000 24000', 'entering leaving', 'guerrilla', 'militia', 'patrol', 'patrol boat', 'people wounding', 'rubber', 'southern lebanon', 'wounding']\n",
      "Topic 62:  ['bk', '50 words', '50 words think', 'apologize rob', 'apologize rob brian', 'appease', 'appease head', 'appease head turn', 'appreciate ll', 'appreciate ll try']\n",
      "Topic 63:  ['accompanied active', 'accompanied active persistent', 'actions crime', 'actions crime certainly', 'actions rushdie', 'actions rushdie feed', 'active persistent', 'active persistent open', 'actually hold', 'actually hold religious']\n",
      "Topic 64:  ['16 god', '16 god loved', '44 55', '44 55 come', '55', '55 come', '55 come unless', '930', '930 glory', '930 glory god']\n",
      "Topic 65:  ['life long', 'actions reactions', 'anxieties', 'arjuna', 'established self', 'fruitive', 'gratification', 'gunya', 'material nature', 'modes material']\n",
      "Topic 66:  ['12 24 26', '24 26', '24 26 assuredly', '26 assuredly', '26 assuredly say', 'actions choices', 'actions choices god', 'actually understands', 'actually understands verses', 'applied equally']\n",
      "Topic 67:  ['050524 9361', '050524', '13 women', 'kr', 'prevost', '1426 733987668 po', 'canadian', 'cwru', 'cwru edu', 'kmr4']\n",
      "Topic 68:  ['aberrations', 'aberrations like', 'aberrations like scientific', 'absence argued', 'absence argued responsible', 'abused', 'abused misused', 'abused misused suffer', 'affairs neo', 'affairs neo fruedian']\n",
      "Topic 69:  ['1a', '1a allegiance', '1a allegiance duty', '2a belief', '2a belief trust', 'accept statement obviously', 'actively say', 'actively say believe', 'actually draw', 'actually draw conclusion']\n",
      "Topic 70:  ['1910', '1910 presbyterian', '1910 presbyterian general', '_gospel fictions_', '_gospel fictions_ randel', 'absolutely room', 'absolutely room debate', 'ancestry fact', 'ancestry fact inconsistencies', 'apologists']\n",
      "Topic 71:  ['absence belief gods', 'accept don', 'accept don share', 'accept isn', 'accept isn particularly', 'ador', 'ador faith', 'ador faith atheist', 'allah proud', 'allah proud embrace']\n",
      "Topic 72:  ['christian devil real', 'cool deny', 'devil real', 'doesn matter man', 'erase', 'erase fantasy', 'fools think', 'fools think cool', 'forever yes', 'forever yes erase']\n",
      "Topic 73:  ['20 25', '22 20 25', 'deuteronomy 22', 'deuteronomy 22 20', '20 25 laws', '20 25 patently', '20 25 wrong', '21 deuteronomy', '21 deuteronomy 22', '25 laws']\n",
      "Topic 74:  ['able attribute', 'able attribute decision', 'action god god', 'adapting commandments', 'adapting commandments transcendent', 'advantage able', 'advantage able attribute', 'allows argue', 'allows argue god', 'answer becuase']\n",
      "Topic 75:  ['ability particular', 'ability particular muslim', 'admitted hadn', 'admitted hadn fact', 'aimed broadly', 'aimed broadly number', 'alia', 'alia said', 'alia said want', 'apologize long']\n",
      "Topic 76:  ['1qvabj', '1qvabj g1j', '1qvabj g1j horus', '_acceptable_', '_acceptable_ _behavior_', '_acceptable_ _behavior_ society', '_behavior_', '_behavior_ society', '_behavior_ society depends', '_societally_']\n",
      "Topic 77:  ['abandoned theism', 'abandoned theism come', 'accept failures', 'accept failures die', 'accept surely', 'accept surely believe', 'accept witness', 'accept witness eyes', 'accustomed', 'accustomed light']\n",
      "Topic 78:  ['actions context', 'actions context moral', 'activate', 'activate moral', 'activate moral driver', 'add moral', 'add moral character', 'agree think', 'agree think definitions', 'amplitudes']\n",
      "Topic 79:  ['3rd highest', '3rd highest official', '58', '58 believe', '58 believe telling', '600 years', '600 years later', 'abraham wealthy', 'abraham wealthy rancher', 'agent takes']\n",
      "Topic 80:  ['defined god', 'haven defined', 'haven defined god', 'pink unicorn', 'requires faith say', 'unicorn', 'accurate representation', 'accurate representation evidence', 'actually driven', 'actually driven does']\n",
      "Topic 81:  ['number beings', 'according poster', 'according poster believe', 'agnostic indeterminate', 'agnostic indeterminate nietzsche', 'ah point', 'ah point didn', 'assumes atheism', 'assumes atheism relevant', 'assumption atheists']\n",
      "Topic 82:  ['04', 'ancient order', 'ancient order oriental', 'hermetic', 'order oriental', 'order oriental templars', 'oriental templars ordo', 'templars ordo', 'templars ordo templi', '16 glorious']\n",
      "Topic 83:  ['descriptions heaven', 'descriptions heaven hell', 'heaven hell', 'just metaphorical', 'justify belief', 'physical descriptions', 'physical descriptions heaven', 'absence theory', 'absence theory germs', 'address question']\n",
      "Topic 84:  ['believed torah', 'multiple moral', 'multiple moral codes', 'parent parent', 'abraham moses case', 'absolute gets', 'absolute gets older', 'analogy does', 'analogy does quite', 'ancestors modern']\n",
      "Topic 85:  ['absolute truth matters', 'accepts consequences', 'accepts consequences matter', 'accepts risk', 'accepts risk innocent', 'accidents totally', 'accidents totally expected', 'actions separating', 'actions separating justice', 'activity presumably']\n",
      "Topic 86:  ['14 responded', '14 responded john', 'kt kt', '16 place', '16 place righteous', 'abode righteous', 'abode righteous dead', 'abraham bosom', 'abraham bosom luke', 'accomplished ones']\n",
      "Topic 87:  ['certainly speak', 'certainly speak muhammad', 'merciful', 'speak humankind', 'speak muhammad', 'speak muhammad saw', '28 29', '28 29 nineteenth', '29 nineteenth', '29 nineteenth thirhy']\n",
      "Topic 88:  ['15 percent', '15 percent rawlins', '15 percent science', '85 percent rawlins', '85 percent religion', 'account hypothesis', 'account hypothesis idiosyncratic', 'actually true', 'actually true context', 'analogy likes']\n",
      "Topic 89:  ['200 feet', '200 feet conduit', '200 pounds', '200 pounds tnt', '35 fuel', '35 fuel air', '_smoke_', '_smoke_ little', '_smoke_ little nonflammable', 'actually means']\n",
      "Topic 90:  ['gave authority', 'make choices', 'morally right', 'permanently', 'permanently damaging', 'right interference', 'set moral standard', 'set standard', 'set standard morality', 'standard morality']\n",
      "Topic 91:  ['achieve', 'fragmentation', 'moral consciousness', 'opposites', 'paradisiac', 'paradisiac state', 'polarities', 'tension', 'warring', '24 follows']\n",
      "Topic 92:  ['ability scissors', 'ability scissors designed', 'abuse contemporaries', 'abuse contemporaries complex', 'abuses don', 'abuses don think', 'according long', 'according long line', 'accounts worse', 'accounts worse contemporaries']\n",
      "Topic 93:  ['100 egalitarian', 'absolute sense don', 'accept non', 'accept non significance', 'accident atheist', 'accident atheist event', 'acquired', 'acquired environment', 'acquired environment teaching', 'activity content']\n",
      "Topic 94:  ['answer question read', 'article certainly', 'article certainly obligation', 'article posting twice', 'article takes', 'article takes quote', 'articles condone', 'articles condone weiss', 'articles jim subject', 'articles wants']\n",
      "Topic 95:  ['000 people killed', '75 000', '75 000 people', 'absolute truth driven', 'act honestly', 'act honestly attempting', 'agree bobby', 'agree bobby far', 'ass spring', 'ass spring mind']\n",
      "Topic 96:  ['action cross', 'action cross border', 'air strip', 'air strip panama', 'allow hitler', 'allow hitler wipe', 'allow legitimately', 'allow legitimately hold', 'arabia gulf', 'arabia gulf war']\n",
      "Topic 97:  ['love story', 'noise', '_darknites_', '_darknites_ series', '_darknites_ series stories', '_journal', '_journal entries_', '_journal entries_ _rings', '_rings', '_rings ii_']\n",
      "Topic 98:  ['able implement', 'able implement legislation', 'accepting homosexuality', 'accepting homosexuality normal', 'accuse judging', 'accuse judging did', 'actions certainly', 'actions certainly use', 'ages persecuted', 'ages persecuted accuse']\n",
      "Topic 99:  ['10 mankind workmanship', '10 mankind', '21 22 heard', '22 heard', '22 heard ancients', 'activity person', 'activity person life', 'actually jon', 'actually jon quite', 'additional rules']\n",
      "Topic 100:  ['absolute morality describes', 'absolute objective standard', 'abstacted', 'abstacted pure', 'abstacted pure nature', 'accord', 'accord true', 'accord true nature', 'according christianity', 'according christianity condemned']\n",
      "Topic 101:  ['abruptly', '1985 functional', '1985 functional illiterate', '1990 god', '1990 god satan', '_the satanic', '_the satanic verses_', 'abruptly lost', 'abruptly lost faith', 'absolutely conception']\n",
      "Topic 102:  ['25 dec', '25 dec ahem', 'abruptly stricken', 'abruptly stricken apparently', 'advancement', 'advancement bureaucracy', 'advancement bureaucracy like', 'agreed obligation', 'agreed obligation blood', 'ahem sure']\n",
      "Topic 103:  ['_must_', '_must_ souls', '_must_ souls verse', '_what_', '_what_ soul', '_what_ soul saying', '_whole_', '_whole_ qur', '_whole_ qur revelation', 'according muslim']\n",
      "Topic 104:  ['7th century', '7th century arabia', 'accurate far', 'accurate far impressed', 'applied logoic', 'applied logoic bible', 'arabia known', 'arabia known include', 'arabic problems', 'arabic problems french']\n",
      "Topic 105:  ['15 apr 93', '93 thoughts', '93 thoughts christians', '_didn', '_didn t_', '_didn t_ expect', 'abilities piece', 'abilities piece mind', 'absurd religion', 'absurd religion exists']\n",
      "Topic 106:  ['1400 scientifically', '1400 scientifically engineered', '18 years', '18 years popehat', '1950 hat', '1950 hat thing', 'actually advanced', 'actually advanced bone', 'advanced bone', 'advanced bone spur']\n",
      "Topic 107:  ['_because_', '_because_ teachings', '_because_ teachings islam', '_in contradiction', '_in contradiction to_', '_in spite', '_in spite of_', '_not_ _because_', '_not_ _because_ teachings', '_not_ harm']\n",
      "Topic 108:  ['cup portal', 'cup portal com', 'misc taoism', 'portal com', 'tao', 'thyagi', '79899', '79899 cup', '79899 cup portal', 'add support']\n",
      "Topic 109:  ['90 population', '90 population members', '_is_ mathematics', '_is_ mathematics usefulness', 'actually minority', 'actually minority possible', 'afraid society', 'afraid society true', 'allow beliefs', 'allow beliefs change']\n",
      "Topic 110:  ['_may_', '_may_ coincide', 'coincide', 'duties', 'group member', '_enforcement_', '_enforcement_ allowed', '_enforcement_ allowed group', '_in conflict', '_in conflict secular']\n",
      "Topic 111:  ['21 april', '21 april 93', '284 lane', '284 lane fox', '391', '391 sunfish', '391 sunfish usd', '85 wit', '85 wit comment', '93 noted']\n",
      "Topic 112:  ['goose', 'goose waddles', 'goose waddles past', 'waddles', 'waddles past', 'accomplice driver', 'accomplice driver getaway', 'accomplice held', 'accomplice held decision', 'action white']\n",
      "Topic 113:  ['dss', 'masoretic', 'masoretic text', 'material far', 'targums', 'variations', '1935', '1935 point', '1935 point masoretic', 'abundance']\n",
      "Topic 114:  ['_assumption_', '_assumption_ god', '_assumption_ god consider', '_is_ important', '_is_ important widely', '_positive_', '_positive_ belief', '_positive_ belief god', 'accept analogy', 'accept analogy big']\n",
      "Topic 115:  ['whos', 'whos life', '_includes_', '_includes_ government', '_includes_ government admittedly', 'able remove', 'able remove _includes_', 'accidental killing', 'accidental killing murder', 'according beautiful']\n",
      "Topic 116:  ['evolution_', 'mechanisms', '_actually_', '_actually_ atoms', '_actually_ atoms decided', '_believe_ _actually_', '_believe_ _actually_ atoms', '_fact', '_fact evolution', '_fact evolution theory_']\n",
      "Topic 117:  ['dying truth', 'access possibly', 'access possibly text', 'answer wouldn', 'answer wouldn able', 'aware order', 'aware order fulfil', 'backs driving', 'backs driving jewish', 'behalf keepers']\n",
      "Topic 118:  ['incredibly wrong', 'abraham gave', 'abraham gave gifts', 'abraham inherit', 'abraham inherit important', 'abraham knowledge', 'abraham knowledge sent', 'acknowledge god', 'acknowledge god selfish', 'ager']\n",
      "Topic 119:  ['mediated', 'absolutely universally', 'absolutely universally definition', 'absurdity question', 'absurdity question does', 'age line', 'age line morality', 'allowed misunderstand', 'allowed misunderstand fault', 'appear god']\n",
      "Topic 120:  ['contrary robert', 'great importance', 'accepted belief', 'accepted belief important', 'agree says', 'agree says say', 'anti mormons presume', 'applied spiritually', 'applied spiritually born', 'ascertainable']\n",
      "Topic 121:  ['absolute contrary', 'absolute contrary reason', 'absolute morality defined', 'absurd suggest', 'absurd suggest common', 'administer justice', 'administer justice agnostic', 'agnostic morality', 'agnostic morality code', 'allows man']\n",
      "Topic 122:  ['accepted jesus', 'accepted jesus christ', 'accuse pre', 'accuse pre judging', 'actions speak', 'actions speak differently', 'arguement point', 'arguement point trying', 'ask draw', 'ask draw conclusions']\n",
      "Topic 123:  ['airport handing', 'airport handing literature', 'anybody ok', 'anybody ok atheists', 'atheist knocking', 'atheist knocking door', 'atheist piece', 'atheist piece reading', 'atheist xtian', 'atheist xtian books']\n",
      "Topic 124:  ['absolute knowledge', 'empirical evidence', 'faith reliable', 'logic empirical', 'logic empirical evidence', 'reliable knowledge', 'reliable way', 'reliable way knowing', 'way knowing', 'absence possibility']\n",
      "Topic 125:  ['nested', 'travesty', 'action isn', 'action isn murderer', 'actually clear', 'actually clear definition', 'actually performed', 'actually performed action', 'apparently unreducable', 'apparently unreducable definition']\n",
      "Topic 126:  ['21 22 25', '22 25', '22 25 22', '22 men', '22 men struggle', '23 injury', '23 injury shall', '24 eye', '24 eye eye', '25 22']\n",
      "Topic 127:  ['db dj', 'dj', 'fellow christians', 'frank does', 'god science', 'god science phamphlet', 'science phamphlet', 'accusation frank', 'accusation frank point', 'answers began']\n",
      "Topic 128:  ['_personal_', '_personal_ accountability', '_personal_ accountability justification', 'absolute moral happens', 'absolute moral truth', 'absolute truth people', 'accountability', 'accountability justification', 'accountability justification socially', 'action belief']\n",
      "Topic 129:  ['consequences law', 'consequences proposition irrelvent', 'incongruency', 'incongruency law', 'irrelvent', 'proposition irrelvent', 'relevent', 'able judge true', 'accept electro', 'accept electro magnetic']\n",
      "Topic 130:  ['102 probably', '_encyclopedia biblical', '_encyclopedia biblical difficulties_', '_see_', '_see_ list', '_see_ list open', 'ad hominen', 'ad hominen point', 'alt slack', 'alt slack posting']\n",
      "Topic 131:  ['word value', 'apply note', 'apply note definitions', 'assigned', 'assigned calculated', 'assigned calculated numerical', 'backtrack', 'backtrack look', 'backtrack look word', 'based systems']\n",
      "Topic 132:  ['immigrants', 'hurt', 'abstract rights', 'abstract rights agreements', 'acooper', 'acooper macalstr', 'acooper macalstr edu', 'adam john', 'adam john cooper', 'agree let']\n",
      "Topic 133:  ['believe bible', 'alt athiesm', 'alt athiesm fact', 'apart oh', 'apart oh little', 'aren plausable', 'aren plausable books', 'aside really', 'aside really wanted', 'athiesm fact']\n",
      "Topic 134:  ['abandon fine', 'abandon fine agree', 'absolute moral absolutist', 'absolutes', 'absolutes moral', 'absolutes moral personally', 'absolutes recognize', 'absolutes recognize moral', 'absolutist moral', 'absolutist moral choose']\n",
      "Topic 135:  ['age tome', 'age tome plus', 'apart rubbish', 'apart rubbish nations', 'beginning civilization', 'beginning civilization david', 'believe bible just', 'believers jesus', 'believers jesus hardly', 'bet plenty']\n",
      "Topic 136:  ['118', 'advice fritter', 'advice fritter away', 'advice matter', 'advice matter whatsoever', 'age deprived', 'age deprived charm', 'al qanawi', 'al qanawi ref', 'ali said']\n",
      "Topic 137:  ['brussel', 'brussel sprouts', 'chemical reactions', 'composed', 'formed matter', 'hudson wrong make', 'humans composed', 'sentient force', 'sprouts', 'undergo']\n",
      "Topic 138:  ['buddhih', 'cetasam', 'determination', 'devotional', 'devotional service', 'does place', 'na', 'na vidhiyate', 'opulence', 'prasaktanam']\n",
      "Topic 139:  ['12 imams preference', 'appropriate topic', 'appropriate topic deeply', 'author introduction', 'author introduction beautiful', 'author think', 'author think goes', 'authors respect', 'authors respect charge', 'available muhammadi']\n",
      "Topic 140:  ['activity parlamentarian', 'activity parlamentarian democracy', 'assumptions forma', 'assumptions forma social', 'assumptions say', 'assumptions say nation', 'assuptions', 'assuptions does', 'assuptions does mean', 'atheism community community']\n",
      "Topic 141:  ['25 mya', '25 mya 40', '40 complete', '40 complete 80', '80 taking', '80 taking consideration', 'absolutely utterly', 'absolutely utterly false', 'afarensis', 'afarensis ca']\n",
      "Topic 142:  ['ahmadi', 'ahmadi muslims', 'ijaz', 'ijaz statement', 'muhammad pbuh', 'muslim brothers', 'tahir', 'tahir ijaz', '_ever_', '_ever_ holy']\n",
      "Topic 143:  ['12 remains', '12 remains correct', 'accept judgements', 'accept judgements reports', 'action analytic', 'action analytic synthetic', 'action guiding', 'action guiding content', 'analytic involve', 'analytic involve lack']\n",
      "Topic 144:  ['boobtist', 'boobtists', 'boobtists realize', 'clowns', 'southern boobtists', 'anti masonic', 'anti masonic crusade', 'babtists', 'babtists masons', 'babtists masons masons']\n",
      "Topic 145:  ['200 shed', '200 shed doubt', 'accepted style', 'accepted style language', 'admittedly 200', 'admittedly 200 shed', 'age authors', 'age authors usual', 'ages says', 'ages says matthew']\n",
      "Topic 146:  ['abortion hell', 'abortion hell interested', 'abortion tibet', 'abortion tibet don', 'accurate say', 'accurate say western', 'answer implies', 'answer implies non', 'answer isn', 'answer isn simple']\n",
      "Topic 147:  ['threaten', '_peacably_', '_peacably_ demonstrate', '_peacably_ demonstrate try', 'acceptable civilized', 'acceptable civilized basis', 'acted like', 'acted like total', 'admit backruptcy', 'admit backruptcy position']\n",
      "Topic 148:  ['12 year', '12 year old', '2a rebroadcast', '2a rebroadcast jerry', 'america new', 'america new jeruselem', 'battle plague', 'battle plague revel', 'began search', 'began search quoted']\n",
      "Topic 149:  ['acts military', 'acts military protection', 'affect genral', 'affect genral population', 'affirm faith', 'affirm faith islam', 'allowed practise', 'allowed practise religion', 'allowed rule', 'allowed rule religious']\n",
      "Topic 150:  ['abound', 'abound surely', 'abound surely seriously', 'allegheny', 'allegheny college', 'allegheny college fills', 'alt atheist allegheny', 'alt fan dan', 'alternatives unless', 'alternatives unless task']\n",
      "Topic 151:  ['22 36', '22 36 note', '34 35 suppose', '35 suppose', '35 suppose come', '36', '36 note', '36 note vs', '37 tell', '37 tell written']\n",
      "Topic 152:  ['amazing repertoire', 'amazing repertoire red', 'anti inerrantists', 'anti inerrantists just', 'apologize pointing', 'apologize pointing straw', 'apology going', 'apology going apologize', 'argument saying', 'argument saying list']\n",
      "Topic 153:  ['allah exist', 'allah exist bobby', 'allah exists', 'allah exists bobby', 'allah infinite', 'allah infinite wisdom', 'allah plus', 'allah plus believer', 'allah thing', 'allah thing relates']\n",
      "Topic 154:  ['effectively', 'non interference', 'able creates', 'able creates self', 'able let', 'able let illustrate', 'actually power', 'actually power wanted', 'affairs nation', 'affairs nation current']\n",
      "Topic 155:  ['16 did follow', '16 did', '17 received', '17 received honor', '18 heard', '18 heard voice', '19 word', '19 word prophets', '2pe 16', '2pe 16 did']\n",
      "Topic 156:  ['151914 1885', 'cs wisc', 'cs wisc edu', 'helium', 'monack', 'wisc', 'wisc edu', '151914', '1885', '1885 daffy']\n",
      "Topic 157:  ['act certain', 'act certain way', 'act particlar', 'act particlar way', 'alternative just', 'asking existence', 'asking existence depend', 'attempting manufacture', 'attempting manufacture contradiction', 'behavior believers']\n",
      "Topic 158:  ['10 gee', '10 gee wouldn', 'added naturally', 'added naturally await', 'ahead details', 'ahead details logic', 'arguing context', 'arguing context extracted', 'arguments context', 'arguments context translation']\n",
      "Topic 159:  ['absurd extreme', 'absurd extreme course', 'achievement', 'achievement book', 'achievement book concludes', 'analyses koran', 'analyses koran version', 'analysis bible', 'analysis bible showing', 'animal know']\n",
      "Topic 160:  ['absence good', 'absence good evil', 'allowing evil', 'allowing evil place', 'appreciate object', 'appreciate object good', 'arguing conceivable', 'arguing conceivable omnipotent', 'argument james', 'argument james tims']\n",
      "Topic 161:  ['130 complete', '1st century', '1st century mean', '325 period', '325 period writings', '4th', '4th century', '4th century 325', '50 yes', '50 yes original']\n",
      "Topic 162:  ['15 16', '16 19 31', '15 16 eternal', '16 19', 'read surrounding', '17 john', '17 john 15', '19 31', '19 31 truth', '31 truth']\n",
      "Topic 163:  ['advisors certainly', 'advisors certainly thought', 'amazed', 'amazed people', 'amazed people try', 'argument claims', 'argument claims follow', 'argument just', 'argument just obviously', 'argument let']\n",
      "Topic 164:  ['0001 usa considered', '0001', '1342 naglee avenue', '1342', '1749', '1749 park', '1749 park ave', '2651', '2651 invoke', '2651 invoke stars']\n",
      "Topic 165:  ['163729 867', '1993apr16', '1993apr16 163729', '1993apr16 163729 867', '867', '867 batman', '867 batman bmd', 'actually stop', 'actually stop murdering', 'agree innocent']\n",
      "Topic 166:  ['absent', 'absent better', 'absent better language', 'absent observations', 'absent observations support', 'absent personal', 'absent personal revelation', 'accountable honest', 'accountable honest mistake', 'ah core']\n",
      "Topic 167:  ['firm conclusion', 'accept authentic', 'accept authentic letter', 'accurately don', 'accurately don argument', 'agreed important', 'agreed important epistles', 'analogy reporters', 'analogy reporters talking', 'argument really leads']\n",
      "Topic 168:  ['analyzing argument interesting', 'anti intellectual', 'anti intellectual childish', 'apparently simpler', 'apparently simpler non', 'argument interesting', 'argument interesting cited', 'atheist pitch', 'atheist pitch partition', 'away confusing']\n",
      "Topic 169:  ['_basis_', '_basis_ argument', '_basis_ argument conclusion', 'actual existance unenfluenced', 'argument conclusion arrived', 'argument science', 'argument science good', 'arrived argument', 'arrived argument science', 'asked little']\n",
      "Topic 170:  ['_hard_', '_hard_ atheism', '_hard_ atheism faith', '_hard_ atheism won', '_in itself_', '_in itself_ arrogant', '_your_ misinformation', '_your_ misinformation shines', 'apologize qualifying', 'apologize qualifying original']\n",
      "Topic 171:  ['accuracy period', 'accuracy period tarot', 'accurate insofar', 'accurate insofar think', 'accurate science', 'accurate science useful', 'accurate shown', 'accurate shown accurate', 'accurate values', 'accurate values objective']\n",
      "Topic 172:  ['200 allow', '200 allow change', 'accuracy tradition', 'accuracy tradition problem', 'allow change', 'allow change dating', 'allow firm', 'allow firm conclusion', 'analysis texts', 'analysis texts yep']\n",
      "Topic 173:  ['natlamp', 'natlamp jesus', 'profess faith', '20 years ago', '_national', '_national lampoon_', '_national lampoon_ comic', 'adams called', 'adams called son', 'ago _national']\n",
      "Topic 174:  ['_anything_', '_anything_ happen', '_anything_ happen atheism', '_behaviors_', '_behaviors_ logical', '_behaviors_ logical punishment', '_knowledgeable', '_knowledgeable believer_', '_knowledgeable believer_ contradiction', '_waving']\n",
      "Topic 175:  ['act contradicted', 'act contradicted nature', 'action god given', 'assume satan', 'assume satan did', 'based biblical', 'based biblical text', 'believe identically', 'believe identically mind', 'benedikt pointed']\n",
      "Topic 176:  ['account smile', 'account smile lot', 'actually important', 'actually important things', 'airports', 'airports just', 'airports just shrug', 'answer question mr', 'apocalypso', 'apocalypso crap']\n",
      "Topic 177:  ['12 23 32', '12 23', '15 18 25', '16 obvious', '16 obvious reason', '18 25 mystery', '21 27', '21 27 fact', '23 32', '23 32 isn']\n",
      "Topic 178:  ['absolutely cares', 'absolutely cares slightest', 'accuracy prediction', 'accuracy prediction reproducibility', 'achieving', 'achieving desired', 'achieving desired expected', 'acknowledges correctness', 'acknowledges correctness meaningful', 'built perceptions']\n",
      "Topic 179:  ['12 inflation', 'according higher', 'according higher value', 'actions course', 'actions course people', 'actions fun', 'actions fun watch', 'actions guide', 'actions guide determine', 'admittedly suffer']\n",
      "Topic 180:  ['address challenges', 'address challenges answers', 'announces', 'announces refuses', 'announces refuses respond', 'answer questions posed', 'answered single', 'answered single challenge', 'answers explictly', 'answers explictly announces']\n",
      "Topic 181:  ['ago bob', 'ago bob beauchaine', 'americans cause', 'americans cause worlds', 'atheists bad', 'atheists bad bad', 'atheists becuase', 'atheists becuase code', 'atheists want feel', 'awright']\n",
      "Topic 182:  ['affecting effecting', 'affecting effecting history', 'atheism time', 'atheism time statement', 'awesome', 'awesome power', 'awesome power dominate', 'believe distort', 'believe distort history', 'cause crusades']\n",
      "Topic 183:  ['_logically_', '_logically_ support', '_logically_ support weak', '_real_god_', '_real_god_ undefinable', '_real_god_ undefinable definition', 'accept sufficient', 'accept sufficient base', 'adequate evidence', 'adequate evidence _real_god_']\n",
      "Topic 184:  ['33 went', '33 went fathers', 'amazing fossil', 'amazing fossil way', 'answer jesus save', 'answer street', 'answer street righteous', 'aphorism', 'aphorism jesus', 'aphorism jesus saves']\n",
      "Topic 185:  ['_pete', '_pete dragon_', '_pete dragon_ maybe', 'advisors curt', 'advisors curt whomever', 'bad guy', 'bad guy good', 'books post', 'books post list', 'buddist']\n",
      "Topic 186:  ['explain wrong', 'explain wrong ok', 'wrong ok', 'admit assumption', 'admit assumption bit', 'agree just', 'agree just say', 'argument disagree', 'argument disagree explain', 'argument sound']\n",
      "Topic 187:  ['accepted kingdom', 'accepted kingdom heaven', 'angels dance', 'angels dance head', 'believed think', 'believed think agree', 'contrived interpretations', 'contrived interpretations knew', 'dance head', 'dance head pin']\n",
      "Topic 188:  ['abortion movement', 'abortion movement usa', 'agree matthew', 'agree matthew certainly', 'anti abortion', 'anti abortion movement', 'anti catholic', 'anti catholic religion', 'anti clericalism', 'anti clericalism follow']\n",
      "Topic 189:  ['access digex', 'access digex com', 'anti sun', 'anti sun represent', 'anti swastika', 'anti swastika goes', 'classic clockwise', 'classic clockwise swastika', 'clock', 'clock going']\n",
      "Topic 190:  ['administer minimum', 'administer minimum punishment', 'advantage removed', 'advantage removed similar', 'answered question concerned', 'apply golden', 'apply golden rule', 'avoid violating', 'avoid violating liberty', 'based concept']\n",
      "Topic 191:  ['investor', 'borrower', 'instead calling', 'investment', 'relabeling', 'return investment', 'stocks', 'thing return', 'bank different', 'bank different stocks']\n",
      "Topic 192:  ['appear undesirable', 'appear undesirable omc', 'attractive choice', 'attractive choice obvious', 'avoid getting', 'avoid getting blasted', 'beings ignore', 'beings ignore influences', 'believe omc', 'believe omc ignored']\n",
      "Topic 193:  ['absolutely ignore', 'absolutely ignore ago', 'accept think', 'accept think unproven', 'ago read', 'ago read interesting', 'agreeable', 'agreeable fellow', 'agreeable fellow really', 'arguments used support']\n",
      "Topic 194:  ['assertion standard', 'assertion standard issue', 'attempt mindless', 'attempt mindless bullying', 'behavior reported', 'behavior reported reminds', 'believe characteristic', 'believe characteristic place', 'believe writer', 'believe writer non']\n",
      "Topic 195:  ['agent looks', 'agent looks like', 'anger thurston', 'anger thurston howell', 'ann', 'ann envy', 'ann envy assorted', 'apeman', 'apeman deceit', 'apeman deceit lust']\n",
      "Topic 196:  ['_bad_ thing', '_bad_ thing misjudgments', '_you_ don', '_you_ don like', 'adolescents', 'adolescents strong', 'adolescents strong overly', 'adult adult', 'adult adult make', 'adult make']\n",
      "Topic 197:  ['000 pounds father', '000 cash rest', '000 pounds free', '50 000', '50 000 cash', '60 000 pounds', 'approached people', 'approached people community', 'bank debt', 'bank debt save']\n",
      "Topic 198:  ['20 eat', '20 eat flesh', 'advanced believe', 'advanced believe cannibalism', 'alive cultures', 'alive cultures including', 'alive story', 'alive story argentinian', 'american indian', 'american indian cultures']\n",
      "Topic 199:  ['basics', 'basics islam', 'explicitly proscribed', 'explicitly proscribed qur', 'proscribed qur', '_as', '_as ve', '_as ve articulated', '_is_ heretical', '_is_ heretical notion']\n",
      "Topic 200:  ['able methodically', 'able methodically judge', 'accordance general', 'accordance general goal', 'action accordance', 'action accordance general', 'actions judged', 'actions judged compatible', 'animals right', 'animals right just']\n",
      "Topic 201:  ['1660 years', '1660', '1660 years nicene', 'gb gb', '325 ad', '325 ad christians', '325 christians', '325 christians recite', 'absurdity afraid', 'absurdity afraid really']\n",
      "Topic 202:  ['addition mentioned', 'addition mentioned hard', 'awful lot quotes', 'banking topic', 'banking topic books', 'bit discussing', 'bit discussing topic', 'books extrapolate', 'books extrapolate awful', 'books subject mentioned']\n",
      "Topic 203:  ['approach subjectively', 'objective approach subjectively', 'objective science', 'surroundings', '_say_', '_say_ physical', '_say_ physical world', 'approach scientist', 'approach scientist takes', 'approach subjectively discuss']\n",
      "Topic 204:  ['religious knowledge', 'accept false', 'accept false hope', 'admit alternative', 'admit alternative possible', 'advance cause', 'alternative basis', 'alternative basis whatsoever', 'alternative point', 'alternative point rationality']\n",
      "Topic 205:  ['aids note', 'aids note idea', 'aids sex', 'aids sex controlled', 'aids spread', 'aids spread sex', 'aids stronger', 'aids stronger religious', 'answers got', 'answers got evidence']\n",
      "Topic 206:  ['actually simple', 'actually simple person', 'administrators', 'administrators similarly', 'administrators similarly company', 'ames', 'ames research', 'ames research lab', 'anti cracking', 'anti cracking legislation']\n",
      "Topic 207:  ['40 day', '40 day period', 'accept evidence matter', 'accept second', 'accept second foundation', 'agree argued', 'agree argued length', 'american college', 'american college site', 'ancients choose']\n",
      "Topic 208:  ['act god post', 'acts god', 'acts god know', 'adam eve people', 'al natural', 'al natural occurrences', 'amazes', 'amazes audacity', 'amazes audacity say', 'asked just']\n",
      "Topic 209:  ['_plausible_', '_plausible_ argument', '_plausible_ argument extra', 'admit probably factors', 'argument mainly', 'argument mainly proposal', 'believe truth', 'believe truth main', 'cause cause just', 'cause just prime']\n",
      "Topic 210:  ['2000 years thinking', 'advice masud', 'advice masud khan', 'agreed maybe', 'agreed maybe speech', 'anti semitism agreed', 'anti semitism gets', 'argue like', 'argue like mr', 'arromdee good']\n",
      "Topic 211:  ['animal kingdom don', 'aren inherent contradict', 'bisexuality', 'bisexuality standard', 'bisexuality standard jargon', 'case ll', 'case ll think', 'cases disprove', 'cases disprove general', 'claim homosexuality']\n",
      "Topic 212:  ['agree heresy', 'agree heresy sweating', 'anti mormons trying', 'atonement getheseme', 'atonement getheseme anti', 'banks n3jxp', 'banks n3jxp skepticism', 'basic mormon', 'basic mormon doctrine', 'believe jehovah']\n",
      "Topic 213:  ['abraham blood', 'abraham blood orthodoxy', 'abraham david', 'abraham david accepting', 'accepting god', 'accepting god word', 'adheres', 'adheres important', 'adheres important god', 'agnostics look']\n",
      "Topic 214:  ['absolutely message', 'absolutely message sin', 'acceptable alternative', 'acceptable alternative lifestyle', 'advocates telling', 'advocates telling don', 'alternative lifestyle', 'alternative lifestyle thanks', 'america saying', 'america saying wouldn']\n",
      "Topic 215:  ['active world', 'active world today', 'ago doesn', 'ago doesn mean', 'ago original', 'ago original letters', 'alive active', 'alive active world', 'alive today question', 'believe didn']\n",
      "Topic 216:  ['absolutists', 'absolutists like', 'absolutists like rest', 'awareness demonstrated', 'awareness demonstrated example', 'believe said', 'believe said morality', 'big deal morality', 'choice self', 'choice self awareness']\n",
      "Topic 217:  ['600 pages', '600 pages book', '90 nearly', '90 nearly 600', '_man', '_man religions_', '_man religions_ john', 'according major', 'according major regions', 'account religions']\n",
      "Topic 218:  ['activities surrounding', 'activities surrounding mating', 'agree morality', 'agree morality necessarily', 'animals likely', 'animals likely ones', 'basically defines', 'basically defines undesired', 'basis does', 'basis does mean']\n",
      "Topic 219:  ['20 children', '20 children prevented', '250k', '250k worth', '250k worth high', 'agents possibly', 'agents possibly executive', 'agents really', 'agents really fit', 'agree sheesh']\n",
      "Topic 220:  ['_antiquities', '_antiquities jews_', '_antiquities jews_ contains', '_antiquities jews_ unquestionably', '_antiquities_', '_antiquities_ lack', '_antiquities_ lack references', 'altered christians', 'altered christians origen', 'arabic manuscript']\n",
      "Topic 221:  ['bad thing defines', 'basically criminal', 'basically criminal justice', 'bugging', 'bugging potential', 'bugging potential criminals', 'chance recidivism', 'chance recidivism basically', 'conducive', 'conducive free']\n",
      "Topic 222:  ['addressing arguments', 'addressing notion', 'addressing notion great', 'americas', 'americas nadja', 'americas nadja explain', 'away crutches', 'away crutches ll', 'awfully convenient', 'awfully convenient way']\n",
      "Topic 223:  ['11 life flesh', '17 11 life', 'accept just means', 'accept maybe', 'accept maybe understanding', 'accept premise god', 'allow perfectly', 'allow perfectly good', 'altar make', 'altar make atonement']\n",
      "Topic 224:  ['act people', 'act people indulge', 'agree theory', 'agree theory hope', 'allah forgive', 'allah forgive end', 'allowed merely', 'allowed merely practice', 'arguments netters', 'arrogant pretend']\n",
      "Topic 225:  ['alunatic', 'alunatic sounds', 'alunatic sounds perfecetly', 'appear true', 'appear true make', 'believed makes', 'believed makes completely', 'cases die', 'cases die lie', 'cause deaths']\n",
      "Topic 226:  ['18 jesus', '18 jesus came', '19 make', '19 make disciples', '20 teaching', '20 teaching obey', '28 18', '28 18 jesus', '28 19', '28 19 make']\n",
      "Topic 227:  ['17th century goes', '750 verses', '750 verses regarding', '_do_ contain', '_do_ contain isn', 'addressing travelling', 'addressing travelling orbit', 'arabic included', 'arabic included brackets', 'arabic regarding']\n",
      "Topic 228:  ['700 club', '700 club david', 'accomplished interpret', 'accomplished interpret correctly', 'atheists really', 'atheists really dishonest', 'bad thing happened', 'bashings', 'bashings plus', 'bashings plus virtually']\n",
      "Topic 229:  ['way define', 'accepted say', 'accepted say way', 'aren observations', 'aren observations based', 'argument far', 'argument far incomplete', 'argument used', 'argument used word', 'assumption exists']\n",
      "Topic 230:  ['750 degree', '750 degree flame', 'ad hominem attack', 'ago backdraft', 'ago backdraft scene', 'analysis yes', 'analysis yes flash', 'analyst', 'analyst cannnot', 'analyst cannnot tell']\n",
      "Topic 231:  ['ants bees', 'ants bees breeding', 'arguments homosexuality', 'arguments homosexuality don', 'bees breeding', 'bees breeding queen', 'bonobos', 'bonobos species', 'bonobos species primate', 'breeding']\n",
      "Topic 232:  ['abuse prophets', 'abuse prophets families', 'allah stereo', 'allah stereo sold', 'ass grass', 'ass grass jim', 'basically prophet', 'basically prophet ve', 'bought dude', 'bought dude guessed']\n",
      "Topic 233:  ['_our_', '_our_ group', '_our_ group mainstream', 'agree came', 'agree came started', 'alt fan brother', 'atheists come', 'atheists come forward', 'bad browns', 'bad browns boobys']\n",
      "Topic 234:  ['premiums', 'able avail', 'able avail alternative', 'abortion coverage', 'abortion coverage health', 'abortion reasons', 'abortion reasons conscience', 'abortion strongly', 'abortion strongly oppose', 'accordingly']\n",
      "Topic 235:  ['_you_ think', '_you_ think christian', 'admit members', 'admit members churches', 'ancestors deserts', 'ancestors deserts utah', 'beliefs followed', 'beliefs followed christian', 'believe christ try', 'believers know']\n",
      "Topic 236:  ['absolutely immoral', '_not_ agree', '_not_ agree things', 'abhorrent', 'abhorrent saying', 'abhorrent saying effect', 'absolutely immoral forever', 'absolutely immoral labeled', 'absolutism', 'absolutism flavour']\n",
      "Topic 237:  ['1000 people lived', '10 000 people', '14000 people', '000 people consult', '17 th', '17 th century', 'acts maybe', 'acts maybe keen', 'alexander destroyed', 'alexander destroyed tyre']\n",
      "Topic 238:  ['apologeticist', 'apologeticist faith', 'apologeticist faith routinely', 'area certain', 'area certain correct', 'assumptions frank', 'assumptions frank decenso', 'big self', 'big self respect', 'board important']\n",
      "Topic 239:  ['apologeticist', 'apologeticist faith', 'apologeticist faith routinely', 'area certain', 'area certain correct', 'assumptions frank', 'assumptions frank decenso', 'big self', 'big self respect', 'board important']\n",
      "Topic 240:  ['accept lost', 'accept lost innocent', 'argue purpose', 'argue purpose continue', 'argument sole', 'argument sole purpose', 'arguments make', 'arguments make claim', 'ask respond', 'ask respond don']\n",
      "Topic 241:  ['actually cancelled', 'actually cancelled school', 'ago climatologist', 'ago climatologist ithink', 'area residents', 'area residents course', 'believed prediction', 'believed prediction bob', 'bob wanting', 'bob wanting excuse']\n",
      "Topic 242:  ['free error', 'advocates view', 'advocates view series', 'agreement matters', 'agreement matters islam', 'behavior muslim', 'behavior muslim free', 'case widespread', 'case widespread agreement', 'certainly viewpoints']\n",
      "Topic 243:  ['7th day adventists', 'acting direct', 'acting direct contradiction', 'actions christian', 'actions christian simple', 'acts christian', 'acts christian bd', 'acts committed', 'acts committed saddam', 'adventists denied']\n",
      "Topic 244:  ['christianity dominant', 'christianity dominant religion', 'dominant', 'dominant religion', 'dominant religion country', 'recognize christianity', 'recognize christianity dominant', 'actually think', 'actually think government', 'agree discussing']\n",
      "Topic 245:  ['93 faq', '93 faq clarified', 'abo', 'abo fi', 'abo fi mats', 'according faq', 'according faq atheism', 'agree don', 'agree don believe', 'andtbacka']\n",
      "Topic 246:  ['_encyclopedia bible_', 'bible_', '_encyclopedia bible_ expect', '_encyclopedia bible_ outrageously', 'arguing guess', 'arguing guess joke', 'arguing substance', 'arguing substance arguing', 'assumed necessarily', 'authority dredge']\n",
      "Topic 247:  ['admire', 'admire tenacity', 'admire tenacity hostile', 'agree getting', 'agree getting stirred', 'appropriate comes', 'appropriate comes close', 'believe characterization', 'believe characterization respond', 'bobby admire']\n",
      "Topic 248:  ['analogous seen', 'analogous seen bicycle', 'applied atheist', 'applied atheist says', 'ask description', 'ask description people', 'atheist says descriptions', 'atheists presented', 'atheists presented simply', 'bad descriptions']\n",
      "Topic 249:  ['50 fine', '50 fine speeding', 'added automatic', 'added automatic appeals', 'appeals lengthy', 'appeals lengthy court', 'automatic appeals', 'automatic appeals lengthy', 'battles relegated', 'battles relegated purpose']\n",
      "Topic 250:  ['american humanist', 'american humanist association', 'asimov asimov', 'asimov asimov funeral', 'asimov funeral', 'asimov funeral vonnegut', 'asimov heaven', 'asimov heaven ignited', 'asimov honorary', 'asimov honorary head']\n",
      "Topic 251:  ['_the good', '_the good atheist', 'argue christian', 'argue christian idea', 'argue theism', 'argue theism usually', 'atheism characteristic lenin', 'atheist handbook_', 'atheist handbook_ law', 'believe face']\n",
      "Topic 252:  ['accept encounter', 'accept encounter deity', 'aside pride', 'aside pride decided', 'aside time', 'aside time time', 'asked god times', 'beliefs aside', 'beliefs aside time', 'believe cos']\n",
      "Topic 253:  ['blamed solely', 'blamed solely mary', 'christian thing', 'christian thing blamed', 'comes ridiculous', 'comes ridiculous story', 'finds getting', 'finds getting mary', 'gets knocked', 'gets knocked think']\n",
      "Topic 254:  ['80 articles', '80 articles january', 'accusation hypocrisy', 'accusation hypocrisy based', 'argument earlier', 'argument earlier taking', 'arguments discussions', 'arguments discussions jim', 'arguments straw', 'arguments straw man']\n",
      "Topic 255:  ['admittedly religion', 'admittedly religion says', 'agreed similar', 'agreed similar similar', 'allows making', 'allows making experiments', 'argue way', 'argue way reason', 'does madness', 'does madness qualify']\n",
      "Topic 256:  ['abraham lincoln', 'abraham lincoln tyrant', 'ago women', 'ago women seven', 'aids isn', 'aids isn fucking', 'big stick', 'calverts', 'calverts fatti', 'calverts fatti maschii']\n",
      "Topic 257:  ['allen schneider', 'allen schneider frank', 'alternating', 'alternating polite', 'alternating polite rude', 'bake timmons shit', 'bobby mozumder maddi', 'calls people names', 'chip shoulder', 'chip shoulder calls']\n",
      "Topic 258:  ['depositors', 'islamically owned', 'islamically owned operated', 'naive depositors', 'operated bank', 'owned operated', 'owned operated bank', 'actually turned', 'actually turned long', 'aren quite']\n",
      "Topic 259:  ['atheism simply', '31mar199321091163', '31mar199321091163 juliet', '31mar199321091163 juliet caltech', '_atheism', '_atheism philosophical', '_atheism philosophical justification_', 'absence belief god', 'agnosticism defined', 'agnosticism defined positive']\n",
      "Topic 260:  ['anthrpomorphic', 'anthrpomorphic quality', 'anthrpomorphic quality desciption', 'assertion complex', 'assertion complex later', 'believe understand evolution', 'biologists instance', 'biologists instance tend', 'birds', 'birds functional']\n",
      "Topic 261:  ['20 52 come', '20 56', '20 56 realize', '20 59', '20 59 thinking', '52 come', '52 come senses', '56 realize', '56 realize maybe', '59 thinking']\n",
      "Topic 262:  ['able investigating', 'able investigating site', 'accidentally set sounds', 'adamantly', 'adamantly denying', 'adamantly denying possible', 'agents insisting', 'agents insisting saw', 'battery', 'battery compound']\n",
      "Topic 263:  ['acceptable acceptable', 'acceptable acceptable fudge', 'acceptable fudge', 'acceptable fudge factor', 'acceptable ideal', 'acceptable ideal moral', 'acceptable probably', 'acceptable probably likely', 'accident likelihood', 'accident likelihood surprisingly']\n",
      "Topic 264:  ['14801', '21 agrino', '21 agrino enkidu', 'agrino', 'agrino enkidu', 'agrino enkidu mic', 'andres', 'andres grino', 'andres grino brandt', 'appreciate comment']\n",
      "Topic 265:  ['14801', '14801 santiago', '21 agrino', '21 agrino enkidu', 'agrino', 'agrino enkidu', 'agrino enkidu mic', 'andres', 'andres grino', 'andres grino brandt']\n",
      "Topic 266:  ['sides', '216 years', '216 years ago', 'aasked', 'aasked come', 'aasked come tv', 'actually aasked', 'actually aasked come', 'affinity', 'affinity subversion']\n",
      "Topic 267:  ['anti societal', 'anti societal opinion', 'apply know', 'apply know plan', 'comment stan', 'comment stan time', 'continue discussion', 'continue discussion knowing', 'continuing', 'continuing discussion']\n",
      "Topic 268:  ['15 years ago', '1919', '1919 street', '1919 street washington', '20054', '20054 expressing', '20054 expressing opposition', 'airways', 'airways america', 'airways america campaigning']\n",
      "Topic 269:  ['adam majority', 'adam majority mankind', 'admit probably read', 'ancestor admit', 'ancestor admit probably', 'away original topic', 'bible days missed', 'collective think', 'collective think insane', 'committed ancestor']\n",
      "Topic 270:  ['accusing bigotry', 'accusing bigotry somewhat', 'alt atheism spend', 'arguments belief', 'arguments belief resorting', 'atheism spend', 'atheism spend time', 'background sex', 'background sex inconsequential', 'belief gods exactly']\n",
      "Topic 271:  ['1804', '1804 king', '1804 king james', 'good news bible', 'news bible', 'actually quotes', 'actually quotes headings', 'anglican', 'anglican greek', 'anglican greek orthodox']\n",
      "Topic 272:  ['accepted religion', 'accepted religion rapidly', 'afterlife', 'afterlife independent', 'afterlife independent social', 'alert unwary', 'alert unwary just', 'amateur', 'amateur sociologist', 'amateur sociologist sub']\n",
      "Topic 273:  ['ability absolute', 'ability absolute proof', 'absolute proof', 'absolute proof atheist', 'accept accept', 'accept accept blind', 'accept blind', 'accept blind faith', 'act faith accept', 'atheist act']\n",
      "Topic 274:  ['abortion lame', 'abortion lame excuse', 'abortion posting', 'abortion posting contents', 'abortion talk', 'abortion talk religion', 'bones contention', 'bones contention lighthearted', 'contention lighthearted', 'contention lighthearted tells']\n",
      "Topic 275:  ['according scriptur', 'according scriptur es', 'almighty maker', 'almighty maker heaven', 'ascended heaven', 'ascended heaven sitteth', 'begotten father', 'begotten father worlds', 'begotten son', 'begotten son god']\n",
      "Topic 276:  ['convincing explanation', 'different explanations', 'theism favor', 'adherents', 'adherents purportedly', 'adherents purportedly theism', 'advice sincerely', 'advice sincerely raymond', 'argument incredulity seen', 'argument theism']\n",
      "Topic 277:  ['ability disregard', 'ability disregard instincts', 'act significant', 'act significant voluntary', 'animals machines', 'animals machines right', 'animals species arbitrarily', 'animals species don', 'arbitrarily killing', 'arbitrarily killing don']\n",
      "Topic 278:  ['action takes', 'action takes place', 'appreciate suggestions', 'appreciate suggestions comments', 'article local', 'article local paper', 'attacks croations', 'attacks croations christian', 'believe press', 'believe press islam']\n",
      "Topic 279:  ['abortion morally', 'abortion morally acceptable', 'acceptable certainly', 'acceptable certainly difference', 'agree abortion', 'agree abortion morally', 'agree day', 'agree day night', 'agree perceive', 'agree perceive brightness']\n",
      "Topic 280:  ['accusing hypocritical', 'accusing hypocritical irrational', 'accusing motivated', 'accusing motivated desire', 'addressing objections', 'addressing objections raise', 'agreed conclusion', 'agreed conclusion bible', 'apologize apologize', 'apologize apologize having']\n",
      "Topic 281:  ['gay lesbian', 'identification', 'pro choice', 'sexual identification', 'unitarian', 'acceptance sexual', 'acceptance sexual identification', 'believe spirituality', 'believe spirituality freedom', 'bibles hook']\n",
      "Topic 282:  ['ago help', 'ago help promote', 'ask need', 'ask need quantify', 'banks got', 'banks got started', 'borrowers', 'borrowers greater', 'borrowers greater loans', 'broke realise']\n",
      "Topic 283:  ['backup', 'backup rantings', 'backup rantings does', 'beatles', 'beatles long', 'beatles long list', 'bible backup', 'bible backup rantings', 'biblical references', 'biblical references impressed']\n",
      "Topic 284:  ['apparently trouble', 'apparently trouble reading', 'areas orient', 'areas orient deletion', 'argument lot', 'argument lot male', 'assume weren', 'assume weren looking', 'assuming misjudgements', 'assuming misjudgements correlated']\n",
      "Topic 285:  ['afraid government', 'afraid government try', 'assume evidence', 'assume evidence responsible', 'bible says commit', 'branch davidians koresh', 'commit murder nkj', 'commit suicide', 'commit suicide furthermore', 'considered murder']\n",
      "Topic 286:  ['14 man', '15 says layed', '15 says', '7th day christians', '7th day looks', '_in regard', '_in regard sabbath', 'acts 15 says', 'alike romans', 'alike romans 14']\n",
      "Topic 287:  ['believe text', 'believe text manner', 'changed little space', 'corrupted theory', 'corrupted theory pretty', 'fact changed', 'fact changed little', 'hebrew text old', 'likely makes', 'likely makes pains']\n",
      "Topic 288:  ['messages posted', 'account using', 'account using irresponsibly', 'action possibility', 'action possibility reason', 'address messages', 'address messages posted', 'apparent intent', 'apparent intent provoking', 'appropriate action']\n",
      "Topic 289:  ['_know_ shouldn', '_know_ shouldn involved', 'actually said reasons', 'believe did make', 'bit deleted', 'bit deleted righto', 'book says muhammad', 'cornflakes', 'cornflakes book', 'cornflakes book says']\n",
      "Topic 290:  ['88 fm', '88 fm st', 'approved media', 'approved media conspirator', 'arsenal', 'arsenal the_doge', 'arsenal the_doge south', 'batf fbi condupes', 'big guy', 'big guy personally']\n",
      "Topic 291:  ['acknowledgement telepathy', 'acknowledgement telepathy precognition', 'answers nope', 'answers nope particular', 'appology', 'appology retraction', 'appology retraction wait', 'article conveniently', 'article conveniently deleted', 'btw david josli']\n",
      "Topic 292:  ['_some_', '_some_ islamic', '_some_ islamic countries', '_that_', '_that_ clear', '_that_ clear trying', '_tsv_', '_tsv_ did', '_tsv_ did make', 'care make']\n",
      "Topic 293:  ['_are_ relative', '_are_ relative relativists', 'ability control', 'ability control events', 'absolute morality disagreeing', 'absolute morals', 'absolute morals accept', 'absolute truth proceed', 'accept amoral', 'accept amoral disagrees']\n",
      "Topic 294:  ['1960s', '1960s issue', '1960s issue wasn', 'acceptable position', 'acceptable position scouting', 'belong intolerance', 'belong intolerance acceptable', 'bsa gestapo', 'bsa hijacked', 'bsa hijacked religious']\n",
      "Topic 295:  ['atlantic', 'atlantic ny', 'atlantic ny nj', 'basis politics', 'basis politics translated', 'california texas', 'california texas florida', 'cause effect', 'cause effect relationship', 'cheese serial']\n",
      "Topic 296:  ['tonite', 'alive murdered', 'alive murdered short', 'batf stayed', 'batf stayed home', 'bds got', 'bds got jim', 'bds punishable', 'bds punishable death', 'better treatment']\n",
      "Topic 297:  ['ability label', 'ability label things', 'actions employ', 'actions employ degree', 'actions people', 'actions people situation', 'admitting actions', 'admitting actions people', 'admitting lot', 'admitting lot admitting']\n",
      "Topic 298:  ['actions angry', 'actions angry fact', 'actions today', 'actions today produce', 'adults knew', 'adults knew supposedly', 'angry fact', 'angry fact people', 'annoyed', 'annoyed adults']\n",
      "Topic 299:  ['believing god consider', 'bits don', 'bits don match', 'bits pleases', 'bits pleases reinterpret', 'brand christianity', 'brand christianity difficulty', 'burdensome', 'burdensome times', 'burdensome times dave']\n",
      "Topic 300:  ['actions presbyterians', 'actions presbyterians methodists', 'ahold', 'ahold thomas', 'ahold thomas stories', 'baptists think', 'baptists think different', 'begging quit', 'begging quit confusing', 'believe way way']\n",
      "Topic 301:  ['commie', 'right rape', 'threat society', 'ahead commie', 'ahead commie singing', 'allowed stuff', 'allowed stuff tell', 'automatic weapons public', 'bd broken', 'bd broken law']\n",
      "Topic 302:  ['human terms', 'finds intuitive', 'finds intuitive silly', 'intuitive silly', 'silly mr', 'accept definition', 'accept definition allow', 'ah law', 'ah law silly', 'allow ascribe']\n",
      "Topic 303:  ['area care', 'area care sceptical', 'assume worried', 'assume worried area', 'atheists basis', 'atheists basis god', 'atheists non', 'atheists non atheists', 'autonomous hand', 'autonomous hand gullible']\n",
      "Topic 304:  ['alley night', 'alley night jump', 'american nra', 'american nra ethos', 'armed teeth', 'armed teeth christian', 'arms christians', 'arms christians rely', 'bear arms', 'bear arms christians']\n",
      "Topic 305:  ['arbitrarily set', 'arbitrarily set possibilities', 'argument false', 'argument false trilemma', 'argument ignore', 'argument ignore instructions', 'argument need', 'argument need learning', 'book rhetoric', 'book rhetoric critical']\n",
      "Topic 306:  ['able understand', 'able understand cause', 'action cell', 'action cell globule', 'actions lesser', 'actions lesser creature', 'actually offspring', 'actually offspring shuold', 'apart quick', 'apart quick perfect']\n",
      "Topic 307:  ['agree reckon', 'agree reckon television', 'basically social', 'basically social interactions', 'changing factors', 'changing factors society', 'communist plot', 'communist plot basically', 'complicated control', 'complicated control just']\n",
      "Topic 308:  ['argument christianity effort', 'argument tammy', 'argument tammy happy', 'astounding', 'astounding claims', 'astounding claims backed', 'atheists like believe', 'author said', 'author said opinion', 'backed evidence']\n",
      "Topic 309:  ['able indoctrinate', 'able indoctrinate children', 'answer conclude', 'answer conclude roger', 'answer far', 'answer far received', 'apparently response', 'apparently response claim', 'ban homosexuals', 'ban homosexuals tell']\n",
      "Topic 310:  ['answer challenges turned', 'articles come', 'articles come think', 'begin new', 'begin new ones', 'best finish', 'best finish thread', 'bloody thing', 'bloody thing ve', 'challenges claim flat']\n",
      "Topic 311:  ['assualt civilians', 'assualt civilians words', 'baseball bats', 'baseball bats feel', 'bats', 'bats feel', 'bats feel spouts', 'broke convicted', 'broke convicted said', 'care rightws']\n",
      "Topic 312:  ['accounts creation', 'acknowledge different', 'acknowledge different accounts', 'agree ve', 'agree ve asked', 'airtight', 'airtight thought', 'airtight thought hope', 'asked help', 'asked help reason']\n",
      "Topic 313:  ['propaganda', '_not_ islamic', '_not_ islamic bank', 'anti islamic slander', 'article want', 'article want true', 'bank seeing', 'bank seeing spending', 'bcci _not_', 'bcci _not_ islamic']\n",
      "Topic 314:  ['0mph', '0mph train 60mph', '0mph train', '60mph', '60mph know', '60mph know train', '60mph moving', '60mph moving speed', '60mph stationary', '60mph stationary train']\n",
      "Topic 315:  ['0mph train 60mph', '0mph', '0mph train', '60mph', '60mph know', '60mph know train', '60mph moving', '60mph moving speed', '60mph stationary', '60mph stationary train']\n",
      "Topic 316:  ['accepting outgoing', 'accepting outgoing postings', 'august school', 'august school newsreader', 'bothered new', 'bothered new feed', 'certainly sympathy', 'certainly sympathy tom', 'computing', 'computing services']\n",
      "Topic 317:  ['atheistic time', 'atheistic time stroll', 'beirut night', 'beirut night ll', 'bobby bother', 'bobby bother tokyo', 'bother tokyo', 'bother tokyo city', 'city ii', 'city ii negligible']\n",
      "Topic 318:  ['approximation perfect', 'approximation perfect objective', 'arbitary', 'arbitary implies', 'arbitary implies clear', 'assume based', 'assume based clear', 'assuming current', 'assuming current manner', 'based clear']\n",
      "Topic 319:  ['american secular', 'american secular capitalist', 'amnesty', 'amnesty international', 'amnesty international pointed', 'application require', 'application require immigration', 'applied citizenship', 'applied citizenship inevitable', 'broadly say']\n",
      "Topic 320:  ['alternative exists', 'alternative exists simply', 'beginnings beginnings', 'beginnings beginnings ok', 'beginnings ok', 'beginnings ok present', 'believing creator', 'believing creator simpler', 'bigger leap', 'bigger leap talk']\n",
      "Topic 321:  ['analogous situation', 'analogous situation supposedly', 'available word', 'available word reliable', 'bad trying', 'bad trying explain', 'better descriptive', 'better descriptive language', 'case bad', 'case bad trying']\n",
      "Topic 322:  ['attacks motivated', 'attacks motivated love', 'attempting ream', 'attempting ream faith', 'belief christ', 'belief christ did', 'believe true attempting', 'better real', 'better real good', 'building replacing']\n",
      "Topic 323:  ['514', '514 844', '844', '1c8', '1c8 tel', '1c8 tel 514', '212', '212 274', '212 274 1989', '274']\n",
      "Topic 324:  ['adherence fundamentalist', 'adherence fundamentalist religions', 'answers don say', 'background need', 'background need validate', 'beliefs pretend', 'beliefs pretend answers', 'brought adherence', 'brought adherence fundamentalist', 'damn right']\n",
      "Topic 325:  ['able assign', 'able assign right', 'does omniscient', 'omniscient definitely', 'assign right', 'assign right wrong', 'right wrong actions', 'wrong actions', 'actions original', 'actions original premises']\n",
      "Topic 326:  ['_are_ important', '_are_ important despite', '_decision_', '_decision_ tried', '_decision_ tried rulings', '_do_ legal', '_do_ legal consequences', 'anarchism', 'anarchism anarchism', 'anarchism anarchism explicitly']\n",
      "Topic 327:  ['affecting believers', 'affecting believers derive', 'believers derive', 'believers derive right', 'cases subjective', 'cases subjective god', 'conclude exist', 'conclude exist meaningless', 'consequence gods', 'consequence gods affecting']\n",
      "Topic 328:  ['_religion_', '_religion_ mythology', '_religion_ mythology accurate', 'absurdity people', 'absurdity people believing', 'accept _religion_', 'accept _religion_ mythology', 'accurate try', 'accurate try tolerant', 'americans heck']\n",
      "Topic 329:  ['24 children', '24 children poison', '45th', '45th symphony', '45th symphony backwards', '_lack_', '_lack_ evidence', '_lack_ evidence shall', 'armed guard', 'armed guard injected']\n",
      "Topic 330:  ['acceptable loss', 'acceptable loss rate', 'americans people', 'americans people jobs', 'answer acceptable', 'answer acceptable loss', 'better solution', 'better solution realize', 'care people demonstrated', 'clothing housing']\n",
      "Topic 331:  ['abortion services different', 'actuarial', 'actuarial costs', 'actuarial costs illustrative', 'acute', 'acute care', 'acute care use', 'add preventative', 'add preventative services', 'add services']\n",
      "Topic 332:  ['admit list', 'admit list exists', 'advance idea', 'advance idea list', 'alt atheism posters', 'assumed ol', 'assumed ol timers', 'atheism posters', 'atheism posters think', 'bobby jim']\n",
      "Topic 333:  ['indonesian government', 'aceh', 'aceh brutally', 'aceh brutally dealt', 'appears indonesian', 'appears indonesian government', 'armed forces', 'armed forces long', 'armed forces people', 'benny murdani']\n",
      "Topic 334:  ['b12', 'eggs', 'products eggs', 'supplements', 'vegan', 'acutally', 'acutally bodies', 'acutally bodies make', 'animal products', 'animal products eggs']\n",
      "Topic 335:  ['indonesian government', 'aceh', 'aceh brutally', 'aceh brutally dealt', 'appears indonesian', 'appears indonesian government', 'armed forces', 'armed forces long', 'armed forces people', 'benny murdani']\n",
      "Topic 336:  ['acuuracy', 'acuuracy nt', 'acuuracy nt making', 'atheism mistaken', 'atheism mistaken pretty', 'atheism mistaken socrates', 'began benedikt', 'began benedikt questioning', 'benedikt questioning', 'benedikt questioning historical']\n",
      "Topic 337:  ['absurd statement', 'absurd statement note', 'advice leave', 'advice leave atheists', 'atheist fails', 'atheist fails recognize', 'atheists peace', 'atheists peace beliefs', 'atheists theists sure', 'beliefs nanci']\n",
      "Topic 338:  ['accused flaming', 'accused flaming faggots', 'accusers', 'accusers controlled', 'accusers controlled records', 'available data', 'available data suggest', 'bearing subject', 'bearing subject true', 'better place departed']\n",
      "Topic 339:  ['11 seen', '11 seen god', '18 said', '18 said thou', '20 man', '20 man hath', '23 lord', '23 lord spake', '30 god', '30 god seen']\n",
      "Topic 340:  ['_the complete', '_the complete golden', 'amusingly', 'amusingly ironic', 'amusingly ironic don', 'belong runs', 'belong runs claiming', 'champaign il', 'champaign il friends', 'claiming true']\n",
      "Topic 341:  ['appears contain', 'appears contain identity', 'case properly', 'case properly symbolized', 'conclusion ej', 'conclusion ej second', 'conclusion follows', 'conclusion properly', 'conclusion properly translated', 'contain identity']\n",
      "Topic 342:  ['accompanied open', 'accompanied open persistent', 'aggravated hostility', 'aggravated hostility islam', 'agree legal', 'agree legal support', 'apostasy accompanied open', 'apostasy punishable', 'apostasy punishable death', 'belief matter']\n",
      "Topic 343:  ['act terrorism', 'act terrorism inspired', 'advancing', 'advancing religion', 'advancing religion doing', 'brutal', 'brutal act', 'brutal act terrorism', 'campus simple', 'campus simple religious']\n",
      "Topic 344:  ['afford', 'afford run', 'afford run camps', 'camp trial', 'camp trial short', 'camps devastation', 'camps devastation caused', 'caused goering', 'caused goering total', 'citizens world']\n",
      "Topic 345:  ['applies reality', 'mathematical concept', 'things certain', 'things certain way', 'actually applies', 'actually applies reality', 'actually continuous', 'actually continuous continuum', 'ancient greeks', 'ancient greeks different']\n",
      "Topic 346:  ['accepting waste', 'accepting waste suggesting', 'alot incentive', 'alot incentive british', 'alot radioactivity', 'alot radioactivity enters', 'appalled dumping', 'appalled dumping saw', 'belfast', 'belfast understanding']\n",
      "Topic 347:  ['33 positive', '33 positive aspect', 'accepted notion', 'accepted notion time', 'arabic research', 'arabic research qur', 'aspect verse', 'aspect verse noted', 'bodies verse', 'bodies verse interpolation']\n",
      "Topic 348:  ['animosity', 'animosity jews', 'animosity jews enemy', 'asking father', 'asking father forgiveness', 'bear animosity', 'bear animosity jews', 'christ died', 'christ died direct', 'christ jesus crucified']\n",
      "Topic 349:  ['3rd debate', '3rd debate cobb', 'champaign urbana', 'champaign urbana clinton', 'class pay', 'class pay programs', 'clinton 3rd', 'clinton 3rd debate', 'cobb won', 'cobb won raise']\n",
      "Topic 350:  ['alternatives imply', 'alternatives imply fact', 'animals moral', 'animals moral significance', 'animals primates', 'animals primates beginnings', 'awareness trying', 'awareness trying say', 'bahaviour', 'bahaviour moral']\n",
      "Topic 351:  ['bit figure', 'bit figure post', 'expresses selfishness idle', 'figure post', 'figure post new', 'getting new', 'getting new postings', 'group population ridiculous', 'idle', 'idle thoughts']\n",
      "Topic 352:  ['absolutely straight', 'absolutely straight arrow', 'antibigot', 'arrow', 'arrow 17', 'arrow 17 year', 'boss', 'boss gay', 'boss gay pansy', 'bsa don']\n",
      "Topic 353:  ['seder', 'feast', 'abuse early', 'abuse early church', 'appears abuse', 'appears abuse early', 'argue wanted', 'argue wanted think', 'argument point', 'argument point fact']\n",
      "Topic 354:  ['big leap', 'big leap sex', 'bond don', 'bond don problem', 'bond emotional', 'bond emotional bond', 'civil religious', 'civil religious bond', 'come emotional', 'come emotional problems']\n",
      "Topic 355:  ['abortion defend', 'abortion defend homosexuality', 'according benedikt', 'according benedikt contardictory', 'argue abortion', 'argue abortion defend', 'atheist object war', 'atheists argue abortion', 'atheists suppose', 'atheists suppose like']\n",
      "Topic 356:  ['accept chastisement', 'accept chastisement better', 'acceptable christians', 'acceptable christians just', 'according bible', 'according bible using', 'attempting pass straight', 'better lot', 'better lot people', 'bible teaches homosexuality']\n",
      "Topic 357:  ['accept hard', 'accept hard atheism', 'actually accept', 'actually accept hard', 'alt atheist hard', 'atheism need', 'atheism need little', 'atheist hard', 'atheist hard won', 'big jim']\n",
      "Topic 358:  ['20 place', '20 place 6th', '666 illuminatti', '666 illuminatti rides', '6th race', '6th race downs', 'announcement old', 'announcement old girl', 'babylon beast', 'babylon beast 666']\n",
      "Topic 359:  ['modelling', 'approved wonder', 'concept presented', 'concept presented got', 'conclusions direct', 'conclusions direct knowledge', 'conclusions drawn description', 'contradictive', 'contradictive modelling', 'contradictive modelling approved']\n",
      "Topic 360:  ['absolute knoweldge', 'absolute knoweldge intent', 'aren inherent defined', 'away isn', 'away isn resolution', 'changes situation', 'changes situation speaking', 'defined objective possible', 'determine morality', 'determine morality action']\n",
      "Topic 361:  ['alexandria manuscripts', 'alexandria manuscripts scrolls', 'answer deleted', 'answer deleted thanks', 'burned soon', 'burned soon septuagint', 'burning valuable', 'burning valuable things', 'changes different', 'changes different versions']\n",
      "Topic 362:  ['act definition', 'act definition instinctive', 'act morality trying', 'alternative does', 'alternative does prevent', 'animals free', 'behavior moral try', 'behavior pattern', 'behavior pattern act', 'class princples']\n",
      "Topic 363:  ['activities considerably', 'activities considerably compelling', 'anti christian', 'anti christian literature', 'arguments christianity', 'arguments christianity arguments', 'arguments used today', 'attacking christianity', 'attacking christianity denies', 'century enormous']\n",
      "Topic 364:  ['arianism', 'arianism rejected', 'arianism rejected nicene', 'attempt aware', 'attempt aware don', 'attempt identify', 'attempt identify vast', 'avoid using', 'avoid using phrase', 'aware don']\n",
      "Topic 365:  ['apostacy mistaken', 'apostacy mistaken tyre', 'bible commentaries', 'bible commentaries interpret', 'calling satan', 'calling satan king', 'christian bible commentaries', 'christian christian bible', 'commentaries interpret', 'commentaries interpret king']\n",
      "Topic 366:  ['_applicability_', '_applicability_ islamic', '_applicability_ islamic law', '_enforcible_', '_enforcible_ despite', '_enforcible_ despite _law_', '_law_', '_law_ applicable', '_law_ applicable disagree', 'act according']\n",
      "Topic 367:  ['bait', 'batf fbi government', 'burned house', 'burned house batf', 'casserole', 'casserole leftover', 'casserole leftover turkey', 'christmas truelove', 'christmas truelove served', 'commercial']\n",
      "Topic 368:  ['whitten', 'accounts clear', 'accounts clear point', 'argue point', 'argue point saw', 'association pointed', 'association pointed haven', 'association tammuz', 'association tammuz osiris', 'blood god']\n",
      "Topic 369:  ['19 feb', '19 feb 1990', '1990 subject', '1990 subject atheist', '473', '473 question', '473 question article', '48 articles', '48 articles day', '53766']\n",
      "Topic 370:  ['1666', '1666 grew', 'alive rough', 'alive rough way', 'alive thousands', 'alive thousands cases', 'antichrist', 'antichrist coming', 'antichrist coming 1666', 'apocalypse obsessed']\n",
      "Topic 371:  ['account way', 'account way useful', 'act exposed', 'act exposed religion', 'agree course', 'agree course really', 'bad account', 'bad account way', 'basically judge', 'basically judge religion']\n",
      "Topic 372:  ['proves did', 'agencies', 'agencies intentionally', 'agencies intentionally started', 'agency did', 'agency did don', 'arguement ll', 'arguement ll throw', 'atf cia', 'atf cia kgb']\n",
      "Topic 373:  ['anthony landreneau', 'anthony landreneau ozonehole', 'anthony slmr', 'anthony slmr difference', 'away lm', 'away lm true', 'body months', 'body months unbelievably', 'cold anthony', 'cold anthony slmr']\n",
      "Topic 374:  ['action sure fbi', 'armageddon theirs', 'armageddon theirs cheers', 'batf didn', 'batf didn deliberately', 'bets fanatical', 'bets fanatical people', 'careful operation', 'careful operation use', 'case sorry']\n",
      "Topic 375:  ['alike god', 'gentile alike god', 'age grace', 'age grace sin', 'apart rest', 'apart rest world', 'applied god', 'applied god chosen', 'available jew', 'available jew gentile']\n",
      "Topic 376:  ['800', '800 alleged', '800 alleged contradictions', '_when', '_when critics', '_when critics ask', 'alleged bible', 'alleged bible contradictions', 'alleged contradictions frank', 'alleged contradictions taking']\n",
      "Topic 377:  ['70 people', '70 people died', 'actually suspicious', 'actually suspicious fbi', 'armored', 'armored vehicles', 'armored vehicles unhappy', 'assault private', 'assault private property', 'believe ramming']\n",
      "Topic 378:  ['actually story', 'actually story goes', 'best way error', 'bit fruit', 'bit fruit gave', 'bow man', 'bow man god', 'commanded lucifer', 'commanded lucifer devoted', 'contradictions stories']\n",
      "Topic 379:  ['amusing novel uses', 'bits lots', 'bits lots killing', 'borrowed', 'borrowed catholicism', 'borrowed catholicism mass', 'bread wine quite', 'catholicism mass', 'catholicism mass communion', 'certainly traditional']\n",
      "Topic 380:  ['11838 vice ico', 'absurd notion', 'absurd notion original', 'accept means', 'accept means statement', 'analysed', 'analysed venue', 'anselm believe', 'anselm believe understand', 'article 11838']\n",
      "Topic 381:  ['accusations fabrications', 'accusations fabrications shi', 'ages seriously', 'ages seriously sheiks', 'al azhar', 'al azhar don', 'arabia wahabis', 'arabia wahabis matter', 'azhar', 'azhar don']\n",
      "Topic 382:  ['agreed change', 'agreed change time', 'agreed interpretations', 'agreed interpretations observations', 'agreed values', 'agreed values interpretation', 'certainly different', 'certainly different quantum', 'change time', 'change time values']\n",
      "Topic 383:  ['agree deleted deleted', 'appeal authority opposition', 'argument fails', 'argument fails statement', 'atheists believed', 'atheists believed use', 'authorities better', 'authorities better second', 'authority opposition', 'authority opposition just']\n",
      "Topic 384:  ['ashes', 'ashes scream', 'ashes scream cheers', 'burn ashes', 'burn ashes scream', 'capable letting', 'capable letting small', 'case dealing', 'case dealing fanatical', 'children burn']\n",
      "Topic 385:  ['american people feds', 'authorities firing', 'authorities firing shot', 'book suite', 'book suite paradise', 'cochrane', 'cochrane james', 'cochrane james shapleigh', 'feds hot', 'feds hot seat']\n",
      "Topic 386:  ['agaist', 'agaist insulting', 'agaist insulting attacks', 'atheism ridiculous', 'atheism ridiculous counterproductive', 'atheists failing', 'atheists failing miserably', 'atheists newsgroup bacause', 'attacks like', 'attacks like like']\n",
      "Topic 387:  ['assume really', 'assume really understands', 'belief saying', 'belief saying things', 'bible order', 'bible order truth', 'biblical passages challenged', 'biblical things', 'biblical things differently', 'body theology']\n",
      "Topic 388:  ['actually cause', 'actually cause mean', 'allowed rules', 'allowed rules omnipotence', 'break god', 'break god does', 'cause mean', 'cause mean like', 'certainly omnipotence', 'certainly omnipotence saying']\n",
      "Topic 389:  ['000 000 inquistions', '000 riots', '000 riots india', '000 000', '000 inquistions america', '000 massacares bangladesh', '000 massacares', '000 civil', '000 000 riots', '1947']\n",
      "Topic 390:  ['accept blame', 'accept blame fatal', 'acceptable likely', 'acceptable likely innocent', 'acceptable similarly', 'acceptable similarly person', 'accepted blame', 'accepted blame ve', 'accepts member', 'accepts member society']\n",
      "Topic 391:  ['1st place', '1st place thing', 'abbreviated', 'abbreviated amorc', 'abbreviated amorc thing', 'amorc didn', 'amorc didn learn', 'amorc thing', 'amorc thing rosicrucian', 'claims descendant']\n",
      "Topic 392:  ['bargin', 'bargin brought', 'bargin brought court', 'batf ones', 'batf ones charge', 'believe independent', 'believe independent negotiators', 'brought court', 'brought court negotiators', 'charge original']\n",
      "Topic 393:  ['abstract ideal', 'abstract ideal strive', 'actually exist', 'actually exist talk', 'alternative therapy', 'alternative therapy respond', 'bohr told', 'bohr told objective', 'consider useful', 'consider useful fiction']\n",
      "Topic 394:  ['accuracy biblical', 'accuracy biblical claims', 'answer charley', 'answer charley challenges', 'argument case', 'argument case wrong', 'biblical claims', 'biblical claims existence', 'case wrong think', 'certain fools']\n",
      "Topic 395:  ['busy saved', 'busy saved frank', 'buy opinions', 'buy opinions causing', 'causing said', 'causing said opinions', 'folks say', 'folks say know', 'forgot ll', 'forgot ll try']\n",
      "Topic 396:  ['argument like know', 'based significance', 'based significance argument', 'based sufficiently', 'based sufficiently familiar', 'basis kind', 'basis kind evolutionary', 'belief air', 'beliefs raise', 'beliefs raise topic']\n",
      "Topic 397:  ['argument like know', 'based significance', 'based significance argument', 'based sufficiently', 'based sufficiently familiar', 'basis kind', 'basis kind evolutionary', 'belief air', 'beliefs raise', 'beliefs raise topic']\n",
      "Topic 398:  ['15 orphaic docterians', 'lewb', '1922 easly', '1922 easly draw', 'add percentages', 'add percentages 13', 'brought lewb', 'brought lewb group', 'cambridge', 'cambridge press']\n",
      "Topic 399:  ['748', '748 grand', '748 grand lodge', 'babtist', 'babtist beliefs', 'babtist beliefs teachings', 'babtized', 'babtized southern', 'babtized southern babtist', 'beliefs teachings']\n",
      "Topic 400:  ['boy finds rb', 'comes rb', 'comes rb later', 'finds rb', 'finds rb priest', 'rb', 'rb later', 'rb later bright', 'rb praying', 'rb praying young']\n",
      "Topic 401:  ['agree sad', 'agree sad saga', 'attribute separates', 'attribute separates groups', 'bosnia combination', 'bosnia combination ethnical', 'bosnia terrible', 'bosnia terrible example', 'bosnian', 'bosnian looks']\n",
      "Topic 402:  ['actually ridiculous', 'actually ridiculous considering', 'actually turn adultress', 'adultress', 'adultress liar', 'adultress liar cause', 'believe tells truth', 'bought mary', 'bought mary story', 'cause mankind']\n",
      "Topic 403:  ['acceptable exactly', 'acceptable exactly don', 'active tacit', 'active tacit support', 'anti semitism perfectly', 'behaved casual', 'behaved casual anti', 'casually', 'casually trashed', 'casually trashed thousand']\n",
      "Topic 404:  ['000 years ago', '10 000 years', 'ago evidence', 'ago evidence showed', 'believe earth', 'believe earth created', 'created 10', 'created 10 000', 'don observe', 'don observe things']\n",
      "Topic 405:  ['advantage mary', 'advantage mary way', 'arriving home', 'arriving home joseph', 'believed came', 'believed came jesus', 'born violence', 'born violence dave', 'came jesus', 'came jesus child']\n",
      "Topic 406:  ['actually believing racism', 'belief belief carries', 'belief carries', 'belief carries psychological', 'believe just', 'believe just try', 'believing racism', 'believing racism course', 'benefits gained', 'benefits gained racism']\n",
      "Topic 407:  ['abuse power', 'abuse power god', 'according description', 'care fine', 'care fine idea', 'choice satan', 'choice satan explain', 'comes possibility', 'comes possibility abuse', 'concerned tyrant']\n",
      "Topic 408:  ['babble', '_pink_unicorns_', '_pink_unicorns_ granted', '_pink_unicorns_ granted faith', '_the_users_guide_to_invisible_', '_the_users_guide_to_invisible_ _pink_unicorns_', '_the_users_guide_to_invisible_ _pink_unicorns_ granted', '_the_wholly_babble', '_the_wholly_babble _the_users_guide_to_invisible_', '_the_wholly_babble _the_users_guide_to_invisible_ _pink_unicorns_']\n",
      "Topic 409:  ['elaborate', 'absurd things', 'absurd things little', 'admit laugh', 'admit laugh stuff', 'aren sort', 'aren sort extended', 'ask posts', 'ask posts aren', 'bobby question']\n",
      "Topic 410:  ['actual evidence', 'actual evidence probably', 'agree jesus', 'agree jesus seen', 'arguments looks', 'arguments looks passed', 'case gospels', 'case gospels agree', 'charlie claims', 'charlie claims jesus']\n",
      "Topic 411:  ['borderline', 'borderline supernatural', 'borderline supernatural insights', 'change world', 'change world wasn', 'christmas don', 'christmas don know', 'communist god', 'communist god man', 'don know fanatics']\n",
      "Topic 412:  ['abstinence', 'abstinence homosexuality', 'abstinence homosexuality isn', 'animal cause', 'animal cause success', 'cause success', 'cause success isn', 'destructive worst', 'destructive worst neutral', 'difficult humans']\n",
      "Topic 413:  ['bustingly', 'bustingly funny', 'bustingly funny parody', 'colon', 'colon use', 'colon use text', 'copy friendly', 'copy friendly neighbourhood', 'friendly neighbourhood', 'friendly neighbourhood subgenius']\n",
      "Topic 414:  ['abstract thought', 'abstract thought matter', 'aggressive', 'aggressive developed', 'aggressive developed brains', 'attack humans', 'attack humans unless', 'attacked inclined', 'attacked inclined blame', 'blame dolphins']\n",
      "Topic 415:  ['admit rosicrucian', 'admit rosicrucian enjoy', 'aurae', 'aurae crucis', 'aurae crucis ruby', 'competing rc', 'competing rc orders', 'course real', 'course real rosicrucians', 'cross rough']\n",
      "Topic 416:  ['7th day adventist', 'adventist', 'adventist particular', 'adventist particular just', 'bad sign', 'bad sign don', 'began scratching', 'began scratching head', 'case christianity 7th', 'christianity 7th']\n",
      "Topic 417:  ['avail caused', 'avail caused problems', 'better don', 'better don think', 'better motto', 'better motto likely', 'books avail', 'books avail caused', 'caused problems', 'caused problems shown']\n",
      "Topic 418:  ['accept magical', 'accept magical order', 'answers easy', 'answers easy questions', 'answers join', 'answers join pay', 'bandwidth', 'bandwidth gods', 'bandwidth gods im', 'constitution sacrificed']\n",
      "Topic 419:  ['ask canaanites', 'ask canaanites point', 'attempt ok', 'attempt ok past', 'believe ordered', 'believe ordered eliminate', 'canaanites', 'canaanites point', 'canaanites point believe', 'correct haven']\n",
      "Topic 420:  ['10 million', '20 30', '20 30 million', '30 million', '30 million socialism', 'area 12', 'area 12 disease', 'bombay', 'bombay iodia', 'bombay iodia sp']\n",
      "Topic 421:  ['act tours', 'act tours eastern', 'birthname', 'birthname saul', 'birthname saul evangeline', 'bob box', 'bob box bob', 'bob dobbs numero', 'bob kills', 'bob kills mon']\n",
      "Topic 422:  ['actions period', 'actions period yes', 'amorc personal', 'amorc personal ok', 'ancient rosae', 'ancient rosae crucis', 'arc', 'arc ancient', 'arc ancient rosae', 'box tx']\n",
      "Topic 423:  ['bee', 'bee dance', 'behaviour died', 'behaviour died left', 'behaviour entirely', 'behaviour entirely chance', 'behaviour natural', 'behaviour natural morality', 'case didn notice', 'chance going']\n",
      "Topic 424:  ['capable reasoning', 'capable reasoning talking', 'care care', 'care care don', 'care don', 'care don care', 'contrary islamic', 'contrary islamic law', 'depends sense', 'depends sense excuse']\n",
      "Topic 425:  ['brothers pray', 'brothers pray thing', 'closed sod', 'closed sod mangoe', 'comparison current', 'comparison current hebrew', 'copies tell', 'copies tell king', 'cs umd', 'cs umd edu']\n",
      "Topic 426:  ['agent drag', 'agent drag women', 'alive better', 'alive better run', 'arriving good', 'arriving good indication', 'better run', 'better run building', 'building case fbi', 'burned instead']\n",
      "Topic 427:  ['girl called', 'girl called mercedes', 'ambiguous writing', 'ambiguous writing carefully', 'called mercedes', 'called mercedes cars', 'called mercedes girl', 'car called', 'car called mercedes', 'car entirely']\n",
      "Topic 428:  ['acknowledgement think', 'acknowledgement think gods', 'america list', 'america list goes', 'ammunition rid', 'ammunition rid guns', 'anthem changed', 'anthem changed god', 'approve motto', 'approve motto removing']\n",
      "Topic 429:  ['answer child', 'answer child parents', 'burying', 'burying truth', 'burying truth order', 'carerr', 'carerr doing', 'carerr doing greatly', 'caused thing', 'caused thing desire']\n",
      "Topic 430:  ['1994', '1994 maddi', '1994 maddi know', '1997', '1997 rest', '1997 rest best', 'best straighten', 'best straighten time', 'connor returned', 'connor returned set']\n",
      "Topic 431:  ['answer child', 'answer child parents', 'burying', 'burying truth', 'burying truth order', 'carerr', 'carerr doing', 'carerr doing greatly', 'caused thing', 'caused thing desire']\n",
      "Topic 432:  ['1qi83b', '1qi83b ec4', '1qi83b ec4 horus', 'claim just say', 'claim longer', 'claim longer stand', 'claimed killing', 'claimed killing religiously', 'contradiction paragraph', 'contradiction paragraph sorry']\n",
      "Topic 433:  ['_claims_', '_claims_ believer', '_claims_ believer _is_', '_is_ believer', '_is_ believer necessarily', '_know_ _real_', '_know_ _real_ believer', '_real_', '_real_ believer', '_real_ believer faking']\n",
      "Topic 434:  ['branch davidians organized', 'btw david koresh', 'christ btw', 'christ btw david', 'christ claimed', 'christ claimed god', 'christian faith stands', 'christians branch', 'christians branch davidians', 'claimed god']\n",
      "Topic 435:  ['attempted justify', 'attempted justify fatwa', 'best quite', 'best quite disingenuous', 'care dig', 'care dig read', 'deny islamic', 'deny islamic teaching', 'dig read', 'dig read posting']\n",
      "Topic 436:  ['attempted justify', 'attempted justify fatwa', 'best quite', 'best quite disingenuous', 'care dig', 'care dig read', 'deny islamic', 'deny islamic teaching', 'dig read', 'dig read posting']\n",
      "Topic 437:  ['assembled', 'proteins', 'proteins assembled', 'abiogenesis', 'abiogenesis posting', 'abiogenesis posting aware', 'acids', 'acids current', 'acids current thinking', 'amino']\n",
      "Topic 438:  ['absolute objective morals', 'ancient religious', 'ancient religious doctrines', 'apply empirical', 'apply empirical measurements', 'based social', 'based social rules', 'common single', 'common single denominator', 'creative']\n",
      "Topic 439:  ['according fondly', 'according fondly imagined', 'assumption interchangeable', 'assumption interchangeable atoms', 'atoms individual', 'atoms individual human', 'certainly despised', 'certainly despised living', 'closer imagine', 'closer imagine certainly']\n",
      "Topic 440:  ['background heavy', 'background heavy christian', 'basic understandings', 'basic understandings christianity', 'believers people', 'believers people don', 'bliss cheers', 'bliss cheers kent', 'christian believers', 'christian believers people']\n",
      "Topic 441:  ['american god', 'american god fearing', 'currency desert', 'currency desert brat', 'dangerous does', 'dangerous does good', 'dangerous tells', 'dangerous tells world', 'describing norm', 'describing norm infact']\n",
      "Topic 442:  ['aren difference', 'aren difference tep', 'arts major', 'arts major weren', 'atoms krypton', 'atoms krypton xenon', 'atoms trees', 'atoms trees electrons', 'deleted liberal', 'deleted liberal arts']\n",
      "Topic 443:  ['allah god', 'allah god unknowable', 'allah presume', 'allah presume allah', 'allow interpret', 'allow interpret mean', 'bible say god', 'claiming know', 'claiming know concrete', 'concrete fact']\n",
      "Topic 444:  ['edge sword', '20 13', '20 13 lord', '21 utterly', '21 utterly destroyed', 'ass edge', 'ass edge sword', 'bith', 'bith man', 'bith man women']\n",
      "Topic 445:  ['argued irrelevent', 'argued irrelevent issue', 'bed don', 'bed don mean', 'believe claim', 'believe claim headed', 'blue certainly', 'blue certainly need', 'books room', 'books room blue']\n",
      "Topic 446:  ['13 lord thy', '20 13', '20 13 lord', '21 utterly', '21 utterly destroyed', 'ass edge', 'ass edge sword', 'bith', 'bith man', 'bith man women']\n",
      "Topic 447:  ['actualization subjective', 'actualization subjective think', 'animals came', 'animals came revisions', 'animals different', 'animals different needs', 'assuming ideals', 'assuming ideals carried', 'bit complicated', 'bit complicated law']\n",
      "Topic 448:  ['anthem don', 'anthem don come', 'changed personally', 'changed personally concerned', 'chatting', 'chatting ones', 'chatting ones refuse', 'come daily', 'come daily nearly', 'concerned anthem']\n",
      "Topic 449:  ['narrow', 'mindedness', 'narrow mindedness', 'amused', 'amused determined', 'amused determined ignorance', 'appreciate amused', 'appreciate amused determined', 'appreciate narrow', 'appreciate narrow mindedness']\n",
      "Topic 450:  ['accept deal', 'accept deal offered', 'america great', 'appeal objective', 'appeal objective values', 'asking accept', 'asking accept deal', 'becasue', 'becasue know', 'becasue know think']\n",
      "Topic 451:  ['cheers david', 'cheers david religion', 'copy point', 'copy point reply', 'couple threads', 'couple threads moment', 'having trouble receiving', 'mail copy', 'mail copy point', 'moment overloaded']\n",
      "Topic 452:  ['bound christian', 'bound christian cheers', 'bound classical', 'bound classical lewis', 'brain filter', 'brain filter does', 'christian cheers', 'christian cheers kent', 'classical lewis', 'classical lewis notion']\n",
      "Topic 453:  ['answer people', 'answer people questions', 'ask believeit', 'ask believeit preaching', 'atheists newsgroup say', 'beleve', 'beleve ask', 'beleve ask believeit', 'beliefs comes', 'beliefs comes post']\n",
      "Topic 454:  ['action imitated', 'action imitated parrot', 'alternatives right', 'alternatives right posting', 'commenting pulchritude', 'commenting pulchritude polly', 'consider alternatives right', 'exactly short', 'exactly short memory', 'examples exactly']\n",
      "Topic 455:  ['27 fundamental', '27 fundamental beliefs', 'appreciate touch', 'appreciate touch going', 'asked post', 'asked post list', 'beliefs warn', 'beliefs warn long', 'best far', 'church 27']\n",
      "Topic 456:  ['accomplice murder', 'accomplice murder didn', 'action example', 'action example driver', 'car held accomplice', 'charge individual', 'charge individual grounds', 'crime criminal', 'crime criminal charge', 'criminal charge']\n",
      "Topic 457:  ['advised unwelcome', 'advised unwelcome city', 'believe letters', 'believe letters mean', 'brush dust', 'brush dust feet', 'cares topic', 'cares topic write', 'city brush', 'city brush dust']\n",
      "Topic 458:  ['book cheers', 'book cheers kent', 'chance maybe', 'chance maybe end', 'connected osiris', 'connected osiris virtual', 'cult gods resurrected', 'database', 'database time', 'database time write']\n",
      "Topic 459:  ['argument disgusting', 'argument disgusting ideas', 'choose possibility', 'choose possibility suits', 'contradictory statements', 'contradictory statements choose', 'cozy', 'cozy protection', 'cozy protection religion', 'crimes repeating']\n",
      "Topic 460:  ['argument disgusting', 'argument disgusting ideas', 'choose possibility', 'choose possibility suits', 'contradictory statements', 'contradictory statements choose', 'cozy', 'cozy protection', 'cozy protection religion', 'crimes repeating']\n",
      "Topic 461:  ['_never_', '_never_ said', '_never_ said depression', '_solely_', '_solely_ extra', '_solely_ extra marital', 'cause cause recognize', 'cause prime', 'cause prime cause', 'cause recognize']\n",
      "Topic 462:  ['washed', 'washed blood', 'animal preferably', 'animal preferably endangered', 'blood bull netland', 'blood lamb mithraist', 'bull netland', 'bull netland process', 'bull reserved', 'bull reserved choose']\n",
      "Topic 463:  ['alt religion feel', 'bob dylan', 'bob dylan happy', 'chur', 'chur process', 'chur process notes', 'day life wouldn', 'discussion alt', 'discussion alt religion', 'dylan']\n",
      "Topic 464:  ['book cheers', 'book cheers kent', 'chance maybe', 'chance maybe end', 'connected osiris', 'connected osiris virtual', 'cult gods resurrected', 'database', 'database time', 'database time write']\n",
      "Topic 465:  ['book cheers', 'book cheers kent', 'chance maybe', 'chance maybe end', 'connected osiris', 'connected osiris virtual', 'cult gods resurrected', 'database', 'database time', 'database time write']\n",
      "Topic 466:  ['allowed constitution founding', 'clean way', 'clean way die', 'consider cruel', 'consider cruel unusual', 'considered cruel definition', 'constitution founding', 'constitution founding fathers', 'cruel definition', 'cruel definition allowed']\n",
      "Topic 467:  ['000 bc', '1800s', '1800s long', '1800s long crowley', '20 000', '20 000 bc', 'amorc yes', 'amorc yes know', 'bc lizard', 'carbon dated']\n",
      "Topic 468:  ['000 bc', '1800s', '1800s long', '1800s long crowley', '20 000', '20 000 bc', 'amorc yes', 'amorc yes know', 'bc lizard', 'carbon dated']\n",
      "Topic 469:  ['automobile', 'automobile certainly', 'automobile certainly don', 'better removing', 'better removing death', 'britain countries', 'britain countries tried', 'certainly don', 'certainly don use', 'choice ways']\n",
      "Topic 470:  ['1987', '1987 right', '1987 right switched', '1988 89', '1988 89 ve', '50 messages', '50 messages time', '89 ve', '89 ve probably', 'alt atheism 1988']\n",
      "Topic 471:  ['amend', 'amend things', 'amend things described', 'authority contradictory', 'authority contradictory property', 'benedikt rosenau', 'benedikt rosenau writes', 'come plainly', 'come plainly false', 'contradictory property']\n",
      "Topic 472:  ['alt atheism didn', 'article sign', 'article sign nickname', 'atheism didn', 'atheism didn know', 'awarded', 'awarded nickname', 'awarded nickname buckminster', 'buckminster', 'buckminster fuller']\n",
      "Topic 473:  ['bible cd', 'bible cd don', 'break existing', 'break existing jewish', 'cd', 'cd don', 'cd don cd', 'cd rom', 'cd rom time', 'christians use saturday']\n",
      "Topic 474:  ['affiliation going', 'affiliation going say', 'arguing legit', 'arguing legit golden', 'argument reflecting', 'argument reflecting affiliation', 'crucis quakertown', 'crucis quakertown penn', 'dawn talking', 'dawn talking just']\n",
      "Topic 475:  ['argument bad', 'argument bad situation', 'argument people', 'argument people did', 'arguments used people', 'attacks used', 'attacks used argument', 'bad situation muslims', 'bad situation party', 'cause bad']\n",
      "Topic 476:  ['analyses matthew', 'analyses matthew luke', 'bit careful', 'bit careful gospels', 'careful gospels', 'careful gospels tell', 'common source', 'common source influenced', 'contrary share', 'contrary share material']\n",
      "Topic 477:  ['true muslims', 'bad situation', 'argument bad', 'argument bad situation', 'argument people', 'argument people did', 'arguments used people', 'attacks used', 'attacks used argument', 'bad situation muslims']\n",
      "Topic 478:  ['alt atheism didn', 'article sign', 'article sign nickname', 'atheism didn', 'atheism didn know', 'awarded', 'awarded nickname', 'awarded nickname buckminster', 'buckminster', 'buckminster fuller']\n",
      "Topic 479:  ['actually handle', 'actually handle human', 'allah cheers', 'allah cheers kent', 'corruption large', 'corruption large scale', 'don stand', 'don stand corruption', 'economical', 'economical systems']\n",
      "Topic 480:  ['anti semitism german', 'arrived social', 'arrived social ranks', 'austria distinguish', 'austria distinguish jews', 'christianity hitler', 'christianity hitler arrived', 'disenfranchisement', 'disenfranchisement jews', 'disenfranchisement jews easier']\n",
      "Topic 481:  ['arabia assuming', 'arabia assuming wouldn', 'assuming people', 'assuming people fitting', 'assuming wouldn', 'assuming wouldn know', 'bobby mozumder muslims', 'chop', 'chop people', 'chop people hands']\n",
      "Topic 482:  ['19 children died', 'armageddon start', 'armageddon start did', 'believers hoped', 'believers hoped took', 'breaking seven', 'breaking seven seals', 'children died god', 'christ believers', 'christ believers hoped']\n",
      "Topic 483:  ['centuries probably', 'centuries probably talking', 'course pull', 'course pull mozumder', 'crap course', 'crap course pull', 'defined depressed', 'defined depressed say', 'defined earlier', 'defined earlier centuries']\n",
      "Topic 484:  ['animal kingdom doesn', 'based concepts', 'based concepts practiced', 'behavior doesn', 'behavior doesn work', 'behavior likely', 'behavior likely null', 'beliefs irrelevant', 'beliefs irrelevant unless', 'cases ve']\n",
      "Topic 485:  ['appears evangelize', 'beliefs don try', 'bissel', 'bissel hasn', 'bissel hasn extended', 'courtesy ve', 'courtesy ve shown', 'deletia wrt', 'deletia wrt pathetic', 'don pie']\n",
      "Topic 486:  ['acronym', 'acronym greek', 'acronym greek isn', 'apologies dreadful', 'apologies dreadful greek', 'christ god iesus', 'christos', 'christos theos', 'christos theos ichthos', 'correct way']\n",
      "Topic 487:  ['1993 nope', '1993 nope fruitcakes', 'barring', 'barring evidence', 'barring evidence contrary', 'bunch folks', 'bunch folks children', 'children satisfy', 'children satisfy delusional', 'circa']\n",
      "Topic 488:  ['arabia assuming', 'arabia assuming wouldn', 'assuming people', 'assuming people fitting', 'assuming wouldn', 'assuming wouldn know', 'bobby mozumder muslims', 'chop', 'chop people', 'chop people hands']\n",
      "Topic 489:  ['afraid bit', 'afraid bit religion', 'award biggist', 'award biggist pile', 'award golden', 'award golden shovel', 'biggist', 'biggist pile', 'biggist pile bullshit', 'bit religion']\n",
      "Topic 490:  ['art', 'state art', 'tornadoes', 'advanced science', 'advanced science degrees', 'art equipment', 'art equipment spend', 'art study', 'art study products', 'degrees use']\n",
      "Topic 491:  ['allah allah things', 'allah does', 'allah does moslem', 'allah things', 'allah things allah', 'aren moral', 'aren moral does', 'atheism just moslems', 'believe god named', 'believes god exists']\n",
      "Topic 492:  ['angels heaven', 'angels heaven father', 'better place know', 'breaking seals', 'breaking seals dk', 'btw god', 'btw god did', 'children heaven', 'children heaven far', 'coming angels']\n",
      "Topic 493:  ['_looks_', '_looks_ like', '_looks_ like myth', 'antiquity tries', 'antiquity tries does', 'colorful', 'colorful story', 'colorful story antiquity', 'convinced know', 'convinced know mean']\n",
      "Topic 494:  ['alien does', 'alien does make', 'animals survive suppose', 'contradicting pretty', 'contradicting pretty massive', 'does make moral', 'don think ve', 'exterminate', 'exterminate species', 'exterminate species terrestrial']\n",
      "Topic 495:  ['absolute morality blurb', 'admits moral', 'admits moral relativist', 'ago desert', 'ago desert brat', 'aha admits', 'aha admits moral', 'better start', 'better start posting', 'blurb']\n",
      "Topic 496:  ['according does', 'according does evil', 'according zlumber', 'according zlumber muslim', 'athiesm members', 'athiesm members claim', 'bobby really', 'bobby really need', 'claim muslims', 'claim muslims stories']\n",
      "Topic 497:  ['account course', 'account course case', 'application rules', 'application rules language', 'case does equal', 'case doesn', 'case doesn imply', 'case equal', 'case equal person', 'context account']\n",
      "Topic 498:  ['assume know', 'assume know ezekiel', 'bible assume', 'bible assume know', 'explicitly stated', 'explicitly stated bible', 'ezekiel indirectly', 'ezekiel indirectly mentioned', 'ezekiel mad', 'ezekiel mad landlord']\n",
      "Topic 499:  ['able wives', 'able wives couldn', 'adam begat', 'adam begat seth', 'adam eve humans', 'begat seth', 'begat seth years', 'begat sons', 'begat sons daughters', 'beings cain']\n"
     ]
    }
   ],
   "source": [
    "terms = news_tf.get_feature_names_out()\n",
    "topics = []\n",
    "for index, component in enumerate(used_tsvd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:10]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Results\n",
    "\n",
    "Using LSA is a good way to condense our feature set that is often extremely large and extremely sparse, especially when we are dealing a large dataset, as is common with NLP. \n",
    "\n",
    "Simple applications in which this technique is used are documented clustering in text analysis, recommender systems, and information retrieval. More detailed use-cases of topic modeling are:\n",
    "<ul>\n",
    "<li> <b>Resume Summarization:</b> It can help recruiters to evaluate resumes by a quick glance. They can reduce effort in filtering pile of resume.\n",
    "<li> <b>Search Engine Optimization:</b> online articles, blogs, and documents can be tag easily by identifying the topics and associated keywords, which can improve optimize search results.\n",
    "<li> <b>Recommender System Optimization:</b> recommender systems act as an information filter and advisor according to the user profile and previous history. It can help us to discover unvisited relevant content based on past visits.\n",
    "<li> <b>Improving Customer Support:</b> Discovering relevant topics and associated keywords in customer complaints and feedback for examples product and service specifications, department, and branch details. Such information help company to directly rotated the complaint in respective department.\n",
    "<li> <b>Healthcare Industry:</b> topic modeling can help us to extract useful and valuable information from unstructured medical reports. This information can be used for patients treatment and medical science research purpose.\n",
    "</ul>\n",
    "\n",
    "In general, non-neural network approaches to NLP tend to be present in areas where we need to be able to process text quickly, without lots of processing. Spam filters are the classic example - we need to say yes or no, without spending ages to do so or burdening an email service with lots of processing. The examples above are similar - we are trying to draw a simple-ish conclusion. This is also somewhere that our old friend Bayes and his classifiers are most commonly seen - they are very fast at generating predictions once trained, so for something like emails, that's likely to be a good choice.\n",
    "\n",
    "An important concept from this example is the idea of condensing multiple features down into a smaller feature set while attempting to maintain the information in the original, that is something we'll revisit with Principal Component Analysis (PCA), a similar technique that is more generally applicable, later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Classification\n",
    "\n",
    "In addition to calculating things solely directly from our data, we can also use some external tools that can help create embeddings that are a little better (hopefully). This is also a neural network running behind the scenes to help us out. Word2Vec is an algorithm made by Google that can help process text and produce embeddings. Word2Vec looks for associations of words that occur with each other. This is an excellent illustrated description of Word2Vec: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "The key difference in what we will do here versus the previous examples is that we will have a more elaborate representation, or embedding, for our tokens. Rather than using something simple like the count or the td-idf values, these embeddings will represent each token with a multidimensional value. In plain language, we'll \"measure\" each token on many (here 20 or more, in real applications it can be thousands or more) dimensions. These values define what each token \"means\", and since that meaning is represented as a number we can allow our models to calculate things with those meanings. For example, we can see which words are similar to each other by seeing if they have similar embedding values. \n",
    "\n",
    "### Word2Vec in Process\n",
    "\n",
    "Word2Vec generates its embeddings by looking at words in a sentence, and the surrounding words in that same sentence. This differs quite a bit from the data that we've generated with the vectorization, as this model is better able to capture the structure of a sentence, beyond only looking at the individual words. We will use word2vec for a couple of different things:\n",
    "<ul>\n",
    "<li> Primarily, we'll use word2vec in a \"two model\" sequence to set us up to do classifications. The word2vec model will replace the count/tf-idf scores that we previously used for our feature set with embeddings that it calculates as the w2v model trains. The w2v model is \"learning\" how to represent words with numbers, in this case dimensions in a multidimensional space.\n",
    "    <ul>\n",
    "    <li> The w2v training is what creates the N-dimension measurements of each token, those then feed into our feature set for our modelling. \n",
    "    </ul>\n",
    "<li> After the word2vec model is created, we can do things like check the similarity of words. \n",
    "</ul>\n",
    "\n",
    "<b>The Word2Vec model is a neural network predictive model. We don't use it to \"predict\", we are using it to predict the embedding value of the words in our text. The model is trained to predict the context of a word, given the word itself. The embeddings are the weights of the model, and they are what we use to represent the words in our text. We use this smart model to generate the numerical representation of our text, then we use the numerical representations in our models that classify or whatever else we want to do. The Word2Vec part replaces the count vectorization process with a far more elaborate process that does the same thing, only (hopefully) far better. </b>\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "Gensim is a package that we can install that has an implementation of Word2Vec that we can use pretty easily. This part just downloads some of the stuff we'll need, like stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /Users/akeem/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package stopwords to /Users/akeem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/akeem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/akeem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "nltk.download('all')\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Since we are not using the vecorizer from sklearn, we need to provide our own tokenization. We can use the nltk based one from last time. We can also do any other types of processing here that we may want - stemming, customized stop words, etc... For this one I chopped out any 1 character tokens and added a regex filter to get rid of punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/ipykernel_13331/3132424846.py:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  tok = re.sub('\\W+','', tok) #Punctuation strip\n"
     ]
    }
   ],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clan Text - Tokenize and Lemmatize\n",
    "\n",
    "Prep some data. The \"second half\" of the dataframe is what we can use with the Word2Vec prediction models - we have cleaned up lists of tokens as well as translating the targets to 1 and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, oni]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, early, hor, already, say]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, nt, think, go, usf, life, around, though]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  target2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, g...    False  \n",
       "1                        [Ok, lar, Joking, wif, oni]    False  \n",
       "2  [Free, entry, wkly, comp, win, FA, Cup, final,...     True  \n",
       "3               [dun, say, early, hor, already, say]    False  \n",
       "4    [Nah, nt, think, go, usf, life, around, though]    False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: tok(x))\n",
    "df[\"target2\"] = pd.get_dummies(df[\"target\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec Embeddings\n",
    "\n",
    "Now comes the word2vec model - instead of taking our clean data and counting it to extract features, we can train our Word2Vec model with our cleaned up data and the output of that model is our set of features. This will have Word2Vec do its magic behind the scenes and perform the training. W2V works in one of two ways, which are roughly opposites of each other, when doing this training:\n",
    "<ul>\n",
    "<li> Continuous Bag of Words: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at words surrounding target to try to predict it. \n",
    "</ul>\n",
    "\n",
    "We'll revisit the details of some of this stuff later on when we look at neural networks, since W2V is a neural network algorithm, it will make more sense in context. \n",
    "\n",
    "<b>Note:</b> this training is not making a model that we are using to make predictions. This is training inside the W2V algorithm to generate representations of our tokens. \n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "The embeddings that we are generating are vectors that represent the words in our text. We can look at the embeddings for a word to see what they look like, but they aren't comprehensible to humans. Our count vectors or the td-idf calculations we made previously are also embeddings, those are just far more simple. Word2Vec will generate embeddings that attempt to group words that are similar together in multidimensional space. We can look at a simple example in 2D:\n",
    "\n",
    "![Similarity](images/similarity.png \"Similarity\")\n",
    "\n",
    "The values here aren't calculated, they are chosen arbitrarily, but each word is represented here in two dimensions - x and y. Words that are similar in meaning should be close to each other in the vector representation, such as \"King\" and \"Queen\". Words that are not similar should be far apart, such as \"King\" and \"Rutabaga\". The embeddings that word2vec will generate from our data as a result of the training below will aim to represent each word in 200 dimension space. We feed the word2vec model our tokens, and it will generate a N-dimension vector for each token. We can use comparisons in this N dimensional space to determine how similar two words are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import inspect\n",
    "  \n",
    "# use signature()\n",
    "print(inspect.signature(Word2Vec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model Architecture\n",
    "\n",
    "We've mentioned that the word2vec model we are making is a neural network. Neural networks, as we'll see later, have an architecture, or basically a size and design. The details don't matter too much to us yet, but one thing that we can change when determining our model in word2vec is that architecture - we can choose between CBOW and Skip-Gram. These two options are roughly opposites of each other. The details of how they work and how they differ are neural network details, so we'll set those details aside for now.\n",
    "\n",
    "<b>Continuous Bag of Words</b></br>\n",
    "![CBOW](images/cbow.webp \"CBOW\")\n",
    "\n",
    "<b>Skip-gram</b><br>\n",
    "![Skip-Gram](images/skip_gram.webp \"Skip-Gram\")\n",
    "\n",
    "These two models look like mirror images of each other, but what do they mean? Each does the same thing, though in a slightly different way. \n",
    "<ul>\n",
    "<li> CBOW: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at a word and tries to predict the surrounding words.\n",
    "</ul>\n",
    "\n",
    "For us, we can ignore the details of the difference and think of the two options similarly to other options like regularization or entropy/gini. The way the internal neural network learns is different in the different architectures. \n",
    "\n",
    "#### Which to Use?\n",
    "\n",
    "For the most part, the real answer is our favorite one - test and choose the best. In general:\n",
    "<ul>\n",
    "<li> Skip Gram tends to work well with small amount of data and is found to represent rare words well.\n",
    "<li> CBOW is normally faster and has better representations for more frequent words.\n",
    "</ul>\n",
    "\n",
    "Parameters other than the ones we have listed here can be tweaked, but we'll somewhat ignore them for now, we're ok with the defaults. The \"sg\" parameter is the one that controls the architecture - 1 is skip-gram, 0 is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(df['clean_text'],min_count=3, vector_size=200, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "Each word in the vocabulary now has a vector representing it - of size 200. We can make a dataframe and see each token in our text and its vector representation. This vector is the internal representation of each token that is generated by Word2Vec. This is how the algorithm calculates things like similarity...\n",
    "\n",
    "The word2vec result that we are printing out here is each word in our vocabulary and its vector representation - or all of its dimensions in the 200D space we created while the model was trained. \n",
    "\n",
    "### Embedding Values\n",
    "\n",
    "These values are important to understand - they aren't part of the model we are making, they are a numerical representation of the value of each token. In effect, each 200 dimension vector is comparable to an actual measurement in space. Each token \"is\" that set of 200 values, and tokens with similar vector values are similar to each other. \n",
    "\n",
    "In large language models this is a very important part of the model's understanding of language and its ability to generate sensible text. The model knows that it needs a token that has a certain value, and it can \"look\" in that area to find which words match what it is looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>0.024051</td>\n",
       "      <td>0.023249</td>\n",
       "      <td>0.077993</td>\n",
       "      <td>0.131591</td>\n",
       "      <td>0.141001</td>\n",
       "      <td>-0.314535</td>\n",
       "      <td>-0.030842</td>\n",
       "      <td>0.254113</td>\n",
       "      <td>-0.123129</td>\n",
       "      <td>0.183461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156797</td>\n",
       "      <td>-0.037821</td>\n",
       "      <td>-0.039748</td>\n",
       "      <td>-0.076138</td>\n",
       "      <td>0.096395</td>\n",
       "      <td>0.240227</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>-0.237828</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>-0.061977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt</th>\n",
       "      <td>0.100774</td>\n",
       "      <td>-0.074504</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.072937</td>\n",
       "      <td>0.084751</td>\n",
       "      <td>-0.035625</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.228953</td>\n",
       "      <td>-0.065756</td>\n",
       "      <td>0.060684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115826</td>\n",
       "      <td>-0.049476</td>\n",
       "      <td>-0.072408</td>\n",
       "      <td>-0.131961</td>\n",
       "      <td>0.048464</td>\n",
       "      <td>-0.031391</td>\n",
       "      <td>0.075936</td>\n",
       "      <td>-0.212056</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.029546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.101085</td>\n",
       "      <td>-0.049436</td>\n",
       "      <td>0.052137</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.113158</td>\n",
       "      <td>-0.106115</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>0.236291</td>\n",
       "      <td>-0.104019</td>\n",
       "      <td>0.119635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>-0.035770</td>\n",
       "      <td>-0.057609</td>\n",
       "      <td>-0.063949</td>\n",
       "      <td>0.085575</td>\n",
       "      <td>0.062909</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>-0.229577</td>\n",
       "      <td>-0.005896</td>\n",
       "      <td>-0.049940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.142111</td>\n",
       "      <td>0.063602</td>\n",
       "      <td>0.152496</td>\n",
       "      <td>0.253718</td>\n",
       "      <td>0.247413</td>\n",
       "      <td>-0.052125</td>\n",
       "      <td>0.344011</td>\n",
       "      <td>0.155928</td>\n",
       "      <td>-0.323852</td>\n",
       "      <td>0.178582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.026177</td>\n",
       "      <td>-0.154310</td>\n",
       "      <td>0.256350</td>\n",
       "      <td>0.212640</td>\n",
       "      <td>-0.003735</td>\n",
       "      <td>-0.159637</td>\n",
       "      <td>-0.356353</td>\n",
       "      <td>0.155768</td>\n",
       "      <td>-0.079264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.086789</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.159666</td>\n",
       "      <td>0.275181</td>\n",
       "      <td>0.262385</td>\n",
       "      <td>-0.052044</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.166708</td>\n",
       "      <td>-0.326069</td>\n",
       "      <td>0.202236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.045260</td>\n",
       "      <td>-0.104895</td>\n",
       "      <td>0.255802</td>\n",
       "      <td>0.257558</td>\n",
       "      <td>-0.033552</td>\n",
       "      <td>-0.167377</td>\n",
       "      <td>-0.328850</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>-0.069883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oru</th>\n",
       "      <td>0.053998</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>0.052010</td>\n",
       "      <td>0.032429</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>-0.102781</td>\n",
       "      <td>-0.026419</td>\n",
       "      <td>0.149818</td>\n",
       "      <td>-0.069836</td>\n",
       "      <td>0.084613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068778</td>\n",
       "      <td>-0.031884</td>\n",
       "      <td>-0.021585</td>\n",
       "      <td>-0.038739</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.061819</td>\n",
       "      <td>0.019232</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>-0.014298</td>\n",
       "      <td>-0.034240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>0.047273</td>\n",
       "      <td>-0.020237</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.055349</td>\n",
       "      <td>-0.081726</td>\n",
       "      <td>-0.035850</td>\n",
       "      <td>0.130517</td>\n",
       "      <td>-0.051618</td>\n",
       "      <td>0.073805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065791</td>\n",
       "      <td>-0.024233</td>\n",
       "      <td>-0.015664</td>\n",
       "      <td>-0.034487</td>\n",
       "      <td>0.051049</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.020288</td>\n",
       "      <td>-0.108980</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>-0.023189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21st</th>\n",
       "      <td>0.055324</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>0.052287</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>0.057523</td>\n",
       "      <td>-0.099110</td>\n",
       "      <td>-0.033163</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>-0.060571</td>\n",
       "      <td>0.073612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072889</td>\n",
       "      <td>-0.023774</td>\n",
       "      <td>-0.024089</td>\n",
       "      <td>-0.039903</td>\n",
       "      <td>0.058851</td>\n",
       "      <td>0.060265</td>\n",
       "      <td>0.018081</td>\n",
       "      <td>-0.121587</td>\n",
       "      <td>-0.015064</td>\n",
       "      <td>-0.025623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cup</th>\n",
       "      <td>0.054086</td>\n",
       "      <td>-0.019621</td>\n",
       "      <td>0.052754</td>\n",
       "      <td>0.030483</td>\n",
       "      <td>0.058528</td>\n",
       "      <td>-0.093420</td>\n",
       "      <td>-0.029780</td>\n",
       "      <td>0.155919</td>\n",
       "      <td>-0.064077</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070813</td>\n",
       "      <td>-0.031300</td>\n",
       "      <td>-0.017531</td>\n",
       "      <td>-0.037896</td>\n",
       "      <td>0.060876</td>\n",
       "      <td>0.060327</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>-0.121742</td>\n",
       "      <td>-0.016344</td>\n",
       "      <td>-0.028831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Available</th>\n",
       "      <td>0.059582</td>\n",
       "      <td>-0.023112</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>0.034375</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>-0.097666</td>\n",
       "      <td>-0.035901</td>\n",
       "      <td>0.154485</td>\n",
       "      <td>-0.066927</td>\n",
       "      <td>0.076567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072680</td>\n",
       "      <td>-0.027726</td>\n",
       "      <td>-0.017158</td>\n",
       "      <td>-0.038630</td>\n",
       "      <td>0.058399</td>\n",
       "      <td>0.062114</td>\n",
       "      <td>0.014117</td>\n",
       "      <td>-0.126128</td>\n",
       "      <td>-0.018696</td>\n",
       "      <td>-0.034042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "call       0.024051  0.023249  0.077993  0.131591  0.141001 -0.314535   \n",
       "nt         0.100774 -0.074504  0.006864  0.072937  0.084751 -0.035625   \n",
       "get        0.101085 -0.049436  0.052137  0.076658  0.113158 -0.106115   \n",
       "gt         0.142111  0.063602  0.152496  0.253718  0.247413 -0.052125   \n",
       "lt         0.086789  0.099853  0.159666  0.275181  0.262385 -0.052044   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "Oru        0.053998 -0.018174  0.052010  0.032429  0.058042 -0.102781   \n",
       "2005       0.047273 -0.020237  0.041990  0.033477  0.055349 -0.081726   \n",
       "21st       0.055324 -0.016235  0.052287  0.033685  0.057523 -0.099110   \n",
       "Cup        0.054086 -0.019621  0.052754  0.030483  0.058528 -0.093420   \n",
       "Available  0.059582 -0.023112  0.056533  0.034375  0.056769 -0.097666   \n",
       "\n",
       "                6         7         8         9    ...       190       191  \\\n",
       "call      -0.030842  0.254113 -0.123129  0.183461  ...  0.156797 -0.037821   \n",
       "nt         0.008723  0.228953 -0.065756  0.060684  ...  0.115826 -0.049476   \n",
       "get       -0.006054  0.236291 -0.104019  0.119635  ...  0.118386 -0.035770   \n",
       "gt         0.344011  0.155928 -0.323852  0.178582  ...  0.004685  0.026177   \n",
       "lt         0.356600  0.166708 -0.326069  0.202236  ...  0.005152  0.045260   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "Oru       -0.026419  0.149818 -0.069836  0.084613  ...  0.068778 -0.031884   \n",
       "2005      -0.035850  0.130517 -0.051618  0.073805  ...  0.065791 -0.024233   \n",
       "21st      -0.033163  0.142643 -0.060571  0.073612  ...  0.072889 -0.023774   \n",
       "Cup       -0.029780  0.155919 -0.064077  0.077301  ...  0.070813 -0.031300   \n",
       "Available -0.035901  0.154485 -0.066927  0.076567  ...  0.072680 -0.027726   \n",
       "\n",
       "                192       193       194       195       196       197  \\\n",
       "call      -0.039748 -0.076138  0.096395  0.240227  0.057799 -0.237828   \n",
       "nt        -0.072408 -0.131961  0.048464 -0.031391  0.075936 -0.212056   \n",
       "get       -0.057609 -0.063949  0.085575  0.062909  0.040103 -0.229577   \n",
       "gt        -0.154310  0.256350  0.212640 -0.003735 -0.159637 -0.356353   \n",
       "lt        -0.104895  0.255802  0.257558 -0.033552 -0.167377 -0.328850   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "Oru       -0.021585 -0.038739  0.053065  0.061819  0.019232 -0.125044   \n",
       "2005      -0.015664 -0.034487  0.051049  0.053828  0.020288 -0.108980   \n",
       "21st      -0.024089 -0.039903  0.058851  0.060265  0.018081 -0.121587   \n",
       "Cup       -0.017531 -0.037896  0.060876  0.060327  0.021341 -0.121742   \n",
       "Available -0.017158 -0.038630  0.058399  0.062114  0.014117 -0.126128   \n",
       "\n",
       "                198       199  \n",
       "call       0.004561 -0.061977  \n",
       "nt        -0.022781 -0.029546  \n",
       "get       -0.005896 -0.049940  \n",
       "gt         0.155768 -0.079264  \n",
       "lt         0.149338 -0.069883  \n",
       "...             ...       ...  \n",
       "Oru       -0.014298 -0.034240  \n",
       "2005      -0.012589 -0.023189  \n",
       "21st      -0.015064 -0.025623  \n",
       "Cup       -0.016344 -0.028831  \n",
       "Available -0.018696 -0.034042  \n",
       "\n",
       "[3150 rows x 200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Similarity\n",
    "\n",
    "One of the things that Word2Vec allows us to do is to look at the similarity of words. This similarity is calculated via the cosine distance of the vectors. Cosine similarity is a technique to calculate the distance between two vectors - smaller distance, more similar. \n",
    "\n",
    "![Cosine Similarity](images/cosine_sim.png \"Cosine Similarity\" )\n",
    "\n",
    "Once the vectors are derived by in the training process, these similarity calculations are pretty easy and quick. \n",
    "\n",
    "<b>Note:</b> the similarities here are calculated by the values derived from our trained model. So they are based on the relationships in our text. Word2Vec and other NLP packages also commonly have pretrained models that can be downloaded that are based on large amounts of text. Words may be represented very differently in those vs whatever we train here - the more data we have, the more consistent they'll be; the more \"unique\" our text is, the more different it will be. If we were, for example, working in a specific domain such as patent law, we could use a large amount of patent law text to train a model that would be more consistent with our domain. Or, perhaps more likely, we could use a pretrained model that has been created with massive amounts of training data. \n",
    "\n",
    "### Types of Similarity\n",
    "\n",
    "When looking at the similarity of different words, we can measure that similarity in a couple of ways - lexical and semantic, that we mentioned before. Here, the model is looking at semantic similarity, the \"meaning\" of each word, in the context of our text, is being compared and the most similar words are returned. Note that we can only calculate similarity here for words that we have in our vocabulary. This is one place where large language models like chat GPT have a massive advantage, their vocabulary is huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('want', 0.9961812496185303),\n",
       " ('say', 0.9921268224716187),\n",
       " ('let', 0.991629958152771)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most similar word to anything in our vocabulary \n",
    "vectors.most_similar(\"know\")[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do phone\n",
      "0.7978207\n"
     ]
    }
   ],
   "source": [
    "# We can also see how similar different words are. \n",
    "# I will grab two arbitrary words from the vocabulary and see how similar they are.\n",
    "# you could use anything in the vocabulary here, try some other ones!\n",
    "\n",
    "word_a = tmp.columns[30]\n",
    "word_b = tmp.columns[40]\n",
    "\n",
    "print(word_a, word_b)\n",
    "print(vectors.similarity(word_a, word_b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "We can take our actual data now and transform it through the Word2Vec model that we've made. This will generate our smaller feature set that we can build our models from, one of the things that the MeanEmbeddingVectorizer does is to collapse the data down to those 200 dimensions in the vector. Our dataset is spit out the other end, each row of text is now represented by a single vector of those 200 dimensions of our embedding values from the word2vec model (the columns).\n",
    "\n",
    "#### Note - We are Flattening the High-Dimensional Data Becuase Our Models Can't Handle It... Yet\n",
    "\n",
    "If this is confusing, please ignore it, this is a bit of a tangent. The meanembeddingvectorizer thing is needed to \"flatten\" our data down from 200D to 1D for each token. This is because our models can only deal with data that is in that format (instances x features). We can't have a 200D vector for each token, we need to collapse it down to a single value. Later, when we look at neural networks, we'll see models with different architectures that can accommodate data that is multidimensional like this. That's one of the reasons that neural networks are so powerful, they can accommodate data that is multidimensional, so something like an image can be treated like an image, not just a bunch of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 200) (1393, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083838</td>\n",
       "      <td>-0.038896</td>\n",
       "      <td>0.070596</td>\n",
       "      <td>0.051307</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>-0.152345</td>\n",
       "      <td>-0.058990</td>\n",
       "      <td>0.236481</td>\n",
       "      <td>-0.090204</td>\n",
       "      <td>0.121682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117860</td>\n",
       "      <td>-0.044517</td>\n",
       "      <td>-0.028388</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>0.080772</td>\n",
       "      <td>0.102277</td>\n",
       "      <td>0.034871</td>\n",
       "      <td>-0.190461</td>\n",
       "      <td>-0.028514</td>\n",
       "      <td>-0.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084724</td>\n",
       "      <td>-0.034071</td>\n",
       "      <td>0.053061</td>\n",
       "      <td>0.057707</td>\n",
       "      <td>0.080709</td>\n",
       "      <td>-0.103224</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>0.211001</td>\n",
       "      <td>-0.083598</td>\n",
       "      <td>0.093608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096767</td>\n",
       "      <td>-0.040943</td>\n",
       "      <td>-0.041493</td>\n",
       "      <td>-0.068582</td>\n",
       "      <td>0.070424</td>\n",
       "      <td>0.049356</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>-0.175227</td>\n",
       "      <td>-0.019822</td>\n",
       "      <td>-0.035722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091977</td>\n",
       "      <td>-0.035910</td>\n",
       "      <td>0.061569</td>\n",
       "      <td>0.065552</td>\n",
       "      <td>0.098904</td>\n",
       "      <td>-0.122448</td>\n",
       "      <td>-0.021288</td>\n",
       "      <td>0.239069</td>\n",
       "      <td>-0.093585</td>\n",
       "      <td>0.097019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118172</td>\n",
       "      <td>-0.045594</td>\n",
       "      <td>-0.048554</td>\n",
       "      <td>-0.086611</td>\n",
       "      <td>0.072518</td>\n",
       "      <td>0.058745</td>\n",
       "      <td>0.041852</td>\n",
       "      <td>-0.200062</td>\n",
       "      <td>-0.017819</td>\n",
       "      <td>-0.038818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058708</td>\n",
       "      <td>-0.022536</td>\n",
       "      <td>0.042605</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>-0.086340</td>\n",
       "      <td>-0.022504</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>-0.062303</td>\n",
       "      <td>0.071361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069982</td>\n",
       "      <td>-0.030059</td>\n",
       "      <td>-0.027795</td>\n",
       "      <td>-0.046783</td>\n",
       "      <td>0.052562</td>\n",
       "      <td>0.048691</td>\n",
       "      <td>0.019873</td>\n",
       "      <td>-0.124560</td>\n",
       "      <td>-0.011352</td>\n",
       "      <td>-0.025845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076732</td>\n",
       "      <td>-0.030838</td>\n",
       "      <td>0.058608</td>\n",
       "      <td>0.049195</td>\n",
       "      <td>0.074968</td>\n",
       "      <td>-0.113593</td>\n",
       "      <td>-0.027128</td>\n",
       "      <td>0.202120</td>\n",
       "      <td>-0.084126</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090505</td>\n",
       "      <td>-0.039031</td>\n",
       "      <td>-0.033528</td>\n",
       "      <td>-0.064686</td>\n",
       "      <td>0.067273</td>\n",
       "      <td>0.057423</td>\n",
       "      <td>0.025715</td>\n",
       "      <td>-0.163988</td>\n",
       "      <td>-0.022203</td>\n",
       "      <td>-0.033683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.079298</td>\n",
       "      <td>-0.029836</td>\n",
       "      <td>0.057278</td>\n",
       "      <td>0.060703</td>\n",
       "      <td>0.083208</td>\n",
       "      <td>-0.109696</td>\n",
       "      <td>-0.014626</td>\n",
       "      <td>0.209529</td>\n",
       "      <td>-0.085183</td>\n",
       "      <td>0.094552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094769</td>\n",
       "      <td>-0.039867</td>\n",
       "      <td>-0.041994</td>\n",
       "      <td>-0.061638</td>\n",
       "      <td>0.070554</td>\n",
       "      <td>0.057182</td>\n",
       "      <td>0.023316</td>\n",
       "      <td>-0.171352</td>\n",
       "      <td>-0.013919</td>\n",
       "      <td>-0.038886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.092555</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>0.093198</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>0.157399</td>\n",
       "      <td>-0.108232</td>\n",
       "      <td>0.111416</td>\n",
       "      <td>0.214502</td>\n",
       "      <td>-0.181833</td>\n",
       "      <td>0.132523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074535</td>\n",
       "      <td>-0.016663</td>\n",
       "      <td>-0.079199</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.043958</td>\n",
       "      <td>-0.030815</td>\n",
       "      <td>-0.256353</td>\n",
       "      <td>0.043232</td>\n",
       "      <td>-0.045512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.085755</td>\n",
       "      <td>-0.037993</td>\n",
       "      <td>0.064008</td>\n",
       "      <td>0.054329</td>\n",
       "      <td>0.083881</td>\n",
       "      <td>-0.122886</td>\n",
       "      <td>-0.037361</td>\n",
       "      <td>0.216698</td>\n",
       "      <td>-0.086051</td>\n",
       "      <td>0.102835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099849</td>\n",
       "      <td>-0.040978</td>\n",
       "      <td>-0.034927</td>\n",
       "      <td>-0.068200</td>\n",
       "      <td>0.071568</td>\n",
       "      <td>0.067807</td>\n",
       "      <td>0.035290</td>\n",
       "      <td>-0.176476</td>\n",
       "      <td>-0.020675</td>\n",
       "      <td>-0.038244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0.084364</td>\n",
       "      <td>-0.030701</td>\n",
       "      <td>0.051576</td>\n",
       "      <td>0.057124</td>\n",
       "      <td>0.078498</td>\n",
       "      <td>-0.109560</td>\n",
       "      <td>-0.025587</td>\n",
       "      <td>0.218379</td>\n",
       "      <td>-0.077942</td>\n",
       "      <td>0.085083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099614</td>\n",
       "      <td>-0.041968</td>\n",
       "      <td>-0.038257</td>\n",
       "      <td>-0.074368</td>\n",
       "      <td>0.066072</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>-0.172479</td>\n",
       "      <td>-0.020009</td>\n",
       "      <td>-0.035677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0.075021</td>\n",
       "      <td>-0.032350</td>\n",
       "      <td>0.073440</td>\n",
       "      <td>0.044573</td>\n",
       "      <td>0.088301</td>\n",
       "      <td>-0.140384</td>\n",
       "      <td>-0.051352</td>\n",
       "      <td>0.204575</td>\n",
       "      <td>-0.088672</td>\n",
       "      <td>0.119068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099152</td>\n",
       "      <td>-0.036643</td>\n",
       "      <td>-0.026687</td>\n",
       "      <td>-0.053414</td>\n",
       "      <td>0.081746</td>\n",
       "      <td>0.093278</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>-0.167517</td>\n",
       "      <td>-0.022225</td>\n",
       "      <td>-0.042898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.083838 -0.038896  0.070596  0.051307  0.093584 -0.152345 -0.058990   \n",
       "1     0.084724 -0.034071  0.053061  0.057707  0.080709 -0.103224 -0.018272   \n",
       "2     0.091977 -0.035910  0.061569  0.065552  0.098904 -0.122448 -0.021288   \n",
       "3     0.058708 -0.022536  0.042605  0.041958  0.058766 -0.086340 -0.022504   \n",
       "4     0.076732 -0.030838  0.058608  0.049195  0.074968 -0.113593 -0.027128   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174  0.079298 -0.029836  0.057278  0.060703  0.083208 -0.109696 -0.014626   \n",
       "4175  0.092555  0.016768  0.093198  0.145602  0.157399 -0.108232  0.111416   \n",
       "4176  0.085755 -0.037993  0.064008  0.054329  0.083881 -0.122886 -0.037361   \n",
       "4177  0.084364 -0.030701  0.051576  0.057124  0.078498 -0.109560 -0.025587   \n",
       "4178  0.075021 -0.032350  0.073440  0.044573  0.088301 -0.140384 -0.051352   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.236481 -0.090204  0.121682  ...  0.117860 -0.044517 -0.028388   \n",
       "1     0.211001 -0.083598  0.093608  ...  0.096767 -0.040943 -0.041493   \n",
       "2     0.239069 -0.093585  0.097019  ...  0.118172 -0.045594 -0.048554   \n",
       "3     0.148800 -0.062303  0.071361  ...  0.069982 -0.030059 -0.027795   \n",
       "4     0.202120 -0.084126  0.089885  ...  0.090505 -0.039031 -0.033528   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174  0.209529 -0.085183  0.094552  ...  0.094769 -0.039867 -0.041994   \n",
       "4175  0.214502 -0.181833  0.132523  ...  0.074535 -0.016663 -0.079199   \n",
       "4176  0.216698 -0.086051  0.102835  ...  0.099849 -0.040978 -0.034927   \n",
       "4177  0.218379 -0.077942  0.085083  ...  0.099614 -0.041968 -0.038257   \n",
       "4178  0.204575 -0.088672  0.119068  ...  0.099152 -0.036643 -0.026687   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0    -0.073269  0.080772  0.102277  0.034871 -0.190461 -0.028514 -0.045906  \n",
       "1    -0.068582  0.070424  0.049356  0.032410 -0.175227 -0.019822 -0.035722  \n",
       "2    -0.086611  0.072518  0.058745  0.041852 -0.200062 -0.017819 -0.038818  \n",
       "3    -0.046783  0.052562  0.048691  0.019873 -0.124560 -0.011352 -0.025845  \n",
       "4    -0.064686  0.067273  0.057423  0.025715 -0.163988 -0.022203 -0.033683  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174 -0.061638  0.070554  0.057182  0.023316 -0.171352 -0.013919 -0.038886  \n",
       "4175  0.036123  0.135500  0.043958 -0.030815 -0.256353  0.043232 -0.045512  \n",
       "4176 -0.068200  0.071568  0.067807  0.035290 -0.176476 -0.020675 -0.038244  \n",
       "4177 -0.074368  0.066072  0.052900  0.031603 -0.172479 -0.020009 -0.035677  \n",
       "4178 -0.053414  0.081746  0.093278  0.025882 -0.167517 -0.022225 -0.042898  \n",
       "\n",
       "[4179 rows x 200 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"target2\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "print(X_train_vectors_w2v.shape, X_test_vectors_w2v.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "We now have a pretty normal dataset and can use the new data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9677390991403637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      1187\n",
      "        True       0.93      0.84      0.88       206\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.95      0.91      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_w2v = RandomForestClassifier()\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Word2Vec\n",
    "\n",
    "Use the newsgroup data and Word2Vec to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datsets and Tokenize\n",
    "tok = lemmaTokenizer(stop_words)\n",
    "X_w2v_news_train = [tok(x) for x in data_train.data]\n",
    "X_w2v_news_test = [tok(x) for x in data_test.data]\n",
    "\n",
    "y_train_news = data_train.target\n",
    "y_test_news = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benedikt',\n",
       " 'Rosenau',\n",
       " 'writes',\n",
       " 'great',\n",
       " 'authority',\n",
       " 'Contradictory',\n",
       " 'property',\n",
       " 'language',\n",
       " 'If',\n",
       " 'correct',\n",
       " 'THINGS',\n",
       " 'DEFINED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 'object',\n",
       " 'definition',\n",
       " 'reality',\n",
       " 'If',\n",
       " 'amend',\n",
       " 'THINGS',\n",
       " 'DESCRIBED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 've',\n",
       " 'come',\n",
       " 'something',\n",
       " 'plainly',\n",
       " 'false',\n",
       " 'Failures',\n",
       " 'description',\n",
       " 'merely',\n",
       " 'failure',\n",
       " 'description',\n",
       " 'objectivist',\n",
       " 'remember']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview\n",
    "X_w2v_news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model_news = Word2Vec(X_w2v_news_train, min_count=1, vector_size=200)\n",
    "w2v_news = dict(zip(model_news.wv.index_to_key, model_news.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_news_w = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_news = model_news_w.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_news = model_news_w.transform(X_w2v_news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n",
      "Confusion Matrix:\n",
      " [[300  19]\n",
      " [236  15]]\n",
      "AUC: 0.5249597222395683\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf = SVC(probability=True)\n",
    "news_clf.fit(X_train_vectors_w2v_news, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news = news_clf.predict(X_val_vectors_w2v_news)\n",
    "y_prob_news = news_clf.predict_proba(X_val_vectors_w2v_news)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test_news, y_predict_news))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_news, y_prob_news)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Models\n",
    "\n",
    "When we do the initial training of the word2vec model (not when we are making final predictions), we are using our corpus to generate the space and the embeddings for all of our tokens. We can also download a pretrained model, that already has the N-dimensional space defined (when it was trained on some different data), and use that to generate our embeddings. This is a common practice, and can be a good way to get started. Gensim has several models that have been trained on varying amounts of data, they are listed here: https://github.com/RaRe-Technologies/gensim-data along with several other datasets that we could use to train a model. \n",
    "\n",
    "The differences with using this pretrained model (or an existing corpus below) are:\n",
    "<ul>\n",
    "<li> Above, when training word2vec with our data, we used our corpus to generate the space in which the tokens are placed, then calculate those embeddings for each token. \n",
    "<li> With a pretrained model, we are using the space that was generated by the model that was trained on some other data, then placing our tokens in that space. \n",
    "</ul>\n",
    "\n",
    "So if we are using some text from wikipedia (like the second example), the space in which embeddings are made is defined by the text in wikipedia. So the \"closeness\" in meaning of words is based on what is in that corpus. We then take our tokens and calculate their embeddings in that space. The big advantage to this is someone else can train a model on lots of data, which hopefully generates a better understanding of the relationships between words, and we can then just score our words on those scales. This approach is common in large models, like text processing or image recognition, where the training load can be too large for \"regular folk\". We can also take these trained models and \"customize\" them to our data, we'll look at that with image recognition at the end of the semester. \n",
    "\n",
    "#### Use a Twitter Trained Model\n",
    "\n",
    "We can try using a pretrained model that was trained on Twitter data. This model has been pretrained, so it already knows how to represent words, we will then feed it all of our tokens, and it will generate the embeddings for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.944882869720459),\n",
       " ('baby', 0.9425430297851562),\n",
       " ('dream', 0.9267040491104126),\n",
       " ('miss', 0.924690842628479),\n",
       " ('much', 0.9215252995491028),\n",
       " ('see', 0.9197866916656494),\n",
       " ('happy', 0.9176183938980103),\n",
       " ('beautiful', 0.9173234105110168),\n",
       " ('smile', 0.9138968586921692),\n",
       " ('loves', 0.9123677611351013)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downlaod the model and do a little test\n",
    "\n",
    "import gensim.downloader as api\n",
    "model_twit = api.load(\"glove-twitter-25\")\n",
    "model_twit.most_similar(\"love\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings\n",
    "\n",
    "The model exists, so we will use it to transform our tokens into numerical representations. Then we can go use those to make classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.57      0.71       517\n",
      "           1       0.12      0.58      0.20        53\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.53      0.58      0.46       570\n",
      "weighted avg       0.86      0.58      0.66       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_twit = dict(zip(model_twit.index_to_key, model_twit.vectors))\n",
    "model_twit_emb = MeanEmbeddingVectorizer(w2v_twit)\n",
    "\n",
    "X_train_twit = model_twit_emb.transform(X_w2v_news_train)\n",
    "X_test_twit = model_twit_emb.transform(X_w2v_news_test)\n",
    "\n",
    "# Make predictions\n",
    "twit_clf = SVC(probability=True)\n",
    "twit_clf.fit(X_train_twit, y_train_news)  #model\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict_twit = twit_clf.predict(X_test_twit)\n",
    "\n",
    "print(classification_report(y_predict_twit, y_test_news))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premade Corpus\n",
    "\n",
    "We can also train a model directly from a preexisting corpus, then generate our embeddings from that model. \n",
    "\n",
    "The \"text8\" corpus is a small corpus of text that is included with gensim. It is a small subset of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "corpus = api.load('text8')\n",
    "model_corp = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corp = dict(zip(model_corp.wv.index_to_key, model_corp.wv.vectors)) \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_corp_emb = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_test)\n",
    "X_train_vectors_w2v_corp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf_corp = SVC(probability=True)\n",
    "news_clf_corp.fit(X_train_vectors_w2v_corp, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news_corp = news_clf.predict(X_val_vectors_w2v_corp)\n",
    "y_prob_news_corp = news_clf.predict_proba(X_val_vectors_w2v_corp)[:,1]\n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news_corp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Me!\n",
    "\n",
    "As we see with things like chatGPT and the assortment of voice assistants, NLP is currently exploding in both capability and prevalence. Those other models are based on these concepts, but there are a few key differences that help those tools be more powerful:\n",
    "<ul>\n",
    "<li> They are trained on much larger datasets. Very, very, very large datasets. In NLP specifically, this helps because it can help address the problem with us having so many words, many of which aren't used super often - i.e. the fact that there are a lot of words that don't occur together in the same sentence. If the training data is massive (e.g. \"the internet\"), we massively reduce the impact of this problem, as we see each word many times. \n",
    "<li> The use of neural networks, in particular recurrant neural networks (RNNs) and now transformer models that are able to deal with data as a sequence, and \"remember\" other parts of a sequence of words. This helps these models understand the context of a sentence, and the relationships between words.\n",
    "    <ul>\n",
    "    <li> Of note with neural networks, especially those using massive training data sets, is that the first layers of the model can perform equivalent data prep work that we've done here. So the model is more able to deal with data in its raw form, and doesn't need to be preprocessed as much separately, in advance. \n",
    "    </ul>\n",
    "<li> Manual intervention is used, humans provide examples of convesation, define labels, and evaluate the quality of the model's work. You may have heard news of Kenyans being paid low wages to label data for these models.\n",
    "<li> Other model types are used to help, such as reinforcement learning. Responses that are good are rewarded, and those that are bad are punished. This helps the model learn what is good and what is bad. This is particularly useful for generative models, such as chatGPT. An example of how this works is that a generative model might create 3 candidate sentences of text, and an actual humnan will select the best one. This provides feedback to the model on what is good, and what is not, and it will refine what it generates based on the feedback it gets. \n",
    "</ul>\n",
    "\n",
    "As noted, this stuff is actively being developed right now, and the more advanced the tool, the more likely we are to see innovation or specific interventions to correct issues. The foundations we have looked at here are the building blocks of that work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mar_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
