{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PTC5srJ2bYG9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, InputLayer, Reshape, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras import metrics\n",
        "\n",
        "import keras\n",
        "import keras.utils as np_utils\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "devices = tf.config.list_physical_devices()\n",
        "print(\"\\nDevices: \", devices)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  details = tf.config.experimental.get_device_details(gpus[0])\n",
        "  print(\"GPU details: \", details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kt2r1tfRg0cj"
      },
      "outputs": [],
      "source": [
        "# Helper to plot loss\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS8tDLiFbYG_"
      },
      "source": [
        "# Keras and Tensorflow Optimizations\n",
        "\n",
        "There are several things that we can do to make our networks a bit better. Unfortunately for much of this there aren't definitive answers for \"what is the best choice\", so we do have to do some trial and error, but we can use some guidelines to get us started in the right direction.\n",
        "\n",
        "## Environment Notes\n",
        "\n",
        "There were some things that changed since I originally wrote this code with respect to versions and some of the naming schemes in the Keras/Tensorflow libraries. I created a new environment from scratch, and had it install several key packages. The command I used, to make an environment called \"blank_tf\" was:\n",
        "\n",
        "```bash\n",
        " conda create -n blank_tf keras tensorflow pandas seaborn matplotlib scipy numpy scikit-learn\n",
        "```\n",
        "This will install those libraries, along with an assortment of their dependencies. If you choose to do something similar, be aware that this environment may lack other libraries that you might need for other stuff, if so, a command of '!pip install <package_name>' will install the package in the current environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQLYUA1XbYHB"
      },
      "source": [
        "## Load MNIST Data\n",
        "\n",
        "We can use the MNIST digit dataset for testing, since it is reasonably large. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE - you can play with these based on speed of whatever processor you have\n",
        "# for mine, I'm using a fairly fast one, so we can get some decent results\n",
        "# Feel free to scale down for practicality\n",
        "PATIENCE = 10\n",
        "SMALL_EPOCHS = 15\n",
        "LARGE_EPOCHS = 200\n",
        "SMALL_BATCH = 4\n",
        "LARGE_BATCH = 512\n",
        "MID_BATCH = 64\n",
        "XL_BATCH = 4096\n",
        "\n",
        "HUGE_BATCHES = False\n",
        "if HUGE_BATCHES:\n",
        "    multiplier = 50\n",
        "    SMALL_BATCH = SMALL_BATCH * multiplier\n",
        "    MID_BATCH = MID_BATCH * multiplier\n",
        "    LARGE_BATCH = LARGE_BATCH * multiplier\n",
        "    XL_BATCH = XL_BATCH * multiplier\n",
        "\n",
        "    SMALL_EPOCHS = SMALL_EPOCHS * 3\n",
        "    LARGE_EPOCHS = LARGE_EPOCHS * 2\n",
        "    PATIENCE = PATIENCE * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGygrQlobYHC",
        "outputId": "e48ea85c-2032-4ddc-85bf-4a02dd76b7f4"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "####################################################################################\n",
        "# Available to test - standardize the data\n",
        "####\n",
        "# THOUGHT EXPERIMENT - enable and disable this, observe the results, and see if you can understand why\n",
        "# Specifically, which change below performs well, and why when this is turned off?\n",
        "####\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "train_images = scaler.fit_transform(train_images.reshape(-1, 28 * 28))\n",
        "test_images = scaler.transform(test_images.reshape(-1, 28 * 28))\n",
        "\n",
        "# Reshape the input image\n",
        "train_images = train_images.reshape(-1, 28, 28)\n",
        "test_images = test_images.reshape(-1, 28, 28)\n",
        "#####################################################################################\n",
        "\n",
        "print(train_images.shape)\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "test_labels = np_utils.to_categorical(test_labels)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=SMALL_EPOCHS,\n",
        "  batch_size=MID_BATCH,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsycEGiGbYHE"
      },
      "source": [
        "## Prequel - Saving and Loading Models\n",
        "\n",
        "As we've seen, models can take a long time to train in many cases. Like with the sklearn models, we can save and load ours as they are trained and reused. This is a pretty integral part of making neural network models usable, so it is pretty easy. \n",
        "\n",
        "In addition to this we often see models saved in the h5 format, which just saves slightly less stuff along with the model. If we are using models trained elsewhere this format is very common. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2PSIZDbbYHE",
        "outputId": "37aa2093-e7d5-4680-fac9-ba4045372d96"
      },
      "outputs": [],
      "source": [
        "# Save my model\n",
        "model.save('model_path.keras')\n",
        "model = keras.models.load_model('model_path.keras')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DoRkV4nTbYHF"
      },
      "source": [
        "## Network Size\n",
        "\n",
        "Probably the first question that we will think of when building networks through Tensorflow is \"how big should it be\"? This is a very big question, and one of those ones without a real answer. We can put some guidelines in place to help us though. \n",
        "\n",
        "### What Does the Size Mean?\n",
        "\n",
        "The size of a neural network is also known as the capacity. We can relate it roughly to the size of our first model, the tree. The larger a network is the higher its capacity to learn. This is similar to a tree, the larger the tree, the more fitted it can become to the training data. \n",
        "\n",
        "### What Size to Use?\n",
        "\n",
        "We can start with a few guidelines to have a reasonably sized neural network. These steps do not ensure an optimal solution, but they'll get us started. There really is not a prescribed method for calculating the optimal network size (beleive me, I've looked), but there are several rules of thumb we can build together to get a rough estimate of a starting point for sizing:\n",
        "\n",
        "<ul>\n",
        "<li> Start with an input layer that is either\n",
        "    <ul>\n",
        "    <li> The width of the data, if the feature set is relatively small. \n",
        "    <li> A reasonably large number if the feature set is large. \n",
        "    <li> We don't have a true diving line, but 512 is a reasonable value to try for an upper end, at least at first. \n",
        "    </ul>\n",
        "<li> Add 1 or 2 hidden layers of the same size and observe the results. We want to keep the model smaller if making it larger doesn't improve things, so first we shoudl see how good of a job a small model does. If the data is very large, skipping past the 1 layer step may save some time since we can predict that we can do better with a larger model in advance. \n",
        "<li> Increase layers of the same size until we get some overfitting and the training loss flattens. We want to reach the point where the model is getting to be excellent at predicting the training data. This is something we can see in the plot by noticing that the validation loss flatlines or starts to get worse. The training loss flattening is an indication that the model is not getting any better at learning the training data; we can use early stopping with a loose patience setting on training loss and lots of epochs to find this. \n",
        "<li> Add regularization steps to cut down that overfitting. We can try regularization and dropouts to cut down on that overfitting. We probably want to try a few options, parameters, and combinations here, there's not really a way to know in advance which regularization will work best on our data. \n",
        "<li> Do a test of \"funneling\" the layer size, potentially adding more layers. The traditional configuration of layers is to gradually decrease the size from the input layer towards the output layer. There is open debate on if this is better than having layers that are all the same size. We can play with this a little to see if results improve or not. \n",
        "<li> Use pruning. Much like a tree we can prune back a model to fight overfitting. \n",
        "</ul>\n",
        "\n",
        "### Height vs Width\n",
        "\n",
        "Another begged question is should we make networks wider (more neurons) or deeper (more layers)? To that effect, we can also think about what happens as layers are added. Each layer allows the network to learn a different representation of the data, and if we flash back to the simple logistic regression and XOR examples, each layer allows the model to capture relationships that are more complex:\n",
        "<ul>\n",
        "<li> No hidden layers - linear relationships only. \n",
        "<li> 1 or more hidden layers - nonlinear relationships, of increasing complexity.\n",
        "</ul>\n",
        "\n",
        "So the number of layers in our model directly relates to the complexity of relationships that we can capture. This doesn't directly mean that more layers are better, but it generally means that we want the number of layers to mirror the \"complexity\" of the data, though complexity is a term with no exact definition or metric here. Once again, there's no universal answer on the balance between width of layers and number of layers, but the general evidence leans towards more layers. There are several reasons for this, none of them definitive, but taken as a whole they add up to a strong case:\n",
        "\n",
        "<ul>\n",
        "<li> Ability to learn different representation of the data - this will be more clear next time when we start to look at some image specific neural networks, but one of the cool features of neural networks is that at each layer the network \"sees\" a different representation of the data, as it goes through each round of transformations. This has the effect of allowing it to identify different features at each layer, and use those features to make more and more accurate predictions. We'll examine this more soon. \n",
        "<li> Avoiding overfitting - extremely wide neural networks tend towards overfitting the training data and not generalizing as well to new data. \n",
        "<li> Ability to add interim steps - with a multi layer network we can add multiple steps such as regularization or dropouts, again to fight overfitting. \n",
        "<li> Automatic feature selection - deep neural networks will automatically perform a type of feature selection as the least important features are minimized in their importance. This is an emerging area of research - some people have argued that well designed neural networks can remove the need for feature selection, and neural networks are being created to be feature selection tools. We can see this illustrated most clearly with images again, we feed a network an entire image, and get a prediction. Note that this isn't a total rejection of feature selection for neural networks, improving the feature set will impact neural network models just as it will for ordinary models; with neural networks we just have the potential for the network to \"cover for mistakes\" in the features. This is more dramatic as data size and network size increase. \n",
        "<li> Results - deep learning has become a common term recently for a reason, due to the success of deep neural networks with many layers. Most of the cool stuff that we see coming from AI such as image recognition, translation, and self navigating robots are the result of deep learning networks. In practice these networks have tended to outperform shallower ones, especially in more complex tasks. \n",
        "</ul>\n",
        "\n",
        "### Overfitting and Underfitting\n",
        "\n",
        "Why not make a model that is both very wide and very deep? This will tend to overfit as it can \"memorize\" the training data. With large datasets we do see very large models in some cases, since the more data we have, the more fitting we can handle. With large datasets and huge models, the training time can potentially explode, so we have to be careful. This is similar to what we saw with unlimited size in trees, we can eventually create a model large enough to memorize the training data. On the whole, the model capacity can be thought of similarly to the size of a tree. If we have large amounts of complex data, we want a model that has a very high capacity, as the relationships are complicated and we have enough data to mitigate overfitting. If we have smaller or more simple data, we want a smaller model, as a large one will overfit our data. \n",
        "\n",
        "Finding the \"optimal\" size is still somewhat of an art, combined with patience for trial and error. <b>The easiest starting point is probably to create a model that can potentially overfit, though not comically large, then use some regularization techniques and early stopping to find the optimal fit.</b> We should also note that it is possible and reasonable that models of different sizes and configurations tend to converge on similar levels of performace. These neural networks are extremely flexible in how they learn during the training process, so they can \"learn around\" some degree of design decisions in a way that is more dramatic than our previous models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-CRvieaiZI2",
        "outputId": "8c109988-428d-481d-d54e-b8eb1954cb2a"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(784, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(784, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  validation_split=0.2,\n",
        "  batch_size=MID_BATCH,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJkWGpsmbYHG",
        "outputId": "cda23fac-98c0-4d7d-d016-56d69a369bda"
      },
      "outputs": [],
      "source": [
        "# Bigger Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  validation_split=0.2,\n",
        "  batch_size=MID_BATCH,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmRw70jwga-g",
        "outputId": "c6dacb57-0449-40b7-e461-457f48745d70"
      },
      "outputs": [],
      "source": [
        "# Tapered Model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  validation_split=0.2,\n",
        "  batch_size=MID_BATCH,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LwRjUyBMbYHG"
      },
      "source": [
        "## Epochs and Batch Sizes\n",
        "\n",
        "![Batch Sizes](images/iterations_epoch.webp \"Batch Sizes\")\n",
        "\n",
        "### Epochs\n",
        "\n",
        "Each epoch is a run through all of the training data. Epochs are simple, we can set a large number and use early stopping to cut things off when we've reached the best result. \n",
        "\n",
        "### Batch Sizes\n",
        "\n",
        "Batch size determines how many records are processed before the gradients are updated - i.e. the number of records between one forward and backwards pass. The batch sizes are a matter of very open debate for the optimal solution. At the high end, batch sizes are limited by what can fit in memory. When dealing with very large data this may matter as a batch that is a small fraction of the data may be a massive absolute size. At the lower end using smaller batches gives the same effect as it does when we looked at regular gradient descent - the gradients become less stable as we are relying on a smaller number of records. In reading more about batch sizes I want to update my recommendation to be even smaller than the 50 to 150 I suggested before, down to less than 100, even as small as into the single digits. There is research that smaller batch sizes tend to produce models that generalize better than ones with larger batches. \n",
        "\n",
        "Larger batch sizes do tend to be processed more quickly, sometimes substantially so, as the hardware is better able to be \"saturated\" with data to process. In big data scenarios, this can matter. One thing that you see in practice is that the GPUs (or similar) that are used to train neural networks have a certain amount of memory, and the batch size is limited by that memory. When doing something that involves large images or video, or similar, this can be a real area for concern. For us, these constraints won't really come up, but it's good to be aware of them. \n",
        "\n",
        "Dont' stress too much on batch size, this is really something that needs to be grid searched to find a great answer and in scenarios where it actually matters, is largely influenced by the hardware that is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMFYDxPdbYHG",
        "outputId": "bff27c4c-5d53-4d25-96f5-ef5c69444d3a"
      },
      "outputs": [],
      "source": [
        "# Big Batch\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  batch_size=LARGE_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJeAtnDipXo",
        "outputId": "e5abf716-b380-4174-f0df-9611e7689a9b"
      },
      "outputs": [],
      "source": [
        "# Small Batch\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "# Note: Batch size of 2 is good to test\n",
        "# But it can take a VERY long time, be aggressive with the early stopping\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=SMALL_EPOCHS,\n",
        "  batch_size=SMALL_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bUFvtavwbYHH"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "Of all options the optimizer is the one we will care about the least. Each different optimizer is a different algorithm for doing the gradient descent. The optimizers have different results with respect to speed, memory usage, computational expense, and likelyhood to get stuck in a local minima. The optimizer is similar to some of the options we saw with logistic regression - there were several solver options for that model, each of which performed the gradient descent process with a slightly different set of calculations. Optimizers tend to manipulate the learning rate, attempting to narrow in on the optimum weights in fewer epochs and thus fewer calculations.\n",
        "\n",
        "![Optimizers](images/optimizers.gif \"Optimizers\")\n",
        "\n",
        "<b>Note:</b> this animation doesn't have Adam, which is unfortumate, but it was the most clear one I could find. The different optimizers are all trying to find the same minimum, and in most cases they all do, but they take differnt paths to get there, over a different number of steps.\n",
        "\n",
        "Adam is a good compromise between all factors and is very commonly used. We'll just use this for our work. One other common one is RMSprop, if you're feeling spicy, give that a try and see if there are any imporvements. These optimizers don't change the model we are making (outside of edge cases, like getting stuck in a minima), they change the process of finding that model. The biggest impact of the optimizer is on the speed of the training process, and in turn the ability to experiment with different models. This is more of a concern as the data gets larger, as small improvements on each individual gradient descent step can add up to large improvements in the overall training time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5qIRrRF2bYHH"
      },
      "outputs": [],
      "source": [
        "optimizer_1 = tf.keras.optimizers.Adam()\n",
        "optimizer_2 = tf.keras.optimizers.RMSprop()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RXb18AySbYHH"
      },
      "source": [
        "## Activation \n",
        "\n",
        "Activation functions are the key to adding non-linearity to the network allowing it to learn complex and non-linear relationships in the data. We've used ReLU as the default and that is a solid choice in most cases. ReLU has one issue, the dying ReLU problem. This can happen when we get inputs to the activation function fall in the negative area. In short there can be neurons that \"die\" and never get updated again because the value becomes 0 and stays 0. These dead neurons are a problem as they now aren't contributing to the learning.\n",
        "\n",
        "![ReLU](images/relu.jpeg \"ReLU\")\n",
        "\n",
        "To combat the dying ReLU problem there are a couple of other activation functions that avoid that issue - Leaky ReLU and ELU. Each one changes the negative values to something other than 0 - Leaky ReLU uses a slight linear gradient, ELU uses an exponential function for a similar, but curved, slight gradient. These ReLU variants are a good choice, and probably the 'best' overall activation functions for most scenarios. One some datasets will be impacted by the dying ReLU problem. \n",
        "\n",
        "These activation function also have an impact on the speed of training. The ReLU function is very fast to calculate, and the Leaky ReLU and ELU functions are a bit slower. Other activation functions may be even more expensive. Will this matter? As with many things, it depends. For the small examples we are using, it probably won't matter much. On very large applications, we may need to consider the ability to train models more quickly, and try more models vs. the improved fit of another activation function.\n",
        "\n",
        "#### Activation Function Guidelines\n",
        "\n",
        "We can write a few rules of thumb to guide us in deciding on activation functions. On the whole, the choice is like a hyperparameter choice, and we want to choose whichever is the best for our data. Some of the guidelines are:\n",
        "<ul>\n",
        "<li> The output layer should have an activation function that matches the type of problem we are solving. \n",
        "    <ul>\n",
        "    <li> <b>Regression:</b> Linear activation function.\n",
        "    <li> <b>Binary Classification:</b> Sigmoid activation function.\n",
        "    <li> <b>Multiclass Classification:</b> Softmax activation function.\n",
        "    </ul>\n",
        "<li> Depending on the type of network/problem, our hidden layers default to different activation functions:\n",
        "    <ul>\n",
        "    <li> <b>Deep Neural Networks:</b> ReLU activation functions.\n",
        "    <li> <b>Convolutional Neural Networks: (Images)</b> ReLU activation functions.\n",
        "    <li> <b>Recurrent Neural Networks:(Seqential, Time Series)</b> Tanh activation function.\n",
        "    </ul>\n",
        "</ul>\n",
        "\n",
        "There are more activation functions, and others are being developed somewhat regularly. If in doubt, just use a ReLU variant. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ81hFfxbYHH",
        "outputId": "d9a522ce-4252-4709-cb3a-c5cebfb1f355"
      },
      "outputs": [],
      "source": [
        "# Take a leak \n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='leaky_relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(350, activation='leaky_relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer=optimizer_2, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=SMALL_EPOCHS,\n",
        "  batch_size=MID_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Batch normalization is a technique that is used to normalize the inputs of each layer, so that the network can learn more quickly and with more stability. This is a technique that is used to help with the vanishing gradient problem, which is a problem that can occur when the gradients become very small and the network stops learning. This is a common problem with deep networks, and batch normalization is a common solution.\n",
        "\n",
        "Using batch normalization can have a few effects on the network:\n",
        "<ul>\n",
        "<li> Faster training - the network can learn more quickly with batch normalization.\n",
        "<li> More stable training - the network is less likely to get stuck in a local minima.\n",
        "<li> Regularization - batch normalization can act as a form of regularization, helping to prevent overfitting.\n",
        "</ul>\n",
        "\n",
        "Batch normalization is a good choice for many networks, and we see it used very commonly in practice. It is unlikely to make things worse. Implementing batch normalization is as simple as adding a layer to the network. In most cases we can add it after the activation function, meaning after a dense layer here. It can be done at other points, such as before the activation layer, if it is separated from the dense layer - this is more of a fine-tuning concern. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with sigmoid\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add( BatchNormalization())\n",
        "model.add(Dense(500, activation='sigmoid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(350, activation='sigmoid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  batch_size=LARGE_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With ReLU\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(350, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  batch_size=LARGE_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With regularization\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(500, activation='relu',kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(350, activation='relu',kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "train_log = model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=LARGE_EPOCHS,\n",
        "  batch_size=LARGE_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_images, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi7DxgCsbYHH"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "The initialization provides the starting point for all the weights and bias values that we start out with. We initially started with random values in the scratch network - this is generally fine, but we can sometimes do better. One specific case where we can do better is when we have data that is highly imbalanced. This is a common problem in things such as fraud detection, where we have a very small number of fraud cases, and a very large number of non-fraud cases. Seeding the model with an \"expectation\" of the bias values can help the model learn faster and converge on an answer more quickly. In cases where we have a lot of data, this can be a big deal. In cases where we may have local minima in the loss curve, this can be significant - the more \"ground\" the gradient descent covers, the greater the odds it encounters a local minima that might be a trap. With imbalanced data, we expect that the model will need to cover a lot of that ground, so starting off with a better bias value can help. \n",
        "\n",
        "### Imbalanced Weighting\n",
        "\n",
        "One application where initialization can help significantly is when dealing with imbalanced data. In this example of credit card fraud (real data that has been put through PCA), very, very few transactions are fraudulent. So we have a very imbalanced target value - the class variable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "jf84UJGPbYHI",
        "outputId": "8293e2c9-d3ae-45d7-a10d-30eeeeb95c1a"
      },
      "outputs": [],
      "source": [
        "file = tf.keras.utils\n",
        "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count the Target Outcomes\n",
        "\n",
        "Credit card fraud is relatively rare, at least in view of the total number of transactions. We can count up the target values to see exactly what the expected skew is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec08WeQEi_Fz",
        "outputId": "c35fb180-9017-4304-ce29-fd36ca25bfc2"
      },
      "outputs": [],
      "source": [
        "# Bincount will count the number in each category\n",
        "neg, pos = np.bincount(raw_df['Class'])\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gr-lqY0tHwp"
      },
      "source": [
        "### We Have an Imbalance\n",
        "\n",
        "A big one. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5S02N45tGfu",
        "outputId": "27b8e170-e838-4c7d-adf1-b9013e4f9a6b"
      },
      "outputs": [],
      "source": [
        "# pop just removes a column. The equivalent of how we normally drop. \n",
        "# the TF docs commonly use this, so I've left it as is. \n",
        "cleaned_df = raw_df.copy()\n",
        "# You don't want the `Time` column.\n",
        "cleaned_df.pop('Time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-3538h2djFHF"
      },
      "outputs": [],
      "source": [
        "# Use a utility from sklearn to split and shuffle your dataset.\n",
        "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
        "\n",
        "# Form np arrays of labels and features.\n",
        "train_labels = np.array(train_df.pop('Class'))\n",
        "test_labels = np.array(test_df.pop('Class'))\n",
        "\n",
        "train_features = np.array(train_df)\n",
        "test_features = np.array(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC5P3kaBjMwN",
        "outputId": "ed06c86d-234b-4045-b4fc-1c117eddf35a"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "#train_features = np.clip(train_features, -5, 5)\n",
        "#test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Test features shape:', test_features.shape)\n",
        "print('Test labels shape:', test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Baseline Model\n",
        "\n",
        "First, we'll set a baseline model with the default settings. I'm going to make this the same structure as the bias model, so we can assess the impact. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "      #keras.metrics.TruePositives(name='tp'),\n",
        "      #keras.metrics.FalsePositives(name='fp'),\n",
        "      #keras.metrics.TrueNegatives(name='tn'),\n",
        "      #keras.metrics.FalseNegatives(name='fn'), \n",
        "      #keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True) \n",
        "\n",
        "fraud_width_1 = 16\n",
        "fraud_width_2 = 16\n",
        "fraud_width_3 = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline model\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(train_features.shape[1],)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_1, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_1, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_3, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=METRICS)\n",
        "\n",
        "#Fit\n",
        "train_log = model.fit(\n",
        "  train_features,\n",
        "  train_labels,\n",
        "  epochs=SMALL_EPOCHS*3,\n",
        "  batch_size=MID_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_features, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jtt_zz0HA6Jp"
      },
      "source": [
        "### Create a Biased Model\n",
        "\n",
        "The bias of the data is inserted in the model compilation step on the output layer. What does this do? It preconfigures the output layer to \"expect\" results to be this skewed. Recall that, along with the weight, the bias values are one of the things that is learned in training. By default the initial values are randomized, so the model needs to learn the skew towards the imbalance - if the balance between classes is moderate, that's not a big deal; if the balance is so drastically skewed in one direction, that's less practical. With the preset bias we can speed convergance and likely reduce loss. \n",
        "\n",
        "#### Other Imbalenced Work\n",
        "\n",
        "Other things that we looked at to improve balance such as under/over sampling still works with neural networks as it would with anything else. This is just one nn-specific thing that we can implement with minimal extra work. \n",
        "\n",
        "#### Metrics\n",
        "\n",
        "We can also add a bunch of metrics to what we get returned by creating a list of the metrics that we want. Keras.metrics has a list, they are all the metrics we might expect. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7R0L7XjTZv",
        "outputId": "1344b694-02cc-4855-8d0a-30cd56f4e6d7"
      },
      "outputs": [],
      "source": [
        "initial_bias = np.log([pos/neg])\n",
        "\n",
        "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
        "model = keras.Sequential()\n",
        "model.add(InputLayer(shape=(train_features.shape[1],)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_1, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_1, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(fraud_width_3, activation='leaky_relu', kernel_regularizer=\"l2\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),loss=keras.losses.BinaryCrossentropy(),metrics=metrics)\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=METRICS)\n",
        "#Fit\n",
        "train_log = model.fit(\n",
        "  train_features,\n",
        "  train_labels,\n",
        "  epochs=SMALL_EPOCHS*5,\n",
        "  batch_size=MID_BATCH,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[callback]\n",
        ")\n",
        "model.evaluate(test_features, test_labels)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Structure and Tuning\n",
        "\n",
        "We'll look at tuning neural network models in more depth soon. For now, we can use the results of our model's training and validation scores, along with a few guidelines, to aim us in the correct direction for making a good model. \n",
        "\n",
        "Some guidelines for tuning neural networks are:\n",
        "<ul>\n",
        "<li> Start with something small-ish, and increase as needed. For our exercises, this likely means 2 or 3 hidden layers to start. </li>\n",
        "<li> Use early stopping to find the optimal number of epochs. </li>\n",
        "<li> If the training score is similar to the validation score, the model is probably underfitting. Try things to allow it to learn more:</li>\n",
        "    <ul>\n",
        "    <li> Increasing the number of layers. </li>\n",
        "    <li> Increasing the number of neurons in each layer. </li>\n",
        "    <li> Training for more epochs. </li>\n",
        "    </ul>\n",
        "<li> If the training score is much higher than the validation score, the model is probably overfitting. Try things to prevent it from learning too much:</li>\n",
        "    <ul>\n",
        "    <li> Adding regularization. </li>\n",
        "    <li> Adding dropout. </li>\n",
        "    <li> Reducing the number of layers or the size of the layers. </li>\n",
        "    </ul>\n",
        "<li> If the model's scores seem to flatline, try adding some batch normalization or changing some other optimization-ish option. </li>\n",
        "</ul>\n",
        "\n",
        "These are just guidelines, but they are a good starting point. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Predict the price of diamonds! (The example solutions are fairly extreme in terms of the approach they take, you'll probably be ok with less dramatic approaches.)\n",
        "\n",
        "![Diamonds](images/diamonds.jpeg \"Diamonds\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_df = sns.load_dataset(\"diamonds\")\n",
        "ex_df = pd.get_dummies(ex_df)\n",
        "\n",
        "ex_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = np.array(ex_df[\"price\"]).astype(np.float32)\n",
        "X = np.array(ex_df.drop(columns={\"price\"})).astype(np.float32)\n",
        "X_tr_ex, X_te_ex, y_tr_ex, y_te_ex = train_test_split(X, y)\n",
        "start_width = X.shape[1]\n",
        "start_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex_df.describe(include=\"all\").T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate a Baseline\n",
        "\n",
        "I'll use a different loss - mean absolute percentage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attempt Optimization\n",
        "\n",
        "Looks like lots of loss! What to do?\n",
        "\n",
        "Things to try:\n",
        "<ol>\n",
        "<li> Depth.\n",
        "<li> Width. \n",
        "<li> Activations. \n",
        "<li> Batches. \n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Comically Deep Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Try Adding Dropouts\n",
        "\n",
        "We don't have a bunch of overfitting, so we may not expect miracles here... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Comically Deep Model with Dropouts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Taper Model Somewhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Taper Deep Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Best Trial\n",
        "\n",
        "This may vary a bit, but what I saw in the results above was broadly:\n",
        "<ul>\n",
        "<li> Train and test results improved by adding layers. </li>\n",
        "<li> Adding dropouts didn't help much. </li>\n",
        "<li> Batch normalization helped a bit. </li>\n",
        "<li> Tapering didn't help, though this varied a lot depending on trials. </li>\n",
        "</ul>\n",
        "\n",
        "I tried a combo of those things to see the results. In general, this seemed to work best with a larger model, lots of training, and some normalization to keep it learning. There may be other structures that work just as well, or better, but from my trials, these patterns emerged from the results. I made a model to mirror that:\n",
        "<ul>\n",
        "<li> Start with a wide layer. </li>\n",
        "<li> Have several layers (the model seemed to keep learning with more layers in previous trails)). </li>\n",
        "<li> Use regularization. </li>\n",
        "<li> Add batch normalization. (The loss flatlined previously). </li>\n",
        "<li> Allow for many epochs, until improvement stops. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Guess - This is kind of a stab in the dark. \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_optimizations_sol.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_mar_2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
