{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Base Parameters\n",
    "# Increase processing demands if on Colab\n",
    "BASE_EPOCHS = 3\n",
    "BATCH_SIZE = 512\n",
    "BASE_PATIENCE = 5\n",
    "MIN_DELTA = .02\n",
    "MONITOR = \"val_loss\"\n",
    "LOGS = \"logs\"\n",
    "DIR_OUT = \"kt_out\"\n",
    "PROJECT = \"kt_basics\"\n",
    "FACTOR = 3\n",
    "VALIDATION = .2\n",
    "MAX_TRIALS = 3\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  BASE_EPOCHS = 5\n",
    "  BATCH_SIZE = 1024\n",
    "  BASE_PATIENCE =3\n",
    "  MAX_TRIALS = 5\n",
    "  !pip install keras_tuner\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, Normalization, BatchNormalization\n",
    "from keras import layers\n",
    "import sklearn\n",
    "import datetime\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "import itertools\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n",
    "print(img_train.shape, label_train.shape, img_test.shape, label_test.shape)\n",
    "\n",
    "images = np.concatenate([img_train, img_test], axis=0)\n",
    "labels = np.concatenate([label_train, label_test], axis=0)\n",
    "test_images = images[-25:]\n",
    "test_labels = labels[-25:]\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "  plt.plot(history.history['accuracy'], label='accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "  # Save the plot to a PNG in memory.\n",
    "  buf = io.BytesIO()\n",
    "  plt.savefig(buf, format='png')\n",
    "  # Closing the figure prevents it from being displayed directly inside\n",
    "  # the notebook.\n",
    "  plt.close(figure)\n",
    "  buf.seek(0)\n",
    "  # Convert PNG buffer to TF image\n",
    "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "  # Add the batch dimension\n",
    "  image = tf.expand_dims(image, 0)\n",
    "  return image\n",
    "\n",
    "def image_grid():\n",
    "  \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
    "  # Create a figure to contain the plot.\n",
    "  figure = plt.figure(figsize=(10,10))\n",
    "  for i in range(25):\n",
    "    # Start next subplot.\n",
    "    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "\n",
    "  return figure\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "  \"\"\"\n",
    "  Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "  Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "  \"\"\"\n",
    "  figure = plt.figure(figsize=(8, 8))\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=\"binary\")\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(class_names))\n",
    "  plt.xticks(tick_marks, class_names, rotation=45)\n",
    "  plt.yticks(tick_marks, class_names)\n",
    "\n",
    "  # Compute the labels from the normalized confusion matrix.\n",
    "  labels = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "  # Use white text if squares are dark; otherwise black.\n",
    "  threshold = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "    plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogConfusionCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_images, test_labels, class_names, directory):\n",
    "        self.test_images = test_images\n",
    "        self.test_labels = test_labels\n",
    "        self.class_names = class_names\n",
    "        self.directory = directory\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Use the model to predict the values from the validation dataset.\n",
    "        test_pred_raw = self.model.predict(test_images)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = sklearn.metrics.confusion_matrix(self.test_labels, test_pred)\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        figure = plot_confusion_matrix(cm, class_names=self.class_names)\n",
    "        cm_image = plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        file_writer_cm = tf.summary.create_file_writer(self.directory)\n",
    "        with file_writer_cm.as_default():\n",
    "            tf.summary.image(\"epoch_confusion_matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Training with Tensorboard\n",
    "\n",
    "We can use the tensorboard to monitor our training as we progess. \n",
    "\n",
    "<b>Note:</b> if you run this in VS Code, the behavior will be different. It will likely ask you to install a plugin. On Colab, it'll appear beneath the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a timestamped log directory.\n",
    "#!rm -rf logs/tb/*\n",
    "#!rm -rf logs/tb/cm/*\n",
    "logdir = \"logs/tb\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(logdir)\n",
    "cm_callback = LogConfusionCallback(test_images, test_labels, class_names, logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/tb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner\n",
    "\n",
    "We can use the Keras Tuner to find the best hyperparameters for our model, in a similar way to how we usd a grid search to find the best hyperparameters for our scikit-learn models. The process is a little more involved, but it's still pretty straightforward. \n",
    "\n",
    "### Logs and Records\n",
    "\n",
    "All of the stuff we are looking at here relies on log and record files being saved to disk. We can save them anywhere, but it is probably a good idea to consolidate them in some organized way. We can do this by creating a new directory called `logs` in the root of our project, and putting the log results in there. One thing to watch out for is that you can sometimes get odd errors or warnings if you are looking at the log files for some other model. This is because it is expecting to find things that it has written for the model to which it belongs. This is very annoying, ask me how I know. For real applications, youd probably want to make specific folders to track these things - for our repeated trials, I have added a little datetime stamp to some file names to just ensure that they are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "\n",
    "# Metrics\n",
    "acc = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "pre = keras.metrics.Precision(name=\"precision\")\n",
    "rec = keras.metrics.Recall(name=\"recall\")\n",
    "metric_list = [acc, pre, rec]\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "file_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stopping = keras.callbacks.EarlyStopping(monitor=MONITOR, patience=BASE_PATIENCE, mode=\"max\", min_delta=MIN_DELTA)\n",
    "#tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode=\"max\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=LOGS+\"/tb/\"+file_time, histogram_freq=1, write_images=True, embeddings_freq=1, update_freq='epoch')\n",
    "#tf.keras.callbacks.TensorBoard(log_dir=LOGS+\"/tb/\"+file_time, histogram_freq=1, write_images=True, embeddings_freq=1, update_freq='epoch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "We will start with using the fashion dataset. \n",
    "\n",
    "This is loaded up above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Shape\", images.shape)\n",
    "print(\"Y Shape\", labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Builder\n",
    "\n",
    "To use Keras Tuner, you need to define a model builder function. This function takes a hyperparameter dictionary as input and returns a compiled model. In plain language, the model builder function is basically a framework for creating a model, with the key difference that we set things up so that the hyperparameters that we are testing are variables in the model construction. \n",
    "\n",
    "### HP.whatever\n",
    "\n",
    "The hyperparameters that we are testing are defined using the HP.[Type_of_choice] functions. For example, HP.Choice defines a hyperparameter that can take on one of a list of values. HP.Float defines a hyperparameter that can take on any value between a minimum and maximum value. HP.Int defines a hyperparameter that can take on any integer value between a minimum and maximum value. Each thing that we want to change in the search is set up with one of these \"hp.\" functions, along with the range of values we want to test.\n",
    "\n",
    "When the hyperparameter search is called, these hyperparameters will be varied according to the search algorithm that we choose - just as a grid search would work through the various combinations of hyperparameters in the list we supply. We can build this model builder function to test almost anything we can think of - different number of neurons, different number of layers, different types of layers, different activation functions, different optimizers, different learning rates, etc... The possibilities of what we can test are really only limited by our imagination and the time it takes to run the search. Some key hp items to be aware of are:\n",
    "<ul>\n",
    "<li>HP.Choice - a hyperparameter that can take on one of a list of values</li>\n",
    "<li>HP.Float - a hyperparameter that can take on any value between a minimum and maximum value</li>\n",
    "<li>HP.Int - a hyperparameter that can take on any integer value between a minimum and maximum value</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder2(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(Input(shape=(28, 28)))\n",
    "  model.add(Flatten())\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  # Layer 2\n",
    "  hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "  \n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning\n",
    "\n",
    "Our model builder is the core of the hyperparameter tuning process, using it is relatively simple. We use the model builder as an input to the type of parameter search we want to do, there are a few options offered in the keras tuner:\n",
    "<ul>\n",
    "<li> Grid search - searches over a grid of hyperparameters like we are used to. </li>\n",
    "<li> Random search - randomly selects hyperparameters from the search space and trains a model for each combination. </li>\n",
    "<li> Hyperband - a more advanced search algorithm that uses early stopping to quickly identify high-performing models. This is a good option when you have a large search space and want to run a quick search. (The factor represents a ratio that the algorithm reduces the number of models trained from trial to trial, lower values leave more models)</li>\n",
    "<li> Bayesian optimization - uses Bayesian optimization to identify high-performing hyperparameter values. Specifically, it starts by randomly selecting things, then uses performance to be smart about which others to select. </li>\n",
    "</ul>\n",
    "\n",
    "Once we've setup the search, we call the search() function in a very similar way to the fit() function. The search() function will run the search and find the best set of hyperparameters for our model. The Baeysian or Hyperband will likely perform much more quickly, especially as the seach space expands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(hypermodel = model_builder2,\n",
    "                     objective=MONITOR,\n",
    "                     max_epochs=BASE_EPOCHS,\n",
    "                     factor=3,\n",
    "                     directory=DIR_OUT,\n",
    "                     project_name=PROJECT,\n",
    "                     overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results\n",
    "\n",
    "We can get the results of the trial, just like in a grid seach. One note on getting the best models or parameters down below is that we can get more than one. Below we are only grabbing the best model and hyperparameters, but we could get the top 3 or however many we wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results\n",
    "tuner.search(img_train, label_train, epochs=BASE_EPOCHS, validation_split=VALIDATION, callbacks=[stopping, tensorboard_callback, cm_callback])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')}, the second is {best_hps.get('units2')}, and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Best Model\n",
    "\n",
    "Once we have the best parameters, we can train a final model. Here I'll train a model on ALL of the data, since we already know the best parameter, this is the model that we think will do the best job and will be the one we can use in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(images, labels, epochs=BASE_EPOCHS, validation_split=VALIDATION, callbacks=[tensorboard_callback, cm_callback])\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We can use this tuning approach to get a good fitting model without as much manual work in running trials. We can also likely repurpose parts of the model builder code, since things like testing layer sizes, number of layers, or activation functions are likely to be useful in other models and are exactly the same, no matter the specific model. \n",
    "\n",
    "The primary downside to running a search like this is that it is computationally expensive and potentially very slow to execute. When applying this to larger datasets we likely want to:\n",
    "<ul>\n",
    "<li> Use a GPU/cloud to speed up the training process. </li>\n",
    "<li> Use a sample of the data to speed up the process, or use a sample to find which types of hyperparameter choices are worthy of a larger test.</li>\n",
    "<li> Reduce number of epochs. </li>\n",
    "<li> Start with larger jumps, then narrow down on what is working. </li>\n",
    "</ul>\n",
    "\n",
    "The idea of the Keras tuning process is pretty simple, and the same as the grid search - we want to find the optimal set of hyperparameters and layer configuration for the model. The Keras Tuner makes this process much easier, and allows us to test a much larger number of combinations than we could manually, however we are much more likely to hit a limit of how many trials we can run due to time, so starting with an educated guess is still beneficial. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Example\n",
    "\n",
    "We can make our model even more complex and flexible. As long as we are able to define the options with ifs and loops, we can probably try almost anything we can think of. We can try a CNN version here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the class names are taken from the documentation\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model4(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='leaky_relu', padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 3)):\n",
    "        model.add(layers.Conv2D(hp.Int(\"filters_L\"+str(i), min_value=128, max_value=512, step=128), (3, 3), activation='relu', padding=\"same\"))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(layers.Dense(1024, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(512, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tuner3 = kt.BayesianOptimization(build_model4,\n",
    "                        objective=MONITOR,\n",
    "                        max_trials=MAX_TRIALS,\n",
    "                        directory=DIR_OUT+\"/\"+file_time,\n",
    "                        project_name=PROJECT,\n",
    "                        overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Search\n",
    "tuner3.search(X_train, y_train, epochs=BASE_EPOCHS, validation_split=VALIDATION, callbacks=[stopping, tensorboard_callback, cm_callback], batch_size=BATCH_SIZE)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps3 = tuner3.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model3 = tuner3.get_best_models(num_models=1)[0]\n",
    "best_model3.build(input_shape=(None, 32, 32, 3))\n",
    "best_model3.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints \n",
    "\n",
    "We can also use checkpoints to save the best model as we go, this is a good way to make sure we don't lose our best model if we have to stop the training process early. This is also a good way to make sure we don't have to retrain the model if we have to restart the notebook.\n",
    "\n",
    "#### Checkpoint Callback\n",
    "\n",
    "We can build the checkpoint saving process into our model training fairly easily, by putting it into a callback. This callback is a lot like the early stopping one, but instead it just saves the weights of the model whenever it sees an improvement. If the next epoch is better, we save the updated model, if it is worse, we do nothing. On subsequent runs, we can load the weights from the checkpoint and continue training if we wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#filepath=\"logs/weights/weights-improvement-\"+file_time+\".hdf5\"\n",
    "filepath=\"logs/weights/weights-improvement-\"+file_time+\".keras\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Weights\n",
    "\n",
    "Only the weights are saved in the checkpoint, it is assumed we already have the model's structure. To pick up from our checkpoint, we need to:\n",
    "<ul>\n",
    "<li> Create a model that is the correct structure. </li>\n",
    "<li> Retreive the weights from the checkpoint file. </li>\n",
    "<li> Load the weights into the model. </li>\n",
    "<li> Compile the model. </li>\n",
    "</ul>\n",
    "\n",
    "This will basically make a new model, that is exactly the same as the one that existed at the point in training when we saved the checkpoint.\n",
    "\n",
    "#### Example of Checkpoint\n",
    "\n",
    "We can see how this works by training a model a little, using the checkpoint callback, then loading the weights from the checkpoint and continuing training. I'll add a few layers and turn down the learning rate, so we can expect the training process to require a decent number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheck = keras.Sequential()\n",
    "modelCheck.add(Input(shape=(32, 32, 3)))\n",
    "modelCheck.add(Conv2D(64, (3, 3), activation='relu',  padding=\"same\"))\n",
    "modelCheck.add(MaxPooling2D((2, 2)))\n",
    "modelCheck.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "modelCheck.add(MaxPooling2D((2, 2)))\n",
    "modelCheck.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "modelCheck.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "modelCheck.add(Flatten())\n",
    "modelCheck.add(Dense(32, activation='relu'))\n",
    "modelCheck.add(Dense(10, activation=\"softmax\"))\n",
    "modelCheck.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheck.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=metric_list)\n",
    "train_logCheck = modelCheck.fit(X_train, y_train, epochs=10, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[stopping, checkpoint, tensorboard_callback, cm_callback])\n",
    "test_evalCheck = modelCheck.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_logCheck)\n",
    "plot_acc(train_logCheck)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating Checkpoint Model\n",
    "\n",
    "We can now take the partially trained model that we just saved, recreate it into a new model, and continue training - or do any other model related activities that we had in mind. For this one I'll let the epochs be larger, so it can run a bit more. This training process will be a \"new\" one, but the starting point is where we left off with the checkpoint weights, in my trials I saw the first epoch ot have an accuracy of about 25%, the last epoch of the previous training to have an accuracy slightly under 50%, and our training here should pick up at somewhere around 50% and continue to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNew = keras.Sequential()\n",
    "model.add(Input(shape=(32, 32, 3)))\n",
    "modelNew.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "modelNew.add(MaxPooling2D((2, 2)))\n",
    "modelNew.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "modelNew.add(MaxPooling2D((2, 2)))\n",
    "modelNew.add(Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n",
    "modelNew.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "modelNew.add(Flatten())\n",
    "modelNew.add(Dense(32, activation='relu'))\n",
    "modelNew.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Load Weights\n",
    "#/Volumes/Storage/git_courses/semester/1222/3950/Intro_to_Machine_Learning_Student_Workbooks/logs/weights\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# There is a \"get_latest_checkpoint\" function in the keras documentation, but it doesn't work for me\n",
    "# locally. That is probably a better solution, but it appears to have some bug\n",
    "# There were a couple of post online about differences on mac/win/linux, but that's not certain. \n",
    "list_of_files = glob.glob(\"logs/weights/*.keras\") \n",
    "latest = max(list_of_files, key=os.path.getctime)\n",
    "print(latest)\n",
    "modelNew.load_weights(latest)\n",
    "modelNew.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=.0001), metrics=metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logNew = modelNew.fit(X_train, y_train, epochs=15, batch_size=BATCH_SIZE, validation_split=.3, verbose=1, callbacks=[stopping, checkpoint, tensorboard_callback, cm_callback])\n",
    "train_evalNew = modelNew.evaluate(X_train, y_train)\n",
    "test_evalNew = modelNew.evaluate(X_test, y_test, verbose=2)\n",
    "plot_loss(train_logNew)\n",
    "plot_acc(train_logNew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Notes\n",
    "\n",
    "Looks pretty decent. One important factor to note is that the checkpoint only saves the weights, so the other details of the model are not carried through. In particular the optimizer is not saved, so we need to make sure we compile the model with the same optimizer that we used to train it. This will also mean that the model is not picking up exactly where it left off from in the training process, as the optimizer is also learning during the training process, and it is being reset. In general though, we should be able to pick up roughly from where we left off. \n",
    "\n",
    "The details vary a little depending on the exact optimizer choice, but the main thing that is \"fogotten\" is the learning rate and momentum of the optimizer. The optimizer will attempt to learn \"how fast to move\" and \"how much to move\" during the training process, in an effort to make the process more efficient and make convergance quicker. This information is not kept, so the new optimizer we create will start with default values that are not tailored to the model. This isn't really an issue, but the training process will likely take a bit longer than it otherwise would have. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement a CNN model to classify the dataset. \n",
    "\n",
    "Try to:\n",
    "<ul>\n",
    "<li> Use a CNN model. </li>\n",
    "<li> Use a checkpoint to save the best model. </li>\n",
    "<li> Use tensorboard to monitor the training process. </li>\n",
    "<li> Use some hyperparameter tuning to find the best model. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Models and Transfer Learning\n",
    "\n",
    "Our ability to save and recall models at will is powerful in allowing for a more managable training process, especially as the data becomes large and complex enough that training in one sitting is impractical. Loading models also leads us to another of the major benefits of using neural networks - we can load a model that has been trained elsewhere, usually with a massive training dataset and industrial level computing resources. \n",
    "\n",
    "Loading a model that was created elsewhere is not much more complex than loading one that we saved ourselves, in some ways it can even be easier, as several common models can be loaded directly from a function call, similar to an \"empty\" model or one of the built in keras or sklearn datasets.\n",
    "\n",
    "### But it was Trained on a Different Dataset????\n",
    "\n",
    "The models that we download were obviously trained on different datasets, not the ones that we actually want to use, so why bother with some premade model? One answer is that many of the tasks that we may want to do with a neural network are generic, for example there are several models that are trained to recognize common items in images or process text. A more targeted answer is that we can use the model as a starting point, and then fine tune it to our specific needs, which we will look at next time. In short, we can use the abilities of a highly trained model to extract information from images, then combine that with part of our own model that tailors the predictions to what we want.\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "This entire area of learning is called <b>Transfer Learning</b> as we are transfering the learning done on one problem to some other problem. \n",
    "\n",
    "![Transfer Learning](images/transfer_learning.png \"Transfer Learning\")\n",
    "\n",
    "The layered nature of neural networks lends itself well to this application, as does the large data and computing power requirements of making large and accurate models. This allows much more widespread use of highly accurate models as the ability to adapt a pretrained model is common while the ability to make a competitive one from scratch is rare. The first variety of transfer learning that we can look at is to replace the classifier, or the dense fully-connected part of a CNN that produces the actual classification. We can literally just slice off those layers and attach our own, then partially retrain the model of the only part that is capable of learning, the classifier we attached, learns to take the features extracted by the convolutional part and produce predictions into the classes we want. \n",
    "\n",
    "![Replace Layers](images/replace_layers.webp \"Replace Layers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "![VGG16](images/vgg16.png \"VGG16\" )\n",
    "\n",
    "We can load an external model and use it to make some predictions. There are several models that are pretrained and available to us to use. VGG16 is one developed to do image recognition, the name stands for \"Visual Geometry Group\" - a group of researchers at the University of Oxford who developed it, and ‘16’ implies that this architecture has 16 layers. The model got ~93% on the ImageNet test that we mentioned a couple of weeks ago. \n",
    "\n",
    "<b>Note:</b> this model, and some others that we can use, are not the sequential type that we are used to, they are functional, which basically means that they are more flexible and offer the ability to confgure models for more complex tasks. The impact that this has on us is that the syntx is a little different, but the ideas are the same. We can adapt the examples in the next 2 workbooks to use pretrained functional models, we don't need to delve into the details of their structure or setup to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_tensor = Input(shape=(32, 32, 3))\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(.2)(x)\n",
    "prediction = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remix Edition of the Model That's Predictin'\n",
    "\n",
    "What have we created here? The functional syntax above is basically chaining together the inputs and outputs of the layers, because one of the differences in these models is that they don't assume a layer-by-layer flow as the sequential ones do. \n",
    "\n",
    "<ul>\n",
    "<li> The first part of the model is the VGG bit, it is frozen (non-trainable). \n",
    "<li> The next part is the Flatten layer, it takes in the input of vgg.output and outputs to x. \n",
    "<li> Next, a dense layer with 128 neurons and a relu activation, takes in that output of x, and outputs to x again. (This type of syntx is common if layers are just chained together.)\n",
    "<li> Then there is a dropout for 20%, takes in that output of x, and outputs to x again.\n",
    "<li> The last part is the prediction, it takes in x as an input, and outputs to our normal prediction - 5 neurons for 5 classes. (This one outputs logits.)\n",
    "<li> The model takes in the vgg.input and outputs the final layer. Again, these models can have multiple in/out, so we have to specify. \n",
    "</ul>\n",
    "\n",
    "Once the model is made, we can use it as normal to fit and predict, it will use the power of VGG combined with the specific classes that we provided in the final two layers. The part we added could theoretically be anything as long as it can deal with the data shape - since we are flattening, that is pretty flexible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "            optimizer=\"adam\", \n",
    "            metrics=metric_list)\n",
    "\n",
    "log_dir = \"logs/tb/VGG/vgg_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True)\n",
    "\n",
    "vgg_hist = model.fit(X_train, y_train,\n",
    "            epochs=BASE_EPOCHS*4,\n",
    "            verbose=1,\n",
    "            callbacks=[stopping, tensorboard_callback],\n",
    "            validation_split=.2)\n",
    "plot_loss(vgg_hist)\n",
    "plot_acc(vgg_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir=logs/tb/VGG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try this VGG model on the other dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_train.shape, label_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mar_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
