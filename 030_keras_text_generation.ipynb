{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_hub in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: keras>=3.5 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (3.9.0)\n",
      "Requirement already satisfied: absl-py in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (24.2)\n",
      "Requirement already satisfied: regex in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2024.11.6)\n",
      "Requirement already satisfied: rich in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (0.3.11)\n",
      "Requirement already satisfied: tensorflow-text in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2.19.0)\n",
      "Requirement already satisfied: namex in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.0.7)\n",
      "Requirement already satisfied: h5py in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (3.12.1)\n",
      "Requirement already satisfied: optree in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (4.67.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from rich->keras_hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from rich->keras_hub) (2.15.1)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow-text->keras_hub) (2.19.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras_hub) (0.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.62.2)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (2.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (2025.1.31)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.45.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens\n",
    "MAX_SEQ = 3000\n",
    "MIN_LEN = 35\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 3000  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 15\n",
    "USE_DATASETS = True\n",
    "LEARN_RATE = 0.01\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 20\n",
    "\n",
    "SAMP = 7\n",
    "\n",
    "LARGER_TRAIN = False\n",
    "if LARGER_TRAIN:\n",
    "  BATCH_SIZE = 512\n",
    "  EPOCHS = 50\n",
    "\n",
    "import keras \n",
    "!pip install keras_hub\n",
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "# Check saved weights\n",
    "weight_url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/simple_generative_weights_1.keras\"\n",
    "use_old_weights = True\n",
    "have_old_weights = False\n",
    "try:\n",
    "  old_weights = keras.utils.get_file('simple_generative_weights_1.keras', weight_url)\n",
    "  have_old_weights = True\n",
    "except:\n",
    "  print(\"No old weights found.\")\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM\n",
    "\n",
    "As we have seen, LSTM models are excellent at dealing with sequential data. As luck would have it, text is also sequental data! We can train a model to predict the next word in a sentence, then use that smarts to generate new text. Basically ChatGPT, but far better. \n",
    "\n",
    "### Text for Training\n",
    "\n",
    "We need some text from which to train our model to speak, I captured a small extract of text from Reddit posts, which vaguely resembles actual language, if that language is filtered through a bucket of social isolation, Adderall, racism, and memes. We'll first need to clean up our data a bit before we can assemble it for modelling. The initial cleaning bits are just like what we used in NLP, we just need to get rid of all the junk. \n",
    "\n",
    "We can use pretty much anything that you can imagine as source, and assuming we can gather enough data and train our model, the generated speech will be styled after the source. I liken it to going on vacation in Indonesia and talking to Indonesians who spoke English like Australian surfer bros - their training data was a little weird, so the output was a little weird too. If you're looking to build your best ChatGPT competitor you will want a lot of data, specifically a lot of data that is representative of the full gamut of how you want your model to write. If you want slang in the new text, you can't really train on Shakespeare and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162411.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149663</th>\n",
       "      <td>went account website pay found total paid amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>good morning name phone tried unsuccessfully r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17685</th>\n",
       "      <td>equifax reporting authorized user partial acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>ftc violation enclosed credit card opened thie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15760</th>\n",
       "      <td>deposited ui benefit cash ap account purchased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106268</th>\n",
       "      <td>someone fraudulently used credit card purchase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>creditor reporting inaccurate fraudulent infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87084</th>\n",
       "      <td>issue credit bureau updated correct address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77835</th>\n",
       "      <td>submitted complaint numerous bill collector ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115838</th>\n",
       "      <td>went store applied credit card lender well far...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                narrative\n",
       "149663  went account website pay found total paid amou...\n",
       "22688   good morning name phone tried unsuccessfully r...\n",
       "17685   equifax reporting authorized user partial acco...\n",
       "5783    ftc violation enclosed credit card opened thie...\n",
       "15760   deposited ui benefit cash ap account purchased...\n",
       "106268  someone fraudulently used credit card purchase...\n",
       "13831   creditor reporting inaccurate fraudulent infor...\n",
       "87084         issue credit bureau updated correct address\n",
       "77835   submitted complaint numerous bill collector ca...\n",
       "115838  went store applied credit card lender well far..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complaint data\n",
    "USE_REDDIT = True\n",
    "if USE_REDDIT:\n",
    "    # Reddit WSB data\n",
    "    train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "    USE_COL = \"body\"\n",
    "    train_text = pd.read_csv(train_text_file)\n",
    "    train_text[USE_COL] = train_text['title'].astype(\"str\") + \" \" + train_text[USE_COL].astype(\"str\")\n",
    "    train_text[USE_COL] = train_text[USE_COL].str.replace(\"\\n\", \" \")\n",
    "    #clean punctuation\n",
    "    train_text[USE_COL] = train_text[USE_COL].str.replace(\"[^\\w\\s]\", \"\")\n",
    "    train_text[USE_COL].dropna(inplace=True)\n",
    "else:\n",
    "    # Complaint data\n",
    "    train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/complaints_processed_clean.csv')\n",
    "    USE_COL = \"narrative\"\n",
    "    train_text = pd.read_csv(train_text_file)\n",
    "    train_text[USE_COL] = train_text[USE_COL].astype(\"str\")\n",
    "\n",
    "\n",
    "print(np.mean(len(train_text[USE_COL])))\n",
    "train_text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> the install of the NLTK stuff can be weird on Colab. Sometimes I need to run it, let it install, and run again. On my laptop, it works normally, I beleive it is just due to the temporary virtual environment on Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "\tDimensions: 83\n",
      "\tSample: ['received', 'information', 'may', 'deploying', 'paid', 'balance', 'credit', 'card', 'kept', 'card', 'knowing', 'charge', 'interest', 'billing', 'cycle', 'without', 'communicating', 'charge', 'accrued', 'interest', 'know', 'reported', 'late', 'payment', 'credit', 'report', 'reached', 'bank', 'reversed', 'late', 'payment', 'fee', 'paid', 'remaining', 'payment', 'decided', 'close', 'account', 'fair', 'put', 'day', 'late', 'payment', 'little', 'amount', 'despite', 'fact', 'already', 'settled', 'payment', 'late', 'payment', 'fault', 'bank', 'failed', 'communicate', 'payment', 'due', 'appreciate', 'late', 'payment', 'remove', 'credit', 'report', 'experian', 'filled', 'dispute', 'remove', 'bank', 'claimed', 'valid', 'charge', 'explained', 'already', 'removed', 'late', 'payment', 'report', 'always', 'ensure', 'make', 'payment', 'time']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "tokenizer_simple = nltk.tokenize.word_tokenize\n",
    "try:\n",
    "  tokenized_text = train_text[USE_COL].apply(tokenizer_simple)\n",
    "except:\n",
    "  nltk.download('punkt_tab')\n",
    "  tokenized_text = train_text[USE_COL].apply(tokenizer_simple)\n",
    "\n",
    "tokenized_text = tokenized_text.apply(lambda x: [word.lower() for word in x])\n",
    "sample_tok = tokenized_text[SAMP]\n",
    "print(\"Sample:\")\n",
    "print(\"\\tDimensions:\", len(sample_tok))\n",
    "print(\"\\tSample:\", sample_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions\n",
    "\n",
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorizer():\n",
    "    def __init__(self, max_tokens):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "    def fit_on_texts(self, texts):\n",
    "        self.vocab = {word: i+1 for i, word in enumerate(set([word for text in texts for word in text]))}\n",
    "        self.reverse_vocab = {i: word for word, i in self.vocab.items()}\n",
    "        #limit to max tokens\n",
    "        tokens_to_keep = list(self.vocab.keys())[:self.max_tokens]\n",
    "        self.vocab = {word: i+1 for i, word in enumerate(tokens_to_keep)}\n",
    "        self.reverse_vocab = {i: word for word, i in self.vocab.items()}\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.vocab[word] for word in text if word in self.vocab] for text in texts]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Sequences\n",
    "\n",
    "We are going to be a little slack in the construction of the datasets for training because we are limited in the amount of resources we can handle. All of the datasets that are fed to the model need to be the same length, so we'll set a cap and trucate it here for resource concerns. Our dataset will be constructed as:\n",
    "<ul>\n",
    "<li> A sequence of (up to) 24 words as the X data. \n",
    "<li> The next word as the Y data.\n",
    "</ul>\n",
    "\n",
    "So each sequence is effectively one set of features, and its target is the next word. If we were doing this in reality, we'd want to prep more records from our sample:\n",
    "<ul>\n",
    "<li> Suppose a sample sentence is \"The quick brown fox jumps over the lazy dog\". Our ideal data would have something like:\n",
    "    <ul> \n",
    "    <li>X = \"the quick brown\", Y = \"fox\"\n",
    "    <li> X = \"quick brown fox\", Y = \"jumps\"\n",
    "    <li> X = \"brown fox jumps\", Y = \"over\"\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This is superior both because we are generating much more data to train the model and because we are training the model to predict words in all different positions in the sentence. We'd be predicting almost every word in the training dataset. The words that frequently end a sentence are not necessarily the same as the words that start a sentence or sit in the middle, so making predictions up and down the text will likely lead to a more useful model.\n",
    "\n",
    "<b>We'd end up with a better model if we generated more sequences from our data and/or added more data. The resource demands make that tough, so we have cut some corners that are easy to remedy in a real-world scenario.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'tue', 'subject', 'please', 'investigate', 'comenity', 'bank', 'retailer', 'card', 'scam', 'sent', 'hello', 'name', 'scammed', 'comenity', 'bank', 'credit', 'card', 'provider', 'company', 'childrens', 'place', 'new', 'york', 'forever', 'victoria', 'secret', 'original', 'credit', 'comenity', 'bank', 'lower', 'limit', 'began', 'charge', 'overage', 'fee', 'along', 'late', 'fee', 'began', 'pay', 'close', 'attention', 'card', 'find', 'limit', 'also', 'changed', 'well', 'incurring', 'overage', 'late', 'fee', 'reached', 'company', 'comenity', 'bank', 'stated', 'would', 'change', 'credit', 'limit', 'original', 'limit', 'reached', 'told', 'summit', 'payment', 'account', 'corrected', 'comenity', 'bank', 'credit', 'card', 'impacted', 'credit', 'score', 'plummeted', 'negative', 'status', 'im', 'currently', 'paying', 'price', 'due', 'corruption', 'affected', 'detrimental', 'way', 'debt', 'due', 'company', 'charging', 'overage', 'fee', 'well', 'late', 'fee', 'even', 'initial', 'credit', 'limit', 'fluctuating', 'tremendously', 'company', 'charge', 'major', 'fee', 'account', 'willing', 'correct', 'account', 'nervous', 'said', 'attorney', 'reason', 'im', 'reaching', 'im', 'employee', 'company', 'ruining', 'credit', 'plz', 'help', 'name', 'contact']\n"
     ]
    }
   ],
   "source": [
    "def construct_input_sequences(data, seq_length, max_sequences, max_per_doc=5):\n",
    "    input_sequences = []\n",
    "    while len(input_sequences) < max_sequences:\n",
    "        for doc in data:\n",
    "            sequences_for_doc = []\n",
    "            num_sequence_for_doc = 0\n",
    "            for i in range(len(doc) - seq_length):\n",
    "                if num_sequence_for_doc < max_per_doc:\n",
    "                    sequences_for_doc.append(doc[i:i + seq_length])\n",
    "                    num_sequence_for_doc += 1\n",
    "                else:\n",
    "                    break\n",
    "            input_sequences.extend(sequences_for_doc)\n",
    "    \n",
    "    return input_sequences\n",
    "input_sequences = construct_input_sequences(tokenized_text, SEQ_LEN, MAX_SEQ)\n",
    "print(input_sequences[SAMP])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize\n",
    "\n",
    "We can take our tokenized data and convert it to a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 22\n",
      "Sample: [1408, 1955, 2983, 380, 1562, 534, 378, 2506, 2469, 1562, 534, 378, 430, 2757, 1955, 1892, 378, 378, 378, 2882, 1033, 69]\n",
      "Sample text: every\n"
     ]
    }
   ],
   "source": [
    "vect_sequences = SimpleVectorizer(max_tokens=VOCAB_SIZE)\n",
    "vect_sequences.fit_on_texts(input_sequences)\n",
    "tokenized_sequences = vect_sequences.texts_to_sequences(input_sequences)\n",
    "SAMP = np.random.randint(0, len(tokenized_sequences))\n",
    "sample_tok = tokenized_sequences[SAMP]\n",
    "print(\"Sample:\")\n",
    "print(\"\\tDimensions:\", len(sample_tok))\n",
    "print(\"\\tSample:\", sample_tok)\n",
    "print(\"\\tSample text:\", vect_sequences.reverse_vocab[sample_tok[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences\n",
    "\n",
    "We need to pad our sequences so that they are all the same length. This is a requirement of the LSTM model. Note that we want the padding at the start of the sequence, not the end. This is because we are trying to predict the next word in the sequence, so we want the model to see the words that come before it. If we pad at the end, the model will not be able to see the words that come before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (124188, 128)\n",
      "type: <class 'numpy.ndarray'>\n",
      "X_t shape: (124188, 127)\n",
      "Y_t shape: (124188,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "padded_sequences = pad_sequences(tokenized_sequences, maxlen=SEQ_LEN, padding='pre', truncating='post')\n",
    "print(\"Dimensions:\", padded_sequences.shape)\n",
    "print(\"type:\", type(padded_sequences))\n",
    "padded_input_sequences = padded_sequences\n",
    "\n",
    "X_t = padded_input_sequences[:, :-1]\n",
    "Y_t = padded_input_sequences[:, -1].ravel()\n",
    "print(\"X_t shape:\", X_t.shape)\n",
    "print(\"Y_t shape:\", Y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 1408 1955 2983  380 1562  534\n",
      "  378 2506 2469 1562  534  378  430 2757 1955 1892  378  378  378 2882\n",
      " 1033]\n",
      "Y: 69\n",
      "X shape: (127,)\n",
      "Y shape: ()\n"
     ]
    }
   ],
   "source": [
    "sample_x_record = X_t[SAMP]\n",
    "sample_y_record = Y_t[SAMP]\n",
    "print(\"X:\", sample_x_record)\n",
    "print(\"Y:\", sample_y_record)\n",
    "print(\"X shape:\", sample_x_record.shape)\n",
    "print(\"Y shape:\", sample_y_record.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_save_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='simple_generative_weights_1.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_hub.api.metrics' has no attribute 'perplexity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[205]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m model.add(layers.Dense(EMBED_DIM, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     12\u001b[39m model.add(layers.Dense(VOCAB_SIZE, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     13\u001b[39m model.compile(\n\u001b[32m     14\u001b[39m     optimizer=keras.optimizers.Adam(),\n\u001b[32m     15\u001b[39m     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mkeras_hub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperplexity\u001b[49m()],\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     21\u001b[39m history = model.fit(\n\u001b[32m     22\u001b[39m     X_t, \n\u001b[32m     23\u001b[39m     Y_t,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     callbacks=[weight_save_callback],\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras_hub.api.metrics' has no attribute 'perplexity'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  del tokenized_sequences, tokenized_text, input_sequences\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM))\n",
    "model.add(layers.LSTM(EMBED_DIM, return_sequences=True))\n",
    "model.add(layers.LSTM(EMBED_DIM))\n",
    "model.add(layers.Dense(EMBED_DIM, activation='relu'))\n",
    "model.add(layers.Dense(VOCAB_SIZE, activation='softmax'))\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARN_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    #train_ds_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[weight_save_callback],\n",
    ")\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Text\n",
    "\n",
    "We can generate text once the model is trained. We'll start with a seed sentence, and then we'll use the model to predict the next word. We'll then append that word to the sentence and use the model to predict the next word, and so on. There is a little helper function to do this for us with limited repetition.\n",
    "\n",
    "Another new thing is that we create an inverse dictionary to map the encoded words back to the original words.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "One weirdly named factor that is important in text generation is the temperature. The temperature is a factor that we can use to control the randomness of the output. To generate text, we are essentially using a probability distribution to determine what the next word should be - the softmax output of the model will tell us the most likely next word. The issue is that certain words are way more likely than others - \"the\", \"it\", \"a\", \"and\", etc. are all very common words so we can expect the model to predict them as \"most likely\" a lot, probably too often. \n",
    "\n",
    "![Temperature](images/temperature.gif \"Temperature\")\n",
    "\n",
    "The most direct way to combat this is to add some degree of randomness to which word we select - we'll still pick the most likely word more often than any other, but we'll also pick other words that have some degree of likelihood at random. The higher the temperature the more randomness is introduced. A correct value requires tuning with human feedback, and it'll vary depending on the base quality of the model - large models that are trained on huge volumes of text and are deep enough to pick up on the \"what type of word should be here\" patterns will be able to generate better text with lower temperatures. Our model here is small and kind of sucks, so the temperature needs to be higher to get anything remotely usable. The implementation here is stolen shamelessly from the internet, the details don't really matter all that much, we just need to vary our predictions away from always simply picking the most likely word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import string\n",
    "import re\n",
    "def text_to_word_sequence(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    seq = text.split()\n",
    "    return seq\n",
    "def sampleWord(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds.flatten(), 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def gen_text(seed_text, next_words, model, length_max, tok, inverse, temperature=1.0, check_in_vocab=True):\n",
    "    out = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tok.texts_to_sequences([out])[0]\n",
    "        if check_in_vocab:\n",
    "            for tok_n in token_list:\n",
    "                if tok_n not in tok.vocab.values():\n",
    "                    token_list.remove(tok_n)\n",
    "        token_list = pad_sequences([token_list], maxlen=length_max-1, padding='pre')\n",
    "        #print(token_list)\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        #print(tok.sequences_to_texts(predicted))\n",
    "        word=\"\"\n",
    "        try:\n",
    "            word = inverse[sampleWord(predicted, temperature=temperature)]\n",
    "        except:\n",
    "            pass\n",
    "        out += \" \"+word\n",
    "        #print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake Text Time!\n",
    "\n",
    "<b>Note: we haven't built in any error handling, so it is possible to get random errors with unknown seeds. </b> In the real world, parsing the inputs from the user would be a more serious task, but for this example, we can just ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/ipykernel_21887/2801008207.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: The customer service was\n",
      "Generated text: The customer service was\n",
      "Seed text: I would like to complain about\n",
      "Generated text: I would like to complain about\n",
      "Seed text: The product was defective\n",
      "Generated text: The product was defective\n"
     ]
    }
   ],
   "source": [
    "seed_text_1 = \"The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.\"\n",
    "seed_text_2 = \"I would like to complain about\"\n",
    "seed_text_3 = \"Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight\"\n",
    "\n",
    "output_1 = gen_text(seed_text_1, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_2 = gen_text(seed_text_2, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_3 = gen_text(seed_text_3, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "print(\"Seed text:\", seed_text_1)\n",
    "print(\"Generated text:\", output_1)\n",
    "print(\"Seed text:\", seed_text_2)\n",
    "print(\"Generated text:\", output_2)\n",
    "print(\"Seed text:\", seed_text_3)\n",
    "print(\"Generated text:\", output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Old Weights\n",
    "\n",
    "I've run several rounds of training this model, and I've posted the weights online. The code here will load it, so we can expect our results to be a bit more logical here. \n",
    "\n",
    "<b>Note:</b> On the whole, our training process - in terms of data, time, and model size, is very tiny. Real text generation models are orders of magnitude larger, so don't expect much. \n",
    "\n",
    "##### Callback Example\n",
    "\n",
    "We can also generate text at the end of each epoch using a callback. Here we will print it, but it could also be logged to any location, such as the tensorboard logs. This is a good example of a simple, custom callback. We could bulk this out a little by creating a full class if we needed, and allow for things like multiple seeds, multiple generations, logging only every N epochs, etc... that's a good exercsie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading old weights.\n",
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.1511 - loss: 7.4653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/ipykernel_21887/2801008207.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text after epoch 0: The customer service was greed arguing survivorship\n",
      "\n",
      "\n",
      "Epoch 1: loss did not improve from 5.24588\n",
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 207ms/step - accuracy: 0.1510 - loss: 7.4632 - val_accuracy: 0.1151 - val_loss: 5.6416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3124678c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_seed_text = \"We're off to see the wizard, the wonderful wizard of\"\n",
    "gen_text_callback = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(\"\\nGenerated text after epoch {}: {}\\n\".format(epoch, gen_text(callback_seed_text, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)))\n",
    ")\n",
    "\n",
    "if have_old_weights and use_old_weights:\n",
    "    print(\"Loading old weights.\")\n",
    "    try:\n",
    "      model.load_weights(old_weights)\n",
    "      print(\"Old weights loaded.\")\n",
    "    except:\n",
    "      print(\"Error loading old weights.\")\n",
    "      pass\n",
    "else:\n",
    "    print(\"No old weights found, starting from scratch.\")\n",
    "\n",
    "model.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[gen_text_callback, weight_save_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Model with Bi-Directional LSTM\n",
    "\n",
    "We can also build a larger model with a bi-directional LSTM. This is a more complex model that will take longer to train, but it should be able to generate better text. The bi-directional LSTM will look at the text in both directions, which should help it understand the context of the words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi = keras.Sequential()\n",
    "model_bi.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM))\n",
    "\n",
    "model_bi.add(layers.Bidirectional(layers.LSTM(EMBED_DIM, return_sequences=True)))\n",
    "model_bi.add(layers.Bidirectional(layers.LSTM(EMBED_DIM)))\n",
    "model_bi.add(layers.Dense(EMBED_DIM, activation='relu'))\n",
    "model_bi.add(layers.Dense(VOCAB_SIZE, activation='softmax'))\n",
    "model_bi.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARN_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history_bi = model_bi.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[gen_text_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = gen_text(seed_text_1, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_2 = gen_text(seed_text_2, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_3 = gen_text(seed_text_3, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "print(\"Seed text:\", seed_text_1)\n",
    "print(\"Generated text:\", output_1)\n",
    "print(\"Seed text:\", seed_text_2)\n",
    "print(\"Generated text:\", output_2)\n",
    "print(\"Seed text:\", seed_text_3)\n",
    "print(\"Generated text:\", output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Create Generative Model\n",
    "\n",
    "Grab some text, try it. There are a bunch in the keras datasets, or on Kaggle. \n",
    "\n",
    "The core approach can be very similar for any column of data. You can also try other details, notably some parts that we did manually can be done more directly with some keras functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample example text:\n",
      "\tTokens:\n",
      " [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "\tText:\n",
      " ['the', 'wattie', 'nondiscriminatory', 'mln', 'loss', 'for', 'plc', 'said', 'at', 'only', 'ended', 'said', 'commonwealth', 'could', '1', 'traders', 'now', 'april', '0', 'a', 'after', 'said', 'from', '1985', 'and', 'from', 'foreign', '000', 'april', '0', 'prices', 'its', 'account', 'year', 'a', 'but', 'in', 'this', 'mln', 'home', 'an', 'states', 'earlier', 'and', 'rise', 'and', 'revs', 'vs', '000', 'its', '16', 'vs', '000', 'a', 'but', '3', 'psbr', 'oils', 'several', 'and', 'shareholders', 'and', 'dividend', 'vs', '000', 'its', 'all', '4', 'vs', '000', '1', 'mln', 'agreed', 'largely', 'april', '0', 'are', '2', 'states', 'will', 'billion', 'total', 'and', 'against', '000', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n",
      "\tText:\n",
      " ['the', 'termination', 'payment', 'airport', 'takes', '6', 'visibility', 'geological', '3', '6', '602', 'begin', 'up', 'said', 'fully', 'bank', 'expects', 'commodity', 'total', 'is', 'giant', 'a', 'recreation', 'this', 'takes', 'leroy', 'series', 'termination', 'payment', 'airport', 'mln', 'a', 'for', 'capital', '1', 'pre', '50', 'american', 'east', 'said', 'in', 'council', 'takes', 'leroy', \"recommend's\", 'france', 'a', 'but', 'u', 'any', '4', 's', '1st', 'losses', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12]\n",
      "\tText:\n",
      " ['the', 'lt', 'dlrs', 'demand', '000', 'reuter', 'dividend', 'year', 'lt', 'plus', 'billion', '04', '000', 'reuter', 'dividend', 'year', 'an', 'worth', 'new', 'vs', 'reuter', 'dlrs', 'warburg', 'on', 'shrs', 'earnings', 'countries', 'new', 'vs', 'reuter', '1985', 'billion', 'vs', '2', 'lt', '4', 'division', '000', 'reuter', 'from', 'go', '000', 'lt', 'plus', 'which', 'mid', '000', 'reuter', 'from', 'total', '000', 'an', '71', 'billion', 'vs', 'reuter', 'dlr', 'also', 'vs', 'shrs', 'earnings', 'countries', '4', 'vs', 'reuter', '1985', 'from', 'vs', 'some', 'now', 'april', '0', 'related', 'in', 'corp', 'it', 'inc', 'strong', 'cents', 'dollar', 'were', 'after', 'april', '0', 'crisis', 'or', 'ontario', 'more', 'index', '10', 'electric', 'company', 'taking', 'report', 'it', 'in', 'estimated', 'but', 'trading', 'texas', 'said', 'united', 'said', 'came', 'a', 'advising', 'up', 'said', 'countries', 'vs', '000', '3', 'delayed', 'central', 'said', 'which', 'objections', 'on', 'future', '617', 'said', 'came', 'a', 'includes', 'refile', 'profit', 'said', 'meeting', 'trade', 'vs', '3', 'supplie', 'up', 'said', '1985', 'were', 'vs', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12]\n",
      "\tText:\n",
      " ['the', 'in', 'currencies', 'hit', 'firms', 'in', 'has', 'would', 'seven', 'jointly', 'those', 'taiwan', '226', 'over', 'nigel', '9', '500', 's', '000', 'a', 'income', 'csr', 'that', 'at', '234', 'thyh', 'yielding', '7', '9', 'inc', 'british', 'said', 'at', 'those', 'our', 'justice', 'in', '24', 'accepted', 'financing', 'conrac', 'mln', 'a', 'know', 'primary', 'it', 'believe', 'and', 'in', 'case', 'seven', 'york', '686', 'assumes', '49', 'leaves', 'england', 'said', 'conrac', 'two', 'mln', 'by', 'for', 'meetings', 'travel', 'said', 'value', 'recently', 'a', 'linares', 'american', 'margarine', 'proved', 'planning', 'loss', '90', 'loss', 'inc', 'can', 'said', 'mcandrews', 'yielding', 'plan', 'holding', 'market', 'decline', 'its', 'in', 'way', 'a', 'but', '5', 'will', 'surpised', 'month', 'in', 'value', 'ago', 'in', 'way', 'as', 'value', 'ucpb', 'unsubsidised', 'european', 'release', 'for', 'comprised', 'said', 'american', 'cpb', 'preliminary', 'said', '018', 'dw', 'that', 'horn', 'global', 'case', 'seven', 'york', 'i', '652', 'extraction', 'process', 'groundwork', 'optioon', 'in', 'value', 'margarine', 'recently', '481', 'mcandrews', '564', 'nonbelligerent', 'amid', 'with', 'for', 'indian', 'by', 'stock', '5', 'name', 'for', 'bangemann', 'planting', 'a', 'in', 'wake', 'pipelines', 'lbs', 'its', 'supply', 'pier', '086', 'said', 'have', 'context', 'behind', 'issuing', 'for', 'arab', \"don't\", '8', 'last', 'amid', 'etl', 'in', 'market', 'williams', 'holdings', 'in', 'way', 'share', 'for', 'urge', 'and', '5865', 'at', 'account', 's', 'end', 'our', 'justice', 'a', 'for', '8231', 'receive', \"regime's\", 'kenya', 'assumes', 'said', 'conrac', 'hadson', 'tully', '226', 'mln', 'in', 'value', 'european', 'effot', 'ago', 'winds', 'and', 'leaving', 'by', 'mcandrews', '5', 'qtr', 'decline', 'department', 'and', 'yielding', 'plan', 'share', 'for', 'expensive', 'support', 'said', 'nervousness', 'interest', 'next', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 22, 265, 9, 621, 575, 1080, 4742, 1149, 15874, 6, 438, 8295, 13, 102, 388, 15, 90, 67, 7, 197, 8295, 8, 4, 270, 416, 23, 527, 6, 15874, 4891, 4, 1055, 742, 16, 8, 36, 1480, 6, 2124, 100, 543, 5, 645, 362, 6, 2912, 4, 49, 8, 15874, 976, 124, 20, 5, 8295, 80, 9, 100, 362, 543, 395, 61, 44, 20, 8295, 8, 16, 40, 1276, 42, 1436, 166, 415, 6, 888, 4, 116, 9, 40, 3089, 4, 303, 163, 16, 64, 772, 13, 94, 156, 17, 12]\n",
      "\tText:\n",
      " ['the', 'bleached', 'could', 'mln', 'at', 'world', 'as', 'holding', 'for', 'include', 'its', 'i', '3', 'start', 'measures', 'gnp', '525', 'process', 'ccb', 'and', 'nations', 'bleached', 'it', '1985', 'do', '000', 'april', '0', 'a', 'agreed', 'bleached', 'mln', 'in', 'ended', 'cost', 'cts', 'must', 'and', 'ccb', 'tenneco', 'in', 'winter', '53', '1', 'mln', 'net', 'diplomats', 'and', 'reorganization', 'group', '38', 'said', '49', '26', 'and', 'plastics', 'in', 'this', 'mln', 'ccb', 'field', 'foreign', 'is', 'said', 'bleached', '10', '3', 'group', '26', '38', 'producers', 'had', '4', 'is', 'bleached', 'mln', '1', 'as', 'equivalent', 'not', '145', 'world', 'york', 'and', 'credits', 'in', '20', '3', 'as', 'permits', 'in', 'set', 'board', '1', 'share', 'turnover', 'it', 'than', 'growth', 'pct', 'dlrs']\n",
      "Sample example text shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Get Text Data\n",
    "\n",
    "sample_example_text = keras.datasets.reuters.load_data()\n",
    "reuters_tokenizer = keras.datasets.reuters.get_word_index()\n",
    "inverse_reuters_tokenizer = {v: k for k, v in reuters_tokenizer.items()}\n",
    "\n",
    "def index_to_word(index):\n",
    "    return str(reuters_tokenizer[index])\n",
    "\n",
    "# preview 5 records\n",
    "sample_example_text = sample_example_text[0][0][:5]\n",
    "print(\"Sample example text:\")\n",
    "for i in range(5):\n",
    "    as_text = [inverse_reuters_tokenizer.get(x, x) for x in sample_example_text[i]]\n",
    "    indicies = np.array(sample_example_text[i])\n",
    "    print(\"\\tTokens:\\n\", sample_example_text[i])\n",
    "    print(\"\\tText:\\n\", as_text)\n",
    "print(\"Sample example text shape:\", sample_example_text.shape)\n",
    "sample_example_text = sample_example_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mar_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
