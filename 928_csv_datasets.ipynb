{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.utils import Sequence\n",
    "from keras.utils import Sequence\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Structured Data\n",
    "\n",
    "When dealing with large amounts of structured text data we can also do some stuff to speed things up, though there are some key differences that lessen our toolkit:\n",
    "<ul>\n",
    "<li> Data manipulation (filtering out features, customized data cleanup, etc...) is far easier to do in a tabular format like a dataframe. If there is going to be a lot of that, and the data is really large, we can do the 'manual' prep separately, write the data to a file, and then read it in again as ready-to-use data. </li>\n",
    "<li> As the data gets really large, most real life scenarios will either use big data approaches like Spark, or store structured data in a DB. That's the 'real' way to deal with large amounts of strucutred data, so there are not as many easy to use tools for this as we find with images. </li>\n",
    "<li> Further to the two ponts above, if the dataset is a CSV, we can likely load it into memory in its entirety as \"too many rows to fit in memory\" and \"this data is stored in a CSV file\" tend not to come around together all that often in a situation where there is actual infrastructure. </li>\n",
    "<li> Really large amounts of text can be broken into multiple smaller files, then we load a file at a time, similar to how we deal with images. This is common with NLP text, much more so than structured data. </li>\n",
    "<li> There is often an assumption that when needing to deal with large amounts of structured CSV data that we have the data already split into training and validation sets. This makes sense, as \n",
    "</ul>\n",
    "\n",
    "On the whole, dealing with large amounts of structured data tends to not be as large of an issue to be solved as dealing with large amounts of unstrucutured data in a non-big data environment. This is because huge data goes to big data strategies, or at least a DB, less huge data can just fit in memory and be dealt with how we have dealt with all other CSV based data. \n",
    "\n",
    "## TensorFlow Datasets\n",
    "\n",
    "Tensorflow Datasets are something that we used when loading image files from disk, as loading all of the data at once can be impossible for larger datasets. These datasets serve the same function as a regular dataframe for modle training purposes, but they are designed more to be able to efficiently load large amounts of data from disk than to allow easy viewing and manipulation of the data. \n",
    "\n",
    "Tensorflow datasets allow us to set several options on how the data is loaded, that we can use to make a dataset that is more efficient for our purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for Structured CSV\n",
    "\n",
    "The function below reads CSV data from disk and generates training and validation datasets that we can feed to our model. We also add batching, shuffle the training data, and use prefetch to make the data loading process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the CSV data and create a tf.data.Dataset object\n",
    "def create_dataset(csv_path, batch_size=32, buffer_size=1024, validation_split=0.2, shuffle=True, start=None):\n",
    "    # Load the CSV data\n",
    "    with open(csv_path) as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        header = next(csv_reader)\n",
    "        feature_names = header[start:-1]\n",
    "        label_name = header[-1]\n",
    "        features = []\n",
    "        labels = []\n",
    "        for row in csv_reader:\n",
    "            features.append([float(x) for x in row[start:-1]])\n",
    "            labels.append(float(row[-1]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    split_idx = int(len(features) * (1.0 - validation_split))\n",
    "    train_features, train_labels = features[:split_idx], labels[:split_idx]\n",
    "    val_features, val_labels = features[split_idx:], labels[split_idx:]\n",
    "\n",
    "    # Create a tf.data.Dataset object for the training data\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    #train_ds = keras.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    #train_ds = keras.utils.\n",
    "    train_ds = train_ds.cache()\n",
    "    if shuffle:\n",
    "        train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Create a tf.data.Dataset object for the validation data\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    #val_ds = keras.data.Dataset.from_tensor_slices((val_features, val_labels)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.cache()\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "BASE_EPOCHS  = 20\n",
    "VAL_SPLIT = 0.2\n",
    "#DIABETES_CSV_PATH = 'diabetes.csv'\n",
    "DIABETES_CSV_PATH = \"data/diabetes.csv\"\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIABETES_CSV_PATH):\n",
    "    #url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/diabetes.csv'\n",
    "    url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/diabetes.csv\"\n",
    "    #d_path = tf.keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    d_path = keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(d_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple and Small Example\n",
    "\n",
    "We can test the generator on a small file. \n",
    "\n",
    "<b>Note:</b> with small examples, we won't really see any advantage in terms of speed as we can probably just load the data into memory without concern, no matter what. This starts to matter more when dealing with large files, where the disk access time can actually add up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679ms/step - accuracy: 0.3746 - loss: 20.4461 - val_accuracy: 0.4610 - val_loss: 16.0785\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3827 - loss: 17.1923 - val_accuracy: 0.4870 - val_loss: 13.4485\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3974 - loss: 14.3821 - val_accuracy: 0.5065 - val_loss: 11.1874\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4218 - loss: 11.8669 - val_accuracy: 0.5000 - val_loss: 9.0443\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4528 - loss: 9.5428 - val_accuracy: 0.5065 - val_loss: 7.0886\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4756 - loss: 7.4423 - val_accuracy: 0.5195 - val_loss: 5.4910\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5114 - loss: 5.6665 - val_accuracy: 0.5130 - val_loss: 4.0937\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5309 - loss: 4.0654 - val_accuracy: 0.4740 - val_loss: 3.2267\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5130 - loss: 2.9536 - val_accuracy: 0.4286 - val_loss: 3.5342\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5228 - loss: 3.1469 - val_accuracy: 0.4091 - val_loss: 3.8370\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5179 - loss: 3.4009 - val_accuracy: 0.5390 - val_loss: 4.4540\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6091 - loss: 3.8361 - val_accuracy: 0.5325 - val_loss: 5.0875\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6287 - loss: 4.3101 - val_accuracy: 0.5455 - val_loss: 5.4455\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6368 - loss: 4.5853 - val_accuracy: 0.5455 - val_loss: 5.5441\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6401 - loss: 4.6556 - val_accuracy: 0.5519 - val_loss: 5.4240\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6417 - loss: 4.5490 - val_accuracy: 0.5455 - val_loss: 5.1271\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6401 - loss: 4.2979 - val_accuracy: 0.5584 - val_loss: 4.6899\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6450 - loss: 3.9316 - val_accuracy: 0.5649 - val_loss: 4.1469\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6368 - loss: 3.4774 - val_accuracy: 0.5519 - val_loss: 3.5373\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6450 - loss: 2.9687 - val_accuracy: 0.5325 - val_loss: 2.9650\n",
      "DS Training time: 1.243988037109375 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "#train_ds, val_ds = create_dataset(d_path, batch_size=BATCH_SIZE)\n",
    "train_ds, val_ds = create_dataset(DIABETES_CSV_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step - accuracy: 0.3469 - loss: 16.6783 - val_accuracy: 0.3571 - val_loss: 7.2215\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3485 - loss: 7.3201 - val_accuracy: 0.6948 - val_loss: 1.2775\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6221 - loss: 1.4925 - val_accuracy: 0.6429 - val_loss: 4.4151\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6401 - loss: 4.4495 - val_accuracy: 0.6429 - val_loss: 5.6441\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6515 - loss: 5.5607 - val_accuracy: 0.6429 - val_loss: 5.4208\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6531 - loss: 5.2286 - val_accuracy: 0.6429 - val_loss: 4.1972\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6515 - loss: 3.9019 - val_accuracy: 0.6234 - val_loss: 2.5235\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6564 - loss: 2.1727 - val_accuracy: 0.5325 - val_loss: 2.0444\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6140 - loss: 1.9066 - val_accuracy: 0.5455 - val_loss: 2.8038\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4919 - loss: 2.7597 - val_accuracy: 0.4610 - val_loss: 3.5423\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4349 - loss: 3.5389 - val_accuracy: 0.4416 - val_loss: 3.2476\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4283 - loss: 3.2826 - val_accuracy: 0.5130 - val_loss: 2.0062\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5130 - loss: 2.0983 - val_accuracy: 0.6494 - val_loss: 0.9175\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6498 - loss: 0.9976 - val_accuracy: 0.6364 - val_loss: 1.5574\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6466 - loss: 1.5634 - val_accuracy: 0.6364 - val_loss: 2.2200\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6482 - loss: 2.2845 - val_accuracy: 0.6364 - val_loss: 2.2380\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6466 - loss: 2.3634 - val_accuracy: 0.6364 - val_loss: 1.7014\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6417 - loss: 1.8839 - val_accuracy: 0.7208 - val_loss: 0.9947\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6401 - loss: 1.2036 - val_accuracy: 0.5844 - val_loss: 0.9683\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5358 - loss: 1.1739 - val_accuracy: 0.3701 - val_loss: 1.5889\n",
      "DS Training time: 1.8731210231781006 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time dataframe for comparison\n",
    "df_small = pd.read_csv(DIABETES_CSV_PATH)\n",
    "df_small_y = df_small[\"Outcome\"]\n",
    "df_small_X = df_small.drop(columns=[\"Outcome\"])\n",
    "width = df_small_X.shape[1]\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(width,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(x=df_small_X, y=df_small_y, epochs=BASE_EPOCHS, validation_split=0.2, batch_size=BATCH_SIZE)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Example\n",
    "\n",
    "We can download a larger file, and try it out. We ill also use the .cache() method to cache the data in memory, so that we don't have to reload it every time we run the code. This CSV file is roughly 150mb in size, so it is large enough to be noticable when we need to load the entire thing, but small enough to fit in memory. For most CSV data that we might encounter, this is probably a good approach - most systems can handle the memory demands of the CSV file size we might see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv: None -- [Errno 8] nodename nor servname provided, or not known",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1472\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1470\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1472\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/http/client.py:1003\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1002\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/socket.py:841\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    840\u001b[39m exceptions = []\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    842\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/socket.py:978\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    977\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    979\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages/keras/src/utils/file_utils.py:311\u001b[39m, in \u001b[36mget_file\u001b[39m\u001b[34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:240\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    238\u001b[39m url_type, path = _splittype(url)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.closing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    241\u001b[39m     headers = fp.info()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m r = h.getresponse()\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno 8] nodename nor servname provided, or not known>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(zip_name):\n\u001b[32m      5\u001b[39m     url = \u001b[33m'\u001b[39m\u001b[33mhttps://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     zip_path = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(zip_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages/keras/src/utils/file_utils.py:315\u001b[39m, in \u001b[36mget_file\u001b[39m\u001b[34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[39m\n\u001b[32m    313\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg.format(origin, e.code, e.msg))\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg.format(origin, e.errno, e.reason))\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(download_target):\n",
      "\u001b[31mException\u001b[39m: URL fetch failure on https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv: None -- [Errno 8] nodename nor servname provided, or not known"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "\n",
    "zip_name = 'fraud.zip'\n",
    "if not os.path.exists(zip_name):\n",
    "    #url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv'\n",
    "    url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv\"\n",
    "    zip_path = keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0204 - accuracy: 0.9990 - val_loss: 0.0056 - val_accuracy: 0.9996\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0103 - accuracy: 0.9993 - val_loss: 0.0058 - val_accuracy: 0.9996\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9996\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9996\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9996\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 0.9996\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 10s 1ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 11s 1ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0061 - val_accuracy: 0.9996\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0084 - val_accuracy: 0.9996\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 10s 1ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0040 - val_accuracy: 0.9995\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0044 - val_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0040 - val_accuracy: 0.9996\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0036 - val_accuracy: 0.9995\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0051 - val_accuracy: 0.9995\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 0.0045 - val_accuracy: 0.9995\n",
      "Time to fit:  250.69341111183167\n"
     ]
    }
   ],
   "source": [
    "#big_file = \"/Users/akeems/.keras/datasets/creditcard.csv\"\n",
    "big_file = zip_path\n",
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "train_ds, val_ds = create_dataset(big_file, start=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 18.3415 - accuracy: 0.9955 - val_loss: 5.7364 - val_accuracy: 0.9987\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 4.5446 - accuracy: 0.9960 - val_loss: 4.9170 - val_accuracy: 0.9987\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.4800 - accuracy: 0.9974 - val_loss: 0.1178 - val_accuracy: 0.9987\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0448 - accuracy: 0.9979 - val_loss: 0.0118 - val_accuracy: 0.9987\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0162 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0202 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0277 - accuracy: 0.9980 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0237 - accuracy: 0.9979 - val_loss: 0.0103 - val_accuracy: 0.9987\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0235 - accuracy: 0.9979 - val_loss: 0.0111 - val_accuracy: 0.9987\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0205 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0194 - accuracy: 0.9978 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0229 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0202 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0185 - accuracy: 0.9979 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0164 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0233 - accuracy: 0.9981 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.0103 - val_accuracy: 0.9987\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0157 - accuracy: 0.9982 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0150 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0156 - accuracy: 0.9982 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Time to fit:  267.8153579235077\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv(big_file)\n",
    "df_large_y = df_large[\"Class\"]\n",
    "df_large_X = df_large.drop(columns={\"Class\"})\n",
    "width = df_large_X.shape[1]\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(width,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.BinaryCrossentropy(),\n",
    "                metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "start = time.time()\n",
    "model.fit(x=df_large_X, y=df_large_y, epochs=BASE_EPOCHS, validation_split=VAL_SPLIT, batch_size=BATCH_SIZE)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to Dataset\n",
    "\n",
    "If we have a dataframe we can convert it to a dataset using the from_tensor_slices() method. Manipulating the data in a dataframe is far easier, so we can prep in a df then convert to a dataset. The function below creates a dataset from a dataframe, long with a few of the other things we commonly want to do in our data prep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_dataset(df, target=\"target\", val_split=0.2, batch_size=32):\n",
    "    # Splitting the dataframe into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)\n",
    "\n",
    "    # Extracting the target variable from the dataframes\n",
    "    train_y = train_df.pop(target)\n",
    "    val_y = val_df.pop(target)\n",
    "\n",
    "    # Converting the target variable to categorical if necessary\n",
    "    num_classes = len(train_y.unique())\n",
    "    if num_classes > 2:\n",
    "        train_y = to_categorical(train_y, num_classes)\n",
    "        val_y = to_categorical(val_y, num_classes)\n",
    "\n",
    "    # Creating a tf.data.Dataset for training and validation sets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_df.values, train_y))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_df.values, val_y))\n",
    "\n",
    "    train_ds =train_ds.cache()\n",
    "    val_ds = val_ds.cache()\n",
    "\n",
    "    # Shuffling and batching the datasets\n",
    "    train_ds = train_ds.shuffle(len(train_df)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 19s 2ms/step - loss: 10.5821 - accuracy: 0.9963 - val_loss: 5.5160 - val_accuracy: 0.9983\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 1.0853 - accuracy: 0.9965 - val_loss: 0.0512 - val_accuracy: 0.9901\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.1486 - accuracy: 0.9978 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0183 - accuracy: 0.9983 - val_loss: 0.0135 - val_accuracy: 0.9983\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0148 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 20s 3ms/step - loss: 0.0138 - accuracy: 0.9983 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0295 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0238 - accuracy: 0.9980 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 21s 3ms/step - loss: 0.0267 - accuracy: 0.9980 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0322 - accuracy: 0.9979 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0298 - accuracy: 0.9982 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0180 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0129 - val_accuracy: 0.9983\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0173 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0147 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 19s 3ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0150 - accuracy: 0.9983 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Time to fit:  348.6873939037323\n"
     ]
    }
   ],
   "source": [
    "train_ds_df, val_ds_df = get_keras_dataset(df_large, target=\"Class\", val_split=VAL_SPLIT, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds_df, epochs=BASE_EPOCHS, validation_data=val_ds_df)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars DataFrame\n",
    "\n",
    "We can also use the faster and more efficient Polars DataFrame to load the data. This is a DataFrame that is written in Rust, and is much faster than Pandas. Polars dataframes aren't promised to be a one-to-one replacement for Pandas, but they are very similar, and can be used in most cases where Pandas is used with few, if any, changes.\n",
    "\n",
    "#### Polars Specifics\n",
    "\n",
    "Polars offers a fair bit of stuff for performance, as that is it's main selling point. Among them:\n",
    "<ul>\n",
    "<li> Low memory parameter - this will try to load the data in a way that uses less memory, but may be slower. </li>\n",
    "<li> Lazy execution - Polars has options to work lazily, which means that it won't actually do work like loading data until it is needed. </li>\n",
    "<li> Parallel execution - Polars can use multiple threads to do work, which can speed things up. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install polars\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       ".pl-dataframe > thead > tr > th {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<table border=\"1\" class=\"dataframe pl-dataframe\">\n",
       "<small>shape: (5, 31)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "Time\n",
       "</th>\n",
       "<th>\n",
       "V1\n",
       "</th>\n",
       "<th>\n",
       "V2\n",
       "</th>\n",
       "<th>\n",
       "V3\n",
       "</th>\n",
       "<th>\n",
       "V4\n",
       "</th>\n",
       "<th>\n",
       "V5\n",
       "</th>\n",
       "<th>\n",
       "V6\n",
       "</th>\n",
       "<th>\n",
       "V7\n",
       "</th>\n",
       "<th>\n",
       "V8\n",
       "</th>\n",
       "<th>\n",
       "V9\n",
       "</th>\n",
       "<th>\n",
       "V10\n",
       "</th>\n",
       "<th>\n",
       "V11\n",
       "</th>\n",
       "<th>\n",
       "V12\n",
       "</th>\n",
       "<th>\n",
       "V13\n",
       "</th>\n",
       "<th>\n",
       "V14\n",
       "</th>\n",
       "<th>\n",
       "V15\n",
       "</th>\n",
       "<th>\n",
       "V16\n",
       "</th>\n",
       "<th>\n",
       "V17\n",
       "</th>\n",
       "<th>\n",
       "V18\n",
       "</th>\n",
       "<th>\n",
       "V19\n",
       "</th>\n",
       "<th>\n",
       "V20\n",
       "</th>\n",
       "<th>\n",
       "V21\n",
       "</th>\n",
       "<th>\n",
       "V22\n",
       "</th>\n",
       "<th>\n",
       "V23\n",
       "</th>\n",
       "<th>\n",
       "V24\n",
       "</th>\n",
       "<th>\n",
       "V25\n",
       "</th>\n",
       "<th>\n",
       "V26\n",
       "</th>\n",
       "<th>\n",
       "V27\n",
       "</th>\n",
       "<th>\n",
       "V28\n",
       "</th>\n",
       "<th>\n",
       "Amount\n",
       "</th>\n",
       "<th>\n",
       "Class\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "-1.359807\n",
       "</td>\n",
       "<td>\n",
       "-0.072781\n",
       "</td>\n",
       "<td>\n",
       "2.536347\n",
       "</td>\n",
       "<td>\n",
       "1.378155\n",
       "</td>\n",
       "<td>\n",
       "-0.338321\n",
       "</td>\n",
       "<td>\n",
       "0.462388\n",
       "</td>\n",
       "<td>\n",
       "0.239599\n",
       "</td>\n",
       "<td>\n",
       "0.098698\n",
       "</td>\n",
       "<td>\n",
       "0.363787\n",
       "</td>\n",
       "<td>\n",
       "0.090794\n",
       "</td>\n",
       "<td>\n",
       "-0.5516\n",
       "</td>\n",
       "<td>\n",
       "-0.617801\n",
       "</td>\n",
       "<td>\n",
       "-0.99139\n",
       "</td>\n",
       "<td>\n",
       "-0.311169\n",
       "</td>\n",
       "<td>\n",
       "1.468177\n",
       "</td>\n",
       "<td>\n",
       "-0.470401\n",
       "</td>\n",
       "<td>\n",
       "0.207971\n",
       "</td>\n",
       "<td>\n",
       "0.025791\n",
       "</td>\n",
       "<td>\n",
       "0.403993\n",
       "</td>\n",
       "<td>\n",
       "0.251412\n",
       "</td>\n",
       "<td>\n",
       "-0.018307\n",
       "</td>\n",
       "<td>\n",
       "0.277838\n",
       "</td>\n",
       "<td>\n",
       "-0.110474\n",
       "</td>\n",
       "<td>\n",
       "0.066928\n",
       "</td>\n",
       "<td>\n",
       "0.128539\n",
       "</td>\n",
       "<td>\n",
       "-0.189115\n",
       "</td>\n",
       "<td>\n",
       "0.133558\n",
       "</td>\n",
       "<td>\n",
       "-0.021053\n",
       "</td>\n",
       "<td>\n",
       "149.62\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "1.191857\n",
       "</td>\n",
       "<td>\n",
       "0.266151\n",
       "</td>\n",
       "<td>\n",
       "0.16648\n",
       "</td>\n",
       "<td>\n",
       "0.448154\n",
       "</td>\n",
       "<td>\n",
       "0.060018\n",
       "</td>\n",
       "<td>\n",
       "-0.082361\n",
       "</td>\n",
       "<td>\n",
       "-0.078803\n",
       "</td>\n",
       "<td>\n",
       "0.085102\n",
       "</td>\n",
       "<td>\n",
       "-0.255425\n",
       "</td>\n",
       "<td>\n",
       "-0.166974\n",
       "</td>\n",
       "<td>\n",
       "1.612727\n",
       "</td>\n",
       "<td>\n",
       "1.065235\n",
       "</td>\n",
       "<td>\n",
       "0.489095\n",
       "</td>\n",
       "<td>\n",
       "-0.143772\n",
       "</td>\n",
       "<td>\n",
       "0.635558\n",
       "</td>\n",
       "<td>\n",
       "0.463917\n",
       "</td>\n",
       "<td>\n",
       "-0.114805\n",
       "</td>\n",
       "<td>\n",
       "-0.183361\n",
       "</td>\n",
       "<td>\n",
       "-0.145783\n",
       "</td>\n",
       "<td>\n",
       "-0.069083\n",
       "</td>\n",
       "<td>\n",
       "-0.225775\n",
       "</td>\n",
       "<td>\n",
       "-0.638672\n",
       "</td>\n",
       "<td>\n",
       "0.101288\n",
       "</td>\n",
       "<td>\n",
       "-0.339846\n",
       "</td>\n",
       "<td>\n",
       "0.16717\n",
       "</td>\n",
       "<td>\n",
       "0.125895\n",
       "</td>\n",
       "<td>\n",
       "-0.008983\n",
       "</td>\n",
       "<td>\n",
       "0.014724\n",
       "</td>\n",
       "<td>\n",
       "2.69\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-1.358354\n",
       "</td>\n",
       "<td>\n",
       "-1.340163\n",
       "</td>\n",
       "<td>\n",
       "1.773209\n",
       "</td>\n",
       "<td>\n",
       "0.37978\n",
       "</td>\n",
       "<td>\n",
       "-0.503198\n",
       "</td>\n",
       "<td>\n",
       "1.800499\n",
       "</td>\n",
       "<td>\n",
       "0.791461\n",
       "</td>\n",
       "<td>\n",
       "0.247676\n",
       "</td>\n",
       "<td>\n",
       "-1.514654\n",
       "</td>\n",
       "<td>\n",
       "0.207643\n",
       "</td>\n",
       "<td>\n",
       "0.624501\n",
       "</td>\n",
       "<td>\n",
       "0.066084\n",
       "</td>\n",
       "<td>\n",
       "0.717293\n",
       "</td>\n",
       "<td>\n",
       "-0.165946\n",
       "</td>\n",
       "<td>\n",
       "2.345865\n",
       "</td>\n",
       "<td>\n",
       "-2.890083\n",
       "</td>\n",
       "<td>\n",
       "1.109969\n",
       "</td>\n",
       "<td>\n",
       "-0.121359\n",
       "</td>\n",
       "<td>\n",
       "-2.261857\n",
       "</td>\n",
       "<td>\n",
       "0.52498\n",
       "</td>\n",
       "<td>\n",
       "0.247998\n",
       "</td>\n",
       "<td>\n",
       "0.771679\n",
       "</td>\n",
       "<td>\n",
       "0.909412\n",
       "</td>\n",
       "<td>\n",
       "-0.689281\n",
       "</td>\n",
       "<td>\n",
       "-0.327642\n",
       "</td>\n",
       "<td>\n",
       "-0.139097\n",
       "</td>\n",
       "<td>\n",
       "-0.055353\n",
       "</td>\n",
       "<td>\n",
       "-0.059752\n",
       "</td>\n",
       "<td>\n",
       "378.66\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-0.966272\n",
       "</td>\n",
       "<td>\n",
       "-0.185226\n",
       "</td>\n",
       "<td>\n",
       "1.792993\n",
       "</td>\n",
       "<td>\n",
       "-0.863291\n",
       "</td>\n",
       "<td>\n",
       "-0.010309\n",
       "</td>\n",
       "<td>\n",
       "1.247203\n",
       "</td>\n",
       "<td>\n",
       "0.237609\n",
       "</td>\n",
       "<td>\n",
       "0.377436\n",
       "</td>\n",
       "<td>\n",
       "-1.387024\n",
       "</td>\n",
       "<td>\n",
       "-0.054952\n",
       "</td>\n",
       "<td>\n",
       "-0.226487\n",
       "</td>\n",
       "<td>\n",
       "0.178228\n",
       "</td>\n",
       "<td>\n",
       "0.507757\n",
       "</td>\n",
       "<td>\n",
       "-0.287924\n",
       "</td>\n",
       "<td>\n",
       "-0.631418\n",
       "</td>\n",
       "<td>\n",
       "-1.059647\n",
       "</td>\n",
       "<td>\n",
       "-0.684093\n",
       "</td>\n",
       "<td>\n",
       "1.965775\n",
       "</td>\n",
       "<td>\n",
       "-1.232622\n",
       "</td>\n",
       "<td>\n",
       "-0.208038\n",
       "</td>\n",
       "<td>\n",
       "-0.1083\n",
       "</td>\n",
       "<td>\n",
       "0.005274\n",
       "</td>\n",
       "<td>\n",
       "-0.190321\n",
       "</td>\n",
       "<td>\n",
       "-1.175575\n",
       "</td>\n",
       "<td>\n",
       "0.647376\n",
       "</td>\n",
       "<td>\n",
       "-0.221929\n",
       "</td>\n",
       "<td>\n",
       "0.062723\n",
       "</td>\n",
       "<td>\n",
       "0.061458\n",
       "</td>\n",
       "<td>\n",
       "123.5\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "2\n",
       "</td>\n",
       "<td>\n",
       "-1.158233\n",
       "</td>\n",
       "<td>\n",
       "0.877737\n",
       "</td>\n",
       "<td>\n",
       "1.548718\n",
       "</td>\n",
       "<td>\n",
       "0.403034\n",
       "</td>\n",
       "<td>\n",
       "-0.407193\n",
       "</td>\n",
       "<td>\n",
       "0.095921\n",
       "</td>\n",
       "<td>\n",
       "0.592941\n",
       "</td>\n",
       "<td>\n",
       "-0.270533\n",
       "</td>\n",
       "<td>\n",
       "0.817739\n",
       "</td>\n",
       "<td>\n",
       "0.753074\n",
       "</td>\n",
       "<td>\n",
       "-0.822843\n",
       "</td>\n",
       "<td>\n",
       "0.538196\n",
       "</td>\n",
       "<td>\n",
       "1.345852\n",
       "</td>\n",
       "<td>\n",
       "-1.11967\n",
       "</td>\n",
       "<td>\n",
       "0.175121\n",
       "</td>\n",
       "<td>\n",
       "-0.451449\n",
       "</td>\n",
       "<td>\n",
       "-0.237033\n",
       "</td>\n",
       "<td>\n",
       "-0.038195\n",
       "</td>\n",
       "<td>\n",
       "0.803487\n",
       "</td>\n",
       "<td>\n",
       "0.408542\n",
       "</td>\n",
       "<td>\n",
       "-0.009431\n",
       "</td>\n",
       "<td>\n",
       "0.798278\n",
       "</td>\n",
       "<td>\n",
       "-0.137458\n",
       "</td>\n",
       "<td>\n",
       "0.141267\n",
       "</td>\n",
       "<td>\n",
       "-0.20601\n",
       "</td>\n",
       "<td>\n",
       "0.502292\n",
       "</td>\n",
       "<td>\n",
       "0.219422\n",
       "</td>\n",
       "<td>\n",
       "0.215153\n",
       "</td>\n",
       "<td>\n",
       "69.99\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (5, 31)\n",
       "┌──────┬───────────┬───────────┬──────────┬─────┬───────────┬───────────┬────────┬───────┐\n",
       "│ Time ┆ V1        ┆ V2        ┆ V3       ┆ ... ┆ V27       ┆ V28       ┆ Amount ┆ Class │\n",
       "│ ---  ┆ ---       ┆ ---       ┆ ---      ┆     ┆ ---       ┆ ---       ┆ ---    ┆ ---   │\n",
       "│ i64  ┆ f64       ┆ f64       ┆ f64      ┆     ┆ f64       ┆ f64       ┆ f64    ┆ i64   │\n",
       "╞══════╪═══════════╪═══════════╪══════════╪═════╪═══════════╪═══════════╪════════╪═══════╡\n",
       "│ 0    ┆ -1.359807 ┆ -0.072781 ┆ 2.536347 ┆ ... ┆ 0.133558  ┆ -0.021053 ┆ 149.62 ┆ 0     │\n",
       "│ 0    ┆ 1.191857  ┆ 0.266151  ┆ 0.16648  ┆ ... ┆ -0.008983 ┆ 0.014724  ┆ 2.69   ┆ 0     │\n",
       "│ 1    ┆ -1.358354 ┆ -1.340163 ┆ 1.773209 ┆ ... ┆ -0.055353 ┆ -0.059752 ┆ 378.66 ┆ 0     │\n",
       "│ 1    ┆ -0.966272 ┆ -0.185226 ┆ 1.792993 ┆ ... ┆ 0.062723  ┆ 0.061458  ┆ 123.5  ┆ 0     │\n",
       "│ 2    ┆ -1.158233 ┆ 0.877737  ┆ 1.548718 ┆ ... ┆ 0.219422  ┆ 0.215153  ┆ 69.99  ┆ 0     │\n",
       "└──────┴───────────┴───────────┴──────────┴─────┴───────────┴───────────┴────────┴───────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file at zip path into a polars dataframe\n",
    "df_polar = pl.read_csv(zip_path, ignore_errors=True, low_memory=True)\n",
    "df_polar.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polars\n",
    "\n",
    "Polars doesn't have the same native support in TensorFlow as Pandas does, so we need to convert it to a Pandas dataframe or an array to feed it into any models. One thing that may be useful with Polars would be to split a very large csv into multiple smallers ones, that could then be loaded one at a time. Something like the function below could be adapted to load a csv into a Polars dataframe, do whatever data manipulation is needed, then write it out to several smaller csv files. The make_csv_dataset is able to natively read in multiple csv files. \n",
    "\n",
    "<b>Note:</b> If we were actually doing something like this, it is likely easier to do a train-validation-test split as we write the output into different subfolders. Manipulating data is easier in a dataframe than a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !mkdir polar_out\n",
    "  !mkdir polar_out/train\n",
    "  !mkdir polar_out/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "split = 0.2\n",
    "#Shuffle data. Takes a peak of 2x memory to do so. \n",
    "df_polar = df_polar.sample(frac=1.0)\n",
    "for frame in df_polar.iter_slices(n_rows=BATCH_SIZE):\n",
    "    record_batch = frame\n",
    "    if i % 5 == 0:\n",
    "        fname = \"polar_out/val/data_{}.csv\".format(i)\n",
    "    else:\n",
    "        fname = \"polar_out/train/data_{}.csv\".format(i)\n",
    "    record_batch.write_csv(fname)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Folder\n",
    "\n",
    "We can create datasets from a folder of csv files. This is useful if we have a large csv file that we have split into multiple smaller ones. We can utilize any of the tuning things like cache and batch size to control the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern = \"polar_out/train/*.csv\",\n",
    "            batch_size=64, \n",
    "            num_epochs=BASE_EPOCHS,\n",
    "            num_parallel_reads=20,\n",
    "            shuffle_buffer_size=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mar_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
