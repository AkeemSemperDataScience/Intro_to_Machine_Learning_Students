{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_hub in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: keras>=3.5 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (3.9.0)\n",
      "Requirement already satisfied: absl-py in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (24.2)\n",
      "Requirement already satisfied: regex in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2024.11.6)\n",
      "Requirement already satisfied: rich in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (0.3.11)\n",
      "Requirement already satisfied: tensorflow-text in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras_hub) (2.19.0)\n",
      "Requirement already satisfied: namex in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.0.7)\n",
      "Requirement already satisfied: h5py in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (3.12.1)\n",
      "Requirement already satisfied: optree in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from keras>=3.5->keras_hub) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from kagglehub->keras_hub) (4.67.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from rich->keras_hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from rich->keras_hub) (2.15.1)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow-text->keras_hub) (2.19.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras_hub) (0.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (1.62.2)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (2.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from requests->kagglehub->keras_hub) (2025.1.31)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.45.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/akeem/anaconda3/envs/tf_mar_2025/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras_hub) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens\n",
    "MAX_SEQ = 3000\n",
    "MIN_LEN = 35\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 3000  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 15\n",
    "USE_DATASETS = True\n",
    "LEARN_RATE = 0.01\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 20\n",
    "\n",
    "SAMP = 7\n",
    "\n",
    "LARGER_TRAIN = False\n",
    "if LARGER_TRAIN:\n",
    "  BATCH_SIZE = 512\n",
    "  EPOCHS = 50\n",
    "\n",
    "import keras \n",
    "!pip install keras_hub\n",
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "# Check saved weights\n",
    "weight_url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/simple_generative_weights_1.keras\"\n",
    "use_old_weights = True\n",
    "have_old_weights = False\n",
    "try:\n",
    "  old_weights = keras.utils.get_file('simple_generative_weights_1.keras', weight_url)\n",
    "  have_old_weights = True\n",
    "except:\n",
    "  print(\"No old weights found.\")\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM\n",
    "\n",
    "As we have seen, LSTM models are excellent at dealing with sequential data. As luck would have it, text is also sequental data! We can train a model to predict the next word in a sentence, then use that smarts to generate new text. Basically ChatGPT, but far better. \n",
    "\n",
    "### Text for Training\n",
    "\n",
    "We need some text from which to train our model to speak, I captured a small extract of text from Reddit posts, which vaguely resembles actual language, if that language is filtered through a bucket of social isolation, Adderall, racism, and memes. We'll first need to clean up our data a bit before we can assemble it for modelling. The initial cleaning bits are just like what we used in NLP, we just need to get rid of all the junk. \n",
    "\n",
    "We can use pretty much anything that you can imagine as source, and assuming we can gather enough data and train our model, the generated speech will be styled after the source. I liken it to going on vacation in Indonesia and talking to Indonesians who spoke English like Australian surfer bros - their training data was a little weird, so the output was a little weird too. If you're looking to build your best ChatGPT competitor you will want a lot of data, specifically a lot of data that is representative of the full gamut of how you want your model to write. If you want slang in the new text, you can't really train on Shakespeare and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/p1/m8wtcgx57417hx9d_r110ctw0000gn/T/ipykernel_80047/1458716329.py:11: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  train_text[USE_COL] = train_text[USE_COL].str.replace(\"[^\\w\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43732027/43732027\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2us/step\n",
      "53187.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Ladies and gentlemen, we got 'em.</td>\n",
       "      <td>14</td>\n",
       "      <td>l6wxb8</td>\n",
       "      <td>https://i.redd.it/d3ivm0vpu2e61.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611870e+09</td>\n",
       "      <td>Ladies and gentlemen, we got 'em. nan</td>\n",
       "      <td>2021-01-28 23:44:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24119</th>\n",
       "      <td>I still canâ€™t stop buyingğŸ’ğŸ™ŒğŸ¾ğŸš€ğŸš€ğŸš€</td>\n",
       "      <td>165</td>\n",
       "      <td>lbrk69</td>\n",
       "      <td>https://i.redd.it/q2h4iqfmiaf61.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>1.612399e+09</td>\n",
       "      <td>I still canâ€™t stop buyingğŸ’ğŸ™ŒğŸ¾ğŸš€ğŸš€ğŸš€ nan</td>\n",
       "      <td>2021-02-04 02:35:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26176</th>\n",
       "      <td>Got 2 whole GME, a bunch of NOK, Socks for Lis...</td>\n",
       "      <td>6</td>\n",
       "      <td>lcmghj</td>\n",
       "      <td>https://i.redd.it/y3qfs3vr8if61.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1.612492e+09</td>\n",
       "      <td>Got 2 whole GME, a bunch of NOK, Socks for Lis...</td>\n",
       "      <td>2021-02-05 04:34:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45677</th>\n",
       "      <td>CRSR earnings next week... pls fly - got $70k ...</td>\n",
       "      <td>46</td>\n",
       "      <td>mzs4iy</td>\n",
       "      <td>https://i.redd.it/59sok8parqv61.jpg</td>\n",
       "      <td>30</td>\n",
       "      <td>1.619569e+09</td>\n",
       "      <td>CRSR earnings next week... pls fly - got $70k ...</td>\n",
       "      <td>2021-04-28 03:18:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45659</th>\n",
       "      <td>MindMed on CNN. People are slowly realizing th...</td>\n",
       "      <td>10853</td>\n",
       "      <td>mzubm7</td>\n",
       "      <td>https://v.redd.it/w0g60exj7rv61</td>\n",
       "      <td>1193</td>\n",
       "      <td>1.619575e+09</td>\n",
       "      <td>MindMed on CNN. People are slowly realizing th...</td>\n",
       "      <td>2021-04-28 04:52:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51895</th>\n",
       "      <td>Ride the Magic ShortBus TA DD - SPY, AMC, SPCE...</td>\n",
       "      <td>47</td>\n",
       "      <td>okcjtr</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>35</td>\n",
       "      <td>1.626322e+09</td>\n",
       "      <td>Ride the Magic ShortBus TA DD - SPY, AMC, SPCE...</td>\n",
       "      <td>2021-07-15 07:11:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44675</th>\n",
       "      <td>More PLTR, canâ€™t wait for next Wednesday</td>\n",
       "      <td>141</td>\n",
       "      <td>mnhm8i</td>\n",
       "      <td>https://i.redd.it/rhar63aek5s61.jpg</td>\n",
       "      <td>50</td>\n",
       "      <td>1.618005e+09</td>\n",
       "      <td>More PLTR, canâ€™t wait for next Wednesday nan</td>\n",
       "      <td>2021-04-10 00:49:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36601</th>\n",
       "      <td>ğŸ¸ğŸ¸ JFrog $FROG DD - A great software company w...</td>\n",
       "      <td>5</td>\n",
       "      <td>lu46g7</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>40</td>\n",
       "      <td>1.614511e+09</td>\n",
       "      <td>ğŸ¸ğŸ¸ JFrog $FROG DD - A great software company w...</td>\n",
       "      <td>2021-02-28 13:20:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>Robin Hood canâ€™t stop us!</td>\n",
       "      <td>3</td>\n",
       "      <td>l6ywmv</td>\n",
       "      <td>https://i.redd.it/5os6myxi83e61.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611875e+09</td>\n",
       "      <td>Robin Hood canâ€™t stop us! nan</td>\n",
       "      <td>2021-01-29 01:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>Squeezed out</td>\n",
       "      <td>1</td>\n",
       "      <td>l70kcs</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611878e+09</td>\n",
       "      <td>Squeezed out Couldn't get the shares yesterday...</td>\n",
       "      <td>2021-01-29 01:59:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "823                    Ladies and gentlemen, we got 'em.     14  l6wxb8   \n",
       "24119                    I still canâ€™t stop buyingğŸ’ğŸ™ŒğŸ¾ğŸš€ğŸš€ğŸš€    165  lbrk69   \n",
       "26176  Got 2 whole GME, a bunch of NOK, Socks for Lis...      6  lcmghj   \n",
       "45677  CRSR earnings next week... pls fly - got $70k ...     46  mzs4iy   \n",
       "45659  MindMed on CNN. People are slowly realizing th...  10853  mzubm7   \n",
       "51895  Ride the Magic ShortBus TA DD - SPY, AMC, SPCE...     47  okcjtr   \n",
       "44675           More PLTR, canâ€™t wait for next Wednesday    141  mnhm8i   \n",
       "36601  ğŸ¸ğŸ¸ JFrog $FROG DD - A great software company w...      5  lu46g7   \n",
       "1378                           Robin Hood canâ€™t stop us!      3  l6ywmv   \n",
       "8541                                        Squeezed out      1  l70kcs   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "823                  https://i.redd.it/d3ivm0vpu2e61.jpg          0   \n",
       "24119                https://i.redd.it/q2h4iqfmiaf61.jpg          5   \n",
       "26176                https://i.redd.it/y3qfs3vr8if61.jpg          1   \n",
       "45677                https://i.redd.it/59sok8parqv61.jpg         30   \n",
       "45659                    https://v.redd.it/w0g60exj7rv61       1193   \n",
       "51895  https://www.reddit.com/r/wallstreetbets/commen...         35   \n",
       "44675                https://i.redd.it/rhar63aek5s61.jpg         50   \n",
       "36601  https://www.reddit.com/r/wallstreetbets/commen...         40   \n",
       "1378                 https://i.redd.it/5os6myxi83e61.jpg          0   \n",
       "8541   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "\n",
       "            created                                               body  \\\n",
       "823    1.611870e+09              Ladies and gentlemen, we got 'em. nan   \n",
       "24119  1.612399e+09                I still canâ€™t stop buyingğŸ’ğŸ™ŒğŸ¾ğŸš€ğŸš€ğŸš€ nan   \n",
       "26176  1.612492e+09  Got 2 whole GME, a bunch of NOK, Socks for Lis...   \n",
       "45677  1.619569e+09  CRSR earnings next week... pls fly - got $70k ...   \n",
       "45659  1.619575e+09  MindMed on CNN. People are slowly realizing th...   \n",
       "51895  1.626322e+09  Ride the Magic ShortBus TA DD - SPY, AMC, SPCE...   \n",
       "44675  1.618005e+09       More PLTR, canâ€™t wait for next Wednesday nan   \n",
       "36601  1.614511e+09  ğŸ¸ğŸ¸ JFrog $FROG DD - A great software company w...   \n",
       "1378   1.611875e+09                      Robin Hood canâ€™t stop us! nan   \n",
       "8541   1.611878e+09  Squeezed out Couldn't get the shares yesterday...   \n",
       "\n",
       "                 timestamp  \n",
       "823    2021-01-28 23:44:29  \n",
       "24119  2021-02-04 02:35:05  \n",
       "26176  2021-02-05 04:34:08  \n",
       "45677  2021-04-28 03:18:21  \n",
       "45659  2021-04-28 04:52:43  \n",
       "51895  2021-07-15 07:11:07  \n",
       "44675  2021-04-10 00:49:54  \n",
       "36601  2021-02-28 13:20:57  \n",
       "1378   2021-01-29 01:01:53  \n",
       "8541   2021-01-29 01:59:56  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complaint data\n",
    "USE_REDDIT = True\n",
    "if USE_REDDIT:\n",
    "    # Reddit WSB data\n",
    "    train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/reddit_wsb.csv')\n",
    "    USE_COL = \"body\"\n",
    "    train_text = pd.read_csv(train_text_file)\n",
    "    train_text[USE_COL] = train_text['title'].astype(\"str\") + \" \" + train_text[USE_COL].astype(\"str\")\n",
    "    train_text[USE_COL] = train_text[USE_COL].str.replace(\"\\n\", \" \")\n",
    "    #clean punctuation\n",
    "    train_text[USE_COL] = train_text[USE_COL].str.replace(\"[^\\w\\s]\", \"\")\n",
    "    train_text[USE_COL].dropna(inplace=True)\n",
    "else:\n",
    "    # Complaint data\n",
    "    train_text_file = keras.utils.get_file('train_text.txt', 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/complaints_processed_clean.csv')\n",
    "    USE_COL = \"narrative\"\n",
    "    train_text = pd.read_csv(train_text_file)\n",
    "    train_text[USE_COL] = train_text[USE_COL].astype(\"str\")\n",
    "\n",
    "\n",
    "print(np.mean(len(train_text[USE_COL])))\n",
    "train_text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> the install of the NLTK stuff can be weird on Colab. Sometimes I need to run it, let it install, and run again. On my laptop, it works normally, I beleive it is just due to the temporary virtual environment on Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "\tDimensions: 428\n",
      "\tSample: ['this', 'is', 'the', 'moment', 'life', 'is', \"n't\", 'fair', '.', 'my', 'mother', 'always', 'told', 'me', 'that', 'when', 'i', 'would', 'complain', 'about', 'arbitrary', 'treatment', '.', 'i', 'would', 'play', 'by', 'the', 'rules', 'and', 'someone', 'else', 'would', 'ignore', 'them', '.', 'when', 'they', 'would', 'win', 'i', 'would', 'appeal', 'to', 'the', 'first', 'authority', 'for', 'an', 'explanation', '.', '``', 'are', 'you', 'going', 'to', 'let', 'them', 'get', 'away', 'with', 'this', \"''\", '?', '``', 'life', 'is', \"n't\", 'fair', \"''\", '.', 'no', ',', 'it', 'is', 'not', '.', 'the', 'game', 'is', 'the', 'game', '.', 'always', '.', 'in', 'this', 'moment', ',', 'the', 'fascade', 'cracks', 'further', '.', 'when', 'the', 'first', 'breach', 'was', 'made', 'i', 'do', 'not', 'know', ',', 'perhaps', 'it', 'was', 'socrates', ',', 'but', 'today', 'i', 'see', 'thousands', '.', 'millions', '.', 'once', 'they', 'were', 'laughing', ',', 'luxuries', 'falling', 'out', 'of', 'their', 'disgusting', 'diseased', 'mouths', 'as', 'they', 'cackled', '.', 'the', 'unmistakable', 'stench', 'of', 'derision', 'carried', 'on', 'their', 'breath', '.', 'they', 'told', 'anyone', 'outside', 'of', 'their', 'elite', 'class', 'that', 'we', 'were', 'fools', 'for', 'even', 'trying', '.', 'they', 'told', 'us', 'that', 'we', 'were', 'naive', '.', 'we', 'needed', 'networks', 'to', 'be', 'successful', '.', 'we', 'needed', 'polish', '.', 'we', 'needed', 'expertise', '.', 'we', 'needed', 'them', '.', 'the', 'game', 'is', 'the', 'game', '.', 'always', '.', 'they', 'are', 'no', 'longer', 'laughing', '.', 'their', 'odious', 'oeuvre', 'still', 'wafts', 'through', 'the', 'air', '.', 'while', 'the', 'rot', ',', 'and', 'hate', ',', 'and', 'condescention', ',', 'remains', ',', 'the', 'noxious', 'air', 'betrays', 'a', 'new', 'addition', '.', 'something', 'all', 'together', 'disconcerting', '.', 'what', 'it', 'betrays', ',', 'is', 'fear', '.', 'they', 'are', 'afraid', '.', 'and', 'they', 'should', 'be', '.', 'we', 'do', 'not', 'need', 'their', 'inherited', 'resources', 'masked', 'as', 'acumen', '.', 'a', 'new', 'day', 'dawns', '.', 'the', 'day', 'where', 'we', 'make', 'an', 'ever', 'so', 'slight', 'step', 'towards', 'what', 'they', 'fear', 'the', 'most', '.', 'an', 'even', 'field', '.', 'life', 'becoming', 'ever', 'so', 'slighty', 'more', 'fair', '.', 'and', '.', 'they', '.', 'are', '.', 'scared', '.', 'they', 'look', 'at', 'us', 'and', 'see', 'roughness', '.', 'we', 'look', 'at', 'them', 'and', 'see', 'softness', '.', 'we', 'are', 'both', 'correct', 'in', 'our', 'estimation', '.', 'the', 'game', 'is', 'the', 'game', '.', 'always', '.', 'fuck', 'them', 'in', 'the', 'street', '.', 'fuck', 'them', 'all', 'in', 'the', 'street', '.', 'we', 'are', 'the', 'righteous', '.', 'we', 'are', 'blessed', 'by', 'phoebe', '.', 'what', 'started', 'here', 'will', 'echo', 'through', 'time', 'in', 'the', 'eons', 'to', 'come', '.', 'mount', 'up', 'and', 'ride', 'with', 'the', 'fury', 'of', 'a', 'thousand', 'rockets', 'into', 'the', 'universal', 'filament', '.', 'may', 'the', 'wind', 'always', 'be', 'at', 'your', 'back', 'and', 'the', 'sun', 'upon', 'your', 'face', '.', 'and', 'may', 'the', 'wings', 'of', 'destiny', 'carry', 'you', 'aloft', ',', 'to', 'dance', 'with', 'the', 'stars', '.', 'gme', '@', 'everything', 'bb', '@', 'everything', 'ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "tokenizer_simple = nltk.tokenize.word_tokenize\n",
    "try:\n",
    "  tokenized_text = train_text[USE_COL].apply(tokenizer_simple)\n",
    "except:\n",
    "  nltk.download('punkt_tab')\n",
    "  tokenized_text = train_text[USE_COL].apply(tokenizer_simple)\n",
    "\n",
    "tokenized_text = tokenized_text.apply(lambda x: [word.lower() for word in x])\n",
    "sample_tok = tokenized_text[SAMP]\n",
    "print(\"Sample:\")\n",
    "print(\"\\tDimensions:\", len(sample_tok))\n",
    "print(\"\\tSample:\", sample_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions\n",
    "\n",
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorizer():\n",
    "    def __init__(self, max_tokens):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "    def fit_on_texts(self, texts):\n",
    "        self.vocab = {word: i+1 for i, word in enumerate(set([word for text in texts for word in text]))}\n",
    "        self.reverse_vocab = {i: word for word, i in self.vocab.items()}\n",
    "        #limit to max tokens\n",
    "        tokens_to_keep = list(self.vocab.keys())[:self.max_tokens]\n",
    "        self.vocab = {word: i+1 for i, word in enumerate(tokens_to_keep)}\n",
    "        self.reverse_vocab = {i: word for word, i in self.vocab.items()}\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.vocab[word] for word in text if word in self.vocab] for text in texts]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Training Sequences\n",
    "\n",
    "We are going to be a little slack in the construction of the datasets for training because we are limited in the amount of resources we can handle. All of the datasets that are fed to the model need to be the same length, so we'll set a cap and trucate it here for resource concerns. Our dataset will be constructed as:\n",
    "<ul>\n",
    "<li> A sequence of (up to) 24 words as the X data. \n",
    "<li> The next word as the Y data.\n",
    "</ul>\n",
    "\n",
    "So each sequence is effectively one set of features, and its target is the next word. If we were doing this in reality, we'd want to prep more records from our sample:\n",
    "<ul>\n",
    "<li> Suppose a sample sentence is \"The quick brown fox jumps over the lazy dog\". Our ideal data would have something like:\n",
    "    <ul> \n",
    "    <li>X = \"the quick brown\", Y = \"fox\"\n",
    "    <li> X = \"quick brown fox\", Y = \"jumps\"\n",
    "    <li> X = \"brown fox jumps\", Y = \"over\"\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This is superior both because we are generating much more data to train the model and because we are training the model to predict words in all different positions in the sentence. We'd be predicting almost every word in the training dataset. The words that frequently end a sentence are not necessarily the same as the words that start a sentence or sit in the middle, so making predictions up and down the text will likely lead to a more useful model.\n",
    "\n",
    "<b>We'd end up with a better model if we generated more sequences from our data and/or added more data. The resource demands make that tough, so we have cut some corners that are easy to remedy in a real-world scenario.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', \"n't\", 'have', 'an', 'expiration', 'date', 'hedgefund', 'whales', 'are', 'spreading', 'disinfo', 'saying', 'friday', 'is', 'make-or-break', 'for', '$', 'gme', '.', 'call', 'options', 'expiring', 'itm', 'on', 'friday', 'will', 'drive', 'the', 'price', 'up', 'if', 'levels', 'are', 'maintained', ',', 'but', 'may', 'not', 'trigger', 'the', 'short', 'squeeze', '.', 'it', 'may', 'be', 'friday', ',', 'but', 'it', 'could', 'be', 'next', 'week', 'the', 'we', 'see', 'the', 'real', 'squeeze', '.', 'do', \"n't\", 'panic', 'if', 'the', 'squeeze', 'does', \"n't\", 'happen', 'friday', '.', 'it', \"'s\", 'not', 'guaranteed', 'to', '.', 'the', 'only', 'thing', 'that', 'is', 'guaranteed', 'mathematically', 'is', 'that', 'the', 'shorts', 'will', 'have', 'to', 'cover', 'at', 'some', 'point', 'in', 'the', 'future', '.', 'they', 'are', 'trying', 'to', 'get', 'enough', 'people', 'hooked', 'on', 'the', 'false', 'expectation', 'of', 'friday', 'so', 'that', 'if/when', 'it', 'does', \"n't\", 'happen', ',', 'enough', 'will', 'sell', 'out', 'of', 'panic/despair']\n"
     ]
    }
   ],
   "source": [
    "def construct_input_sequences(data, seq_length, max_sequences, max_per_doc=5):\n",
    "    input_sequences = []\n",
    "    while len(input_sequences) < max_sequences:\n",
    "        for doc in data:\n",
    "            sequences_for_doc = []\n",
    "            num_sequence_for_doc = 0\n",
    "            for i in range(len(doc) - seq_length):\n",
    "                if num_sequence_for_doc < max_per_doc:\n",
    "                    sequences_for_doc.append(doc[i:i + seq_length])\n",
    "                    num_sequence_for_doc += 1\n",
    "                else:\n",
    "                    break\n",
    "            input_sequences.extend(sequences_for_doc)\n",
    "    \n",
    "    return input_sequences\n",
    "input_sequences = construct_input_sequences(tokenized_text, SEQ_LEN, MAX_SEQ)\n",
    "print(input_sequences[SAMP])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize\n",
    "\n",
    "We can take our tokenized data and convert it to a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "\tDimensions: 5\n",
      "\tSample: [1461, 265, 65, 65, 2391]\n",
      "\tSample text: was\n"
     ]
    }
   ],
   "source": [
    "vect_sequences = SimpleVectorizer(max_tokens=VOCAB_SIZE)\n",
    "vect_sequences.fit_on_texts(input_sequences)\n",
    "tokenized_sequences = vect_sequences.texts_to_sequences(input_sequences)\n",
    "SAMP = np.random.randint(0, len(tokenized_sequences))\n",
    "sample_tok = tokenized_sequences[SAMP]\n",
    "print(\"Sample:\")\n",
    "print(\"\\tDimensions:\", len(sample_tok))\n",
    "print(\"\\tSample:\", sample_tok)\n",
    "print(\"\\tSample text:\", vect_sequences.reverse_vocab[sample_tok[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences\n",
    "\n",
    "We need to pad our sequences so that they are all the same length. This is a requirement of the LSTM model. Note that we want the padding at the start of the sequence, not the end. This is because we are trying to predict the next word in the sequence, so we want the model to see the words that come before it. If we pad at the end, the model will not be able to see the words that come before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (59017, 128)\n",
      "type: <class 'numpy.ndarray'>\n",
      "X_t shape: (59017, 127)\n",
      "Y_t shape: (59017,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "padded_sequences = pad_sequences(tokenized_sequences, maxlen=SEQ_LEN, padding='pre', truncating='post')\n",
    "print(\"Dimensions:\", padded_sequences.shape)\n",
    "print(\"type:\", type(padded_sequences))\n",
    "padded_input_sequences = padded_sequences\n",
    "\n",
    "X_t = padded_input_sequences[:, :-1]\n",
    "Y_t = padded_input_sequences[:, -1].ravel()\n",
    "print(\"X_t shape:\", X_t.shape)\n",
    "print(\"Y_t shape:\", Y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 1461  265   65\n",
      "   65]\n",
      "Y: 2391\n",
      "X shape: (127,)\n",
      "Y shape: ()\n"
     ]
    }
   ],
   "source": [
    "sample_x_record = X_t[SAMP]\n",
    "sample_y_record = Y_t[SAMP]\n",
    "print(\"X:\", sample_x_record)\n",
    "print(\"Y:\", sample_y_record)\n",
    "print(\"X shape:\", sample_x_record.shape)\n",
    "print(\"Y shape:\", sample_y_record.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_save_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='simple_generative_weights_1.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 16:20:19.702922: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-15 16:20:19.703063: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-04-15 16:20:19.703068: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744755619.703642  908801 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744755619.704012  908801 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-04-15 16:20:20.506501: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.0715 - loss: nan\n",
      "Epoch 1: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 142ms/step - accuracy: 0.0714 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 2/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 2: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 120ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 3/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 3: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 119ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 4/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 4: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 134ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 5: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 150ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 6/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 6: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 151ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 7/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 7: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 149ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 8/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 8: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 153ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 9/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 9: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 157ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 10/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 10: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 157ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 11/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 11: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 163ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 12/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 12: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 165ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 13/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 13: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 168ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 14/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 14: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 163ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 15/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Epoch 15: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 163ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "  del tokenized_sequences, tokenized_text, input_sequences\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM))\n",
    "model.add(layers.LSTM(EMBED_DIM, return_sequences=True))\n",
    "model.add(layers.LSTM(EMBED_DIM))\n",
    "model.add(layers.Dense(EMBED_DIM, activation='relu'))\n",
    "model.add(layers.Dense(VOCAB_SIZE, activation='softmax'))\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARN_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    #train_ds_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[weight_save_callback],\n",
    ")\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Some Text\n",
    "\n",
    "We can generate text once the model is trained. We'll start with a seed sentence, and then we'll use the model to predict the next word. We'll then append that word to the sentence and use the model to predict the next word, and so on. There is a little helper function to do this for us with limited repetition.\n",
    "\n",
    "Another new thing is that we create an inverse dictionary to map the encoded words back to the original words.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "One weirdly named factor that is important in text generation is the temperature. The temperature is a factor that we can use to control the randomness of the output. To generate text, we are essentially using a probability distribution to determine what the next word should be - the softmax output of the model will tell us the most likely next word. The issue is that certain words are way more likely than others - \"the\", \"it\", \"a\", \"and\", etc. are all very common words so we can expect the model to predict them as \"most likely\" a lot, probably too often. \n",
    "\n",
    "![Temperature](images/temperature.gif \"Temperature\")\n",
    "\n",
    "The most direct way to combat this is to add some degree of randomness to which word we select - we'll still pick the most likely word more often than any other, but we'll also pick other words that have some degree of likelihood at random. The higher the temperature the more randomness is introduced. A correct value requires tuning with human feedback, and it'll vary depending on the base quality of the model - large models that are trained on huge volumes of text and are deep enough to pick up on the \"what type of word should be here\" patterns will be able to generate better text with lower temperatures. Our model here is small and kind of sucks, so the temperature needs to be higher to get anything remotely usable. The implementation here is stolen shamelessly from the internet, the details don't really matter all that much, we just need to vary our predictions away from always simply picking the most likely word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import string\n",
    "import re\n",
    "def text_to_word_sequence(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    seq = text.split()\n",
    "    return seq\n",
    "def sampleWord(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds.flatten(), 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def gen_text(seed_text, next_words, model, length_max, tok, inverse, temperature=1.0, check_in_vocab=True):\n",
    "    out = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tok.texts_to_sequences([out])[0]\n",
    "        if check_in_vocab:\n",
    "            for tok_n in token_list:\n",
    "                if tok_n not in tok.vocab.values():\n",
    "                    token_list.remove(tok_n)\n",
    "        token_list = pad_sequences([token_list], maxlen=length_max-1, padding='pre')\n",
    "        #print(token_list)\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        #print(tok.sequences_to_texts(predicted))\n",
    "        word=\"\"\n",
    "        try:\n",
    "            word = inverse[sampleWord(predicted, temperature=temperature)]\n",
    "        except:\n",
    "            pass\n",
    "        out += \" \"+word\n",
    "        #print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake Text Time!\n",
    "\n",
    "<b>Note: we haven't built in any error handling, so it is possible to get random errors with unknown seeds. </b> In the real world, parsing the inputs from the user would be a more serious task, but for this example, we can just ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.\n",
      "Generated text: The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.                    \n",
      "Seed text: I would like to complain about\n",
      "Generated text: I would like to complain about                    \n",
      "Seed text: Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight\n",
      "Generated text: Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight                    \n"
     ]
    }
   ],
   "source": [
    "seed_text_1 = \"The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.\"\n",
    "seed_text_2 = \"I would like to complain about\"\n",
    "seed_text_3 = \"Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight\"\n",
    "\n",
    "output_1 = gen_text(seed_text_1, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_2 = gen_text(seed_text_2, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_3 = gen_text(seed_text_3, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "print(\"Seed text:\", seed_text_1)\n",
    "print(\"Generated text:\", output_1)\n",
    "print(\"Seed text:\", seed_text_2)\n",
    "print(\"Generated text:\", output_2)\n",
    "print(\"Seed text:\", seed_text_3)\n",
    "print(\"Generated text:\", output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Old Weights\n",
    "\n",
    "I've run several rounds of training this model, and I've posted the weights online. The code here will load it, so we can expect our results to be a bit more logical here. \n",
    "\n",
    "<b>Note:</b> On the whole, our training process - in terms of data, time, and model size, is very tiny. Real text generation models are orders of magnitude larger, so don't expect much. \n",
    "\n",
    "##### Callback Example\n",
    "\n",
    "We can also generate text at the end of each epoch using a callback. Here we will print it, but it could also be logged to any location, such as the tensorboard logs. This is a good example of a simple, custom callback. We could bulk this out a little by creating a full class if we needed, and allow for things like multiple seeds, multiple generations, logging only every N epochs, etc... that's a good exercsie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading old weights.\n",
      "Old weights loaded.\n",
      "Epoch 1/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 0: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 1: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 177ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 2/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 1: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 2: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 170ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 3/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 2: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 3: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 161ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 4/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 3: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 4: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 155ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 4: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 5: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 144ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 6/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 5: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 6: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 148ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 7/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 6: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 7: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 143ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 8/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 7: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 8: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 143ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 9/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 8: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 9: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 144ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 10/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 9: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 10: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 138ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 11/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 10: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 11: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 140ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 12/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 11: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 12: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 139ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 13/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 12: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 13: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 138ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 14/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 13: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 14: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 144ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 15/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 14: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\n",
      "Epoch 15: loss did not improve from inf\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 146ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e0ba6cf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_seed_text = \"We're off to see the wizard, the wonderful wizard of\"\n",
    "gen_text_callback = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(\"\\nGenerated text after epoch {}: {}\\n\".format(epoch, gen_text(callback_seed_text, NUM_TOKENS_TO_GENERATE, model, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)))\n",
    ")\n",
    "\n",
    "if have_old_weights and use_old_weights:\n",
    "    print(\"Loading old weights.\")\n",
    "    try:\n",
    "      model.load_weights(old_weights)\n",
    "      print(\"Old weights loaded.\")\n",
    "    except:\n",
    "      print(\"Error loading old weights.\")\n",
    "      pass\n",
    "else:\n",
    "    print(\"No old weights found, starting from scratch.\")\n",
    "\n",
    "model.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[gen_text_callback, weight_save_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Model with Bi-Directional LSTM\n",
    "\n",
    "We can also build a larger model with a bi-directional LSTM. This is a more complex model that will take longer to train, but it should be able to generate better text. The bi-directional LSTM will look at the text in both directions, which should help it understand the context of the words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.1506 - loss: 6.6340\n",
      "Generated text after epoch 0: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 589ms/step - accuracy: 0.1507 - loss: 6.6333 - val_accuracy: 0.1645 - val_loss: 11.3954\n",
      "Epoch 2/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.1630 - loss: 7.2029\n",
      "Generated text after epoch 1: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 588ms/step - accuracy: 0.1630 - loss: 7.2022 - val_accuracy: 0.1734 - val_loss: 9.9045\n",
      "Epoch 3/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624ms/step - accuracy: 0.1827 - loss: 6.3051\n",
      "Generated text after epoch 2: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 746ms/step - accuracy: 0.1827 - loss: 6.3062 - val_accuracy: 0.1795 - val_loss: 10.9558\n",
      "Epoch 4/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733ms/step - accuracy: 0.0189 - loss: nan\n",
      "Generated text after epoch 3: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 863ms/step - accuracy: 0.0189 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 5/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 4: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 866ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 6/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 5: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 879ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 7/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 6: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 915ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 8/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 7: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 901ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 9/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 8: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 839ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 10/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 9: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 878ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 11/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 10: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 861ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 12/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 11: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 802ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 13/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 12: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 769ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 14/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 13: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 766ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n",
      "Epoch 15/15\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675ms/step - accuracy: 0.0000e+00 - loss: nan\n",
      "Generated text after epoch 14: We're off to see the wizard, the wonderful wizard of                    \n",
      "\n",
      "\u001b[1m323/323\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 791ms/step - accuracy: 0.0000e+00 - loss: nan - val_accuracy: 0.0000e+00 - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model_bi = keras.Sequential()\n",
    "model_bi.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM))\n",
    "\n",
    "model_bi.add(layers.Bidirectional(layers.LSTM(EMBED_DIM, return_sequences=True)))\n",
    "model_bi.add(layers.Bidirectional(layers.LSTM(EMBED_DIM)))\n",
    "model_bi.add(layers.Dense(EMBED_DIM, activation='relu'))\n",
    "model_bi.add(layers.Dense(VOCAB_SIZE, activation='softmax'))\n",
    "model_bi.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARN_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "history_bi = model_bi.fit(\n",
    "    X_t, \n",
    "    Y_t,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[gen_text_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.\n",
      "Generated text: The customer service was the best thing I've ever seen, now I love this company and want to buy all their products. No other store will ever do it for me after this experience.                    \n",
      "Seed text: I would like to complain about\n",
      "Generated text: I would like to complain about                    \n",
      "Seed text: Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight\n",
      "Generated text: Based crypto lit awesome subreddit bruh. I love this place, it's so sick and tight                    \n"
     ]
    }
   ],
   "source": [
    "output_1 = gen_text(seed_text_1, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_2 = gen_text(seed_text_2, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "output_3 = gen_text(seed_text_3, NUM_TOKENS_TO_GENERATE, model_bi, SEQ_LEN, vect_sequences, vect_sequences.reverse_vocab)\n",
    "print(\"Seed text:\", seed_text_1)\n",
    "print(\"Generated text:\", output_1)\n",
    "print(\"Seed text:\", seed_text_2)\n",
    "print(\"Generated text:\", output_2)\n",
    "print(\"Seed text:\", seed_text_3)\n",
    "print(\"Generated text:\", output_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Create Generative Model\n",
    "\n",
    "Grab some text, try it. There are a bunch in the keras datasets, or on Kaggle. \n",
    "\n",
    "The core approach can be very similar for any column of data. You can also try other details, notably some parts that we did manually can be done more directly with some keras functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample example text:\n",
      "\tTokens:\n",
      " [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "\tText:\n",
      " ['the', 'wattie', 'nondiscriminatory', 'mln', 'loss', 'for', 'plc', 'said', 'at', 'only', 'ended', 'said', 'commonwealth', 'could', '1', 'traders', 'now', 'april', '0', 'a', 'after', 'said', 'from', '1985', 'and', 'from', 'foreign', '000', 'april', '0', 'prices', 'its', 'account', 'year', 'a', 'but', 'in', 'this', 'mln', 'home', 'an', 'states', 'earlier', 'and', 'rise', 'and', 'revs', 'vs', '000', 'its', '16', 'vs', '000', 'a', 'but', '3', 'psbr', 'oils', 'several', 'and', 'shareholders', 'and', 'dividend', 'vs', '000', 'its', 'all', '4', 'vs', '000', '1', 'mln', 'agreed', 'largely', 'april', '0', 'are', '2', 'states', 'will', 'billion', 'total', 'and', 'against', '000', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n",
      "\tText:\n",
      " ['the', 'termination', 'payment', 'airport', 'takes', '6', 'visibility', 'geological', '3', '6', '602', 'begin', 'up', 'said', 'fully', 'bank', 'expects', 'commodity', 'total', 'is', 'giant', 'a', 'recreation', 'this', 'takes', 'leroy', 'series', 'termination', 'payment', 'airport', 'mln', 'a', 'for', 'capital', '1', 'pre', '50', 'american', 'east', 'said', 'in', 'council', 'takes', 'leroy', \"recommend's\", 'france', 'a', 'but', 'u', 'any', '4', 's', '1st', 'losses', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12]\n",
      "\tText:\n",
      " ['the', 'lt', 'dlrs', 'demand', '000', 'reuter', 'dividend', 'year', 'lt', 'plus', 'billion', '04', '000', 'reuter', 'dividend', 'year', 'an', 'worth', 'new', 'vs', 'reuter', 'dlrs', 'warburg', 'on', 'shrs', 'earnings', 'countries', 'new', 'vs', 'reuter', '1985', 'billion', 'vs', '2', 'lt', '4', 'division', '000', 'reuter', 'from', 'go', '000', 'lt', 'plus', 'which', 'mid', '000', 'reuter', 'from', 'total', '000', 'an', '71', 'billion', 'vs', 'reuter', 'dlr', 'also', 'vs', 'shrs', 'earnings', 'countries', '4', 'vs', 'reuter', '1985', 'from', 'vs', 'some', 'now', 'april', '0', 'related', 'in', 'corp', 'it', 'inc', 'strong', 'cents', 'dollar', 'were', 'after', 'april', '0', 'crisis', 'or', 'ontario', 'more', 'index', '10', 'electric', 'company', 'taking', 'report', 'it', 'in', 'estimated', 'but', 'trading', 'texas', 'said', 'united', 'said', 'came', 'a', 'advising', 'up', 'said', 'countries', 'vs', '000', '3', 'delayed', 'central', 'said', 'which', 'objections', 'on', 'future', '617', 'said', 'came', 'a', 'includes', 'refile', 'profit', 'said', 'meeting', 'trade', 'vs', '3', 'supplie', 'up', 'said', '1985', 'were', 'vs', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12]\n",
      "\tText:\n",
      " ['the', 'in', 'currencies', 'hit', 'firms', 'in', 'has', 'would', 'seven', 'jointly', 'those', 'taiwan', '226', 'over', 'nigel', '9', '500', 's', '000', 'a', 'income', 'csr', 'that', 'at', '234', 'thyh', 'yielding', '7', '9', 'inc', 'british', 'said', 'at', 'those', 'our', 'justice', 'in', '24', 'accepted', 'financing', 'conrac', 'mln', 'a', 'know', 'primary', 'it', 'believe', 'and', 'in', 'case', 'seven', 'york', '686', 'assumes', '49', 'leaves', 'england', 'said', 'conrac', 'two', 'mln', 'by', 'for', 'meetings', 'travel', 'said', 'value', 'recently', 'a', 'linares', 'american', 'margarine', 'proved', 'planning', 'loss', '90', 'loss', 'inc', 'can', 'said', 'mcandrews', 'yielding', 'plan', 'holding', 'market', 'decline', 'its', 'in', 'way', 'a', 'but', '5', 'will', 'surpised', 'month', 'in', 'value', 'ago', 'in', 'way', 'as', 'value', 'ucpb', 'unsubsidised', 'european', 'release', 'for', 'comprised', 'said', 'american', 'cpb', 'preliminary', 'said', '018', 'dw', 'that', 'horn', 'global', 'case', 'seven', 'york', 'i', '652', 'extraction', 'process', 'groundwork', 'optioon', 'in', 'value', 'margarine', 'recently', '481', 'mcandrews', '564', 'nonbelligerent', 'amid', 'with', 'for', 'indian', 'by', 'stock', '5', 'name', 'for', 'bangemann', 'planting', 'a', 'in', 'wake', 'pipelines', 'lbs', 'its', 'supply', 'pier', '086', 'said', 'have', 'context', 'behind', 'issuing', 'for', 'arab', \"don't\", '8', 'last', 'amid', 'etl', 'in', 'market', 'williams', 'holdings', 'in', 'way', 'share', 'for', 'urge', 'and', '5865', 'at', 'account', 's', 'end', 'our', 'justice', 'a', 'for', '8231', 'receive', \"regime's\", 'kenya', 'assumes', 'said', 'conrac', 'hadson', 'tully', '226', 'mln', 'in', 'value', 'european', 'effot', 'ago', 'winds', 'and', 'leaving', 'by', 'mcandrews', '5', 'qtr', 'decline', 'department', 'and', 'yielding', 'plan', 'share', 'for', 'expensive', 'support', 'said', 'nervousness', 'interest', 'next', 'pct', 'dlrs']\n",
      "\tTokens:\n",
      " [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 22, 265, 9, 621, 575, 1080, 4742, 1149, 15874, 6, 438, 8295, 13, 102, 388, 15, 90, 67, 7, 197, 8295, 8, 4, 270, 416, 23, 527, 6, 15874, 4891, 4, 1055, 742, 16, 8, 36, 1480, 6, 2124, 100, 543, 5, 645, 362, 6, 2912, 4, 49, 8, 15874, 976, 124, 20, 5, 8295, 80, 9, 100, 362, 543, 395, 61, 44, 20, 8295, 8, 16, 40, 1276, 42, 1436, 166, 415, 6, 888, 4, 116, 9, 40, 3089, 4, 303, 163, 16, 64, 772, 13, 94, 156, 17, 12]\n",
      "\tText:\n",
      " ['the', 'bleached', 'could', 'mln', 'at', 'world', 'as', 'holding', 'for', 'include', 'its', 'i', '3', 'start', 'measures', 'gnp', '525', 'process', 'ccb', 'and', 'nations', 'bleached', 'it', '1985', 'do', '000', 'april', '0', 'a', 'agreed', 'bleached', 'mln', 'in', 'ended', 'cost', 'cts', 'must', 'and', 'ccb', 'tenneco', 'in', 'winter', '53', '1', 'mln', 'net', 'diplomats', 'and', 'reorganization', 'group', '38', 'said', '49', '26', 'and', 'plastics', 'in', 'this', 'mln', 'ccb', 'field', 'foreign', 'is', 'said', 'bleached', '10', '3', 'group', '26', '38', 'producers', 'had', '4', 'is', 'bleached', 'mln', '1', 'as', 'equivalent', 'not', '145', 'world', 'york', 'and', 'credits', 'in', '20', '3', 'as', 'permits', 'in', 'set', 'board', '1', 'share', 'turnover', 'it', 'than', 'growth', 'pct', 'dlrs']\n",
      "Sample example text shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Get Text Data\n",
    "\n",
    "sample_example_text = keras.datasets.reuters.load_data()\n",
    "reuters_tokenizer = keras.datasets.reuters.get_word_index()\n",
    "inverse_reuters_tokenizer = {v: k for k, v in reuters_tokenizer.items()}\n",
    "\n",
    "def index_to_word(index):\n",
    "    return str(reuters_tokenizer[index])\n",
    "\n",
    "# preview 5 records\n",
    "sample_example_text = sample_example_text[0][0][:5]\n",
    "print(\"Sample example text:\")\n",
    "for i in range(5):\n",
    "    as_text = [inverse_reuters_tokenizer.get(x, x) for x in sample_example_text[i]]\n",
    "    indicies = np.array(sample_example_text[i])\n",
    "    print(\"\\tTokens:\\n\", sample_example_text[i])\n",
    "    print(\"\\tText:\\n\", as_text)\n",
    "print(\"Sample example text shape:\", sample_example_text.shape)\n",
    "sample_example_text = sample_example_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mar_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
